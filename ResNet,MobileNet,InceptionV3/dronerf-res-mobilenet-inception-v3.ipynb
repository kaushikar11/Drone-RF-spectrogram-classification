{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14cb9c84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:24:01.935227Z",
     "iopub.status.busy": "2025-04-13T11:24:01.934979Z",
     "iopub.status.idle": "2025-04-13T11:25:15.956005Z",
     "shell.execute_reply": "2025-04-13T11:25:15.955150Z"
    },
    "papermill": {
     "duration": 74.025857,
     "end_time": "2025-04-13T11:25:15.957389",
     "exception": false,
     "start_time": "2025-04-13T11:24:01.931532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thop\r\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from thop) (2.5.1+cu124)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (4.13.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->thop)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->thop)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->thop)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->thop)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->thop)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->thop)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->thop)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->thop) (3.0.2)\r\n",
      "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 thop-0.1.1.post2209072238\r\n"
     ]
    }
   ],
   "source": [
    "!pip install thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdf062cc",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-13T11:25:15.994792Z",
     "iopub.status.busy": "2025-04-13T11:25:15.994516Z",
     "iopub.status.idle": "2025-04-13T14:00:30.425248Z",
     "shell.execute_reply": "2025-04-13T14:00:30.424424Z"
    },
    "papermill": {
     "duration": 9314.451056,
     "end_time": "2025-04-13T14:00:30.426580",
     "exception": false,
     "start_time": "2025-04-13T11:25:15.975524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total number of samples: 17744\n",
      "Class to index mapping: {'DJI': 0, 'FutabaT14': 1, 'FutabaT7': 2, 'Graupner': 3, 'Noise': 4, 'Taranis': 5, 'Turnigy': 6}\n",
      "Class DJI: 1280 samples\n",
      "Class FutabaT14: 3472 samples\n",
      "Class FutabaT7: 801 samples\n",
      "Class Graupner: 801 samples\n",
      "Class Noise: 8872 samples\n",
      "Class Taranis: 1663 samples\n",
      "Class Turnigy: 855 samples\n",
      "Number of classes: 7\n",
      "Class Names: ['DJI', 'FutabaT14', 'FutabaT7', 'Graupner', 'Noise', 'Taranis', 'Turnigy']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING RESNET-50\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 192MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training and Evaluating ResNet-50\n",
      "==================================================\n",
      "\n",
      "ResNet-50 Summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                    [-1, 7]          14,343\n",
      "================================================================\n",
      "Total params: 23,522,375\n",
      "Trainable params: 23,522,375\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.55\n",
      "Params size (MB): 89.73\n",
      "Estimated Total Size (MB): 376.86\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Starting training ResNet-50...\n",
      "Epoch [1/50], Step [10/777], Loss: 1.4456\n",
      "Epoch [1/50], Step [20/777], Loss: 1.2779\n",
      "Epoch [1/50], Step [30/777], Loss: 0.7493\n",
      "Epoch [1/50], Step [40/777], Loss: 0.9832\n",
      "Epoch [1/50], Step [50/777], Loss: 0.3870\n",
      "Epoch [1/50], Step [60/777], Loss: 0.7752\n",
      "Epoch [1/50], Step [70/777], Loss: 0.4541\n",
      "Epoch [1/50], Step [80/777], Loss: 0.8203\n",
      "Epoch [1/50], Step [90/777], Loss: 1.0488\n",
      "Epoch [1/50], Step [100/777], Loss: 0.4135\n",
      "Epoch [1/50], Step [110/777], Loss: 0.6751\n",
      "Epoch [1/50], Step [120/777], Loss: 0.4552\n",
      "Epoch [1/50], Step [130/777], Loss: 0.2035\n",
      "Epoch [1/50], Step [140/777], Loss: 0.7426\n",
      "Epoch [1/50], Step [150/777], Loss: 0.7643\n",
      "Epoch [1/50], Step [160/777], Loss: 0.5647\n",
      "Epoch [1/50], Step [170/777], Loss: 0.2778\n",
      "Epoch [1/50], Step [180/777], Loss: 0.4714\n",
      "Epoch [1/50], Step [190/777], Loss: 0.5409\n",
      "Epoch [1/50], Step [200/777], Loss: 0.2547\n",
      "Epoch [1/50], Step [210/777], Loss: 0.2672\n",
      "Epoch [1/50], Step [220/777], Loss: 0.3383\n",
      "Epoch [1/50], Step [230/777], Loss: 0.4336\n",
      "Epoch [1/50], Step [240/777], Loss: 0.4117\n",
      "Epoch [1/50], Step [250/777], Loss: 0.3932\n",
      "Epoch [1/50], Step [260/777], Loss: 0.6656\n",
      "Epoch [1/50], Step [270/777], Loss: 0.4442\n",
      "Epoch [1/50], Step [280/777], Loss: 0.5395\n",
      "Epoch [1/50], Step [290/777], Loss: 0.3162\n",
      "Epoch [1/50], Step [300/777], Loss: 0.2261\n",
      "Epoch [1/50], Step [310/777], Loss: 0.0641\n",
      "Epoch [1/50], Step [320/777], Loss: 0.4733\n",
      "Epoch [1/50], Step [330/777], Loss: 0.1141\n",
      "Epoch [1/50], Step [340/777], Loss: 0.5296\n",
      "Epoch [1/50], Step [350/777], Loss: 0.5796\n",
      "Epoch [1/50], Step [360/777], Loss: 0.0935\n",
      "Epoch [1/50], Step [370/777], Loss: 0.1091\n",
      "Epoch [1/50], Step [380/777], Loss: 0.5954\n",
      "Epoch [1/50], Step [390/777], Loss: 0.4412\n",
      "Epoch [1/50], Step [400/777], Loss: 0.5583\n",
      "Epoch [1/50], Step [410/777], Loss: 0.5163\n",
      "Epoch [1/50], Step [420/777], Loss: 0.2230\n",
      "Epoch [1/50], Step [430/777], Loss: 0.5128\n",
      "Epoch [1/50], Step [440/777], Loss: 0.3907\n",
      "Epoch [1/50], Step [450/777], Loss: 0.1609\n",
      "Epoch [1/50], Step [460/777], Loss: 0.0685\n",
      "Epoch [1/50], Step [470/777], Loss: 0.1627\n",
      "Epoch [1/50], Step [480/777], Loss: 0.3276\n",
      "Epoch [1/50], Step [490/777], Loss: 0.5405\n",
      "Epoch [1/50], Step [500/777], Loss: 0.0458\n",
      "Epoch [1/50], Step [510/777], Loss: 0.0657\n",
      "Epoch [1/50], Step [520/777], Loss: 0.1885\n",
      "Epoch [1/50], Step [530/777], Loss: 0.1911\n",
      "Epoch [1/50], Step [540/777], Loss: 0.4932\n",
      "Epoch [1/50], Step [550/777], Loss: 0.6421\n",
      "Epoch [1/50], Step [560/777], Loss: 0.4363\n",
      "Epoch [1/50], Step [570/777], Loss: 0.2129\n",
      "Epoch [1/50], Step [580/777], Loss: 0.0696\n",
      "Epoch [1/50], Step [590/777], Loss: 0.5876\n",
      "Epoch [1/50], Step [600/777], Loss: 0.2047\n",
      "Epoch [1/50], Step [610/777], Loss: 0.3098\n",
      "Epoch [1/50], Step [620/777], Loss: 0.1516\n",
      "Epoch [1/50], Step [630/777], Loss: 0.3621\n",
      "Epoch [1/50], Step [640/777], Loss: 0.0630\n",
      "Epoch [1/50], Step [650/777], Loss: 0.2991\n",
      "Epoch [1/50], Step [660/777], Loss: 0.5014\n",
      "Epoch [1/50], Step [670/777], Loss: 0.2680\n",
      "Epoch [1/50], Step [680/777], Loss: 0.2956\n",
      "Epoch [1/50], Step [690/777], Loss: 0.1349\n",
      "Epoch [1/50], Step [700/777], Loss: 0.2450\n",
      "Epoch [1/50], Step [710/777], Loss: 0.3631\n",
      "Epoch [1/50], Step [720/777], Loss: 0.0681\n",
      "Epoch [1/50], Step [730/777], Loss: 0.0205\n",
      "Epoch [1/50], Step [740/777], Loss: 0.0906\n",
      "Epoch [1/50], Step [750/777], Loss: 1.0146\n",
      "Epoch [1/50], Step [760/777], Loss: 0.2935\n",
      "Epoch [1/50], Step [770/777], Loss: 0.5005\n",
      "Epoch [1/50], Train Loss: 0.4261, Val Loss: 0.2341, Val Accuracy: 0.9387\n",
      "Model saved with validation accuracy: 0.9387\n",
      "Epoch [2/50], Step [10/777], Loss: 0.0965\n",
      "Epoch [2/50], Step [20/777], Loss: 0.2384\n",
      "Epoch [2/50], Step [30/777], Loss: 0.1006\n",
      "Epoch [2/50], Step [40/777], Loss: 0.0619\n",
      "Epoch [2/50], Step [50/777], Loss: 0.0513\n",
      "Epoch [2/50], Step [60/777], Loss: 0.2848\n",
      "Epoch [2/50], Step [70/777], Loss: 0.1191\n",
      "Epoch [2/50], Step [80/777], Loss: 0.0257\n",
      "Epoch [2/50], Step [90/777], Loss: 0.6522\n",
      "Epoch [2/50], Step [100/777], Loss: 0.0445\n",
      "Epoch [2/50], Step [110/777], Loss: 0.1809\n",
      "Epoch [2/50], Step [120/777], Loss: 0.5180\n",
      "Epoch [2/50], Step [130/777], Loss: 0.0641\n",
      "Epoch [2/50], Step [140/777], Loss: 0.0327\n",
      "Epoch [2/50], Step [150/777], Loss: 0.1989\n",
      "Epoch [2/50], Step [160/777], Loss: 0.0369\n",
      "Epoch [2/50], Step [170/777], Loss: 0.0890\n",
      "Epoch [2/50], Step [180/777], Loss: 0.5422\n",
      "Epoch [2/50], Step [190/777], Loss: 0.0297\n",
      "Epoch [2/50], Step [200/777], Loss: 0.1122\n",
      "Epoch [2/50], Step [210/777], Loss: 0.3194\n",
      "Epoch [2/50], Step [220/777], Loss: 0.0180\n",
      "Epoch [2/50], Step [230/777], Loss: 0.0736\n",
      "Epoch [2/50], Step [240/777], Loss: 0.6274\n",
      "Epoch [2/50], Step [250/777], Loss: 0.4247\n",
      "Epoch [2/50], Step [260/777], Loss: 0.7748\n",
      "Epoch [2/50], Step [270/777], Loss: 0.4851\n",
      "Epoch [2/50], Step [280/777], Loss: 0.0516\n",
      "Epoch [2/50], Step [290/777], Loss: 0.2386\n",
      "Epoch [2/50], Step [300/777], Loss: 0.2377\n",
      "Epoch [2/50], Step [310/777], Loss: 0.8322\n",
      "Epoch [2/50], Step [320/777], Loss: 0.5838\n",
      "Epoch [2/50], Step [330/777], Loss: 0.3111\n",
      "Epoch [2/50], Step [340/777], Loss: 0.1302\n",
      "Epoch [2/50], Step [350/777], Loss: 0.2781\n",
      "Epoch [2/50], Step [360/777], Loss: 0.0773\n",
      "Epoch [2/50], Step [370/777], Loss: 0.3375\n",
      "Epoch [2/50], Step [380/777], Loss: 0.4192\n",
      "Epoch [2/50], Step [390/777], Loss: 0.0752\n",
      "Epoch [2/50], Step [400/777], Loss: 0.0431\n",
      "Epoch [2/50], Step [410/777], Loss: 0.3667\n",
      "Epoch [2/50], Step [420/777], Loss: 0.2574\n",
      "Epoch [2/50], Step [430/777], Loss: 0.0958\n",
      "Epoch [2/50], Step [440/777], Loss: 0.0490\n",
      "Epoch [2/50], Step [450/777], Loss: 0.0362\n",
      "Epoch [2/50], Step [460/777], Loss: 0.5475\n",
      "Epoch [2/50], Step [470/777], Loss: 0.0500\n",
      "Epoch [2/50], Step [480/777], Loss: 0.0858\n",
      "Epoch [2/50], Step [490/777], Loss: 0.4258\n",
      "Epoch [2/50], Step [500/777], Loss: 0.3596\n",
      "Epoch [2/50], Step [510/777], Loss: 0.4351\n",
      "Epoch [2/50], Step [520/777], Loss: 0.1506\n",
      "Epoch [2/50], Step [530/777], Loss: 0.6187\n",
      "Epoch [2/50], Step [540/777], Loss: 0.0684\n",
      "Epoch [2/50], Step [550/777], Loss: 0.3055\n",
      "Epoch [2/50], Step [560/777], Loss: 0.4400\n",
      "Epoch [2/50], Step [570/777], Loss: 0.0165\n",
      "Epoch [2/50], Step [580/777], Loss: 0.3258\n",
      "Epoch [2/50], Step [590/777], Loss: 0.0801\n",
      "Epoch [2/50], Step [600/777], Loss: 0.2579\n",
      "Epoch [2/50], Step [610/777], Loss: 0.2667\n",
      "Epoch [2/50], Step [620/777], Loss: 0.5724\n",
      "Epoch [2/50], Step [630/777], Loss: 0.1863\n",
      "Epoch [2/50], Step [640/777], Loss: 0.2357\n",
      "Epoch [2/50], Step [650/777], Loss: 0.5921\n",
      "Epoch [2/50], Step [660/777], Loss: 0.7039\n",
      "Epoch [2/50], Step [670/777], Loss: 0.0649\n",
      "Epoch [2/50], Step [680/777], Loss: 0.0650\n",
      "Epoch [2/50], Step [690/777], Loss: 0.0785\n",
      "Epoch [2/50], Step [700/777], Loss: 0.0259\n",
      "Epoch [2/50], Step [710/777], Loss: 0.1697\n",
      "Epoch [2/50], Step [720/777], Loss: 0.1689\n",
      "Epoch [2/50], Step [730/777], Loss: 0.3969\n",
      "Epoch [2/50], Step [740/777], Loss: 0.2573\n",
      "Epoch [2/50], Step [750/777], Loss: 0.4329\n",
      "Epoch [2/50], Step [760/777], Loss: 0.3657\n",
      "Epoch [2/50], Step [770/777], Loss: 0.0709\n",
      "Epoch [2/50], Train Loss: 0.2499, Val Loss: 0.1935, Val Accuracy: 0.9534\n",
      "Model saved with validation accuracy: 0.9534\n",
      "Epoch [3/50], Step [10/777], Loss: 0.0958\n",
      "Epoch [3/50], Step [20/777], Loss: 0.0498\n",
      "Epoch [3/50], Step [30/777], Loss: 0.4657\n",
      "Epoch [3/50], Step [40/777], Loss: 0.3366\n",
      "Epoch [3/50], Step [50/777], Loss: 0.4260\n",
      "Epoch [3/50], Step [60/777], Loss: 0.0395\n",
      "Epoch [3/50], Step [70/777], Loss: 0.0430\n",
      "Epoch [3/50], Step [80/777], Loss: 0.4672\n",
      "Epoch [3/50], Step [90/777], Loss: 0.4277\n",
      "Epoch [3/50], Step [100/777], Loss: 0.2240\n",
      "Epoch [3/50], Step [110/777], Loss: 0.2463\n",
      "Epoch [3/50], Step [120/777], Loss: 0.0800\n",
      "Epoch [3/50], Step [130/777], Loss: 0.1723\n",
      "Epoch [3/50], Step [140/777], Loss: 0.3866\n",
      "Epoch [3/50], Step [150/777], Loss: 0.1066\n",
      "Epoch [3/50], Step [160/777], Loss: 0.1087\n",
      "Epoch [3/50], Step [170/777], Loss: 0.1437\n",
      "Epoch [3/50], Step [180/777], Loss: 0.2602\n",
      "Epoch [3/50], Step [190/777], Loss: 0.1423\n",
      "Epoch [3/50], Step [200/777], Loss: 0.5341\n",
      "Epoch [3/50], Step [210/777], Loss: 0.1148\n",
      "Epoch [3/50], Step [220/777], Loss: 0.0377\n",
      "Epoch [3/50], Step [230/777], Loss: 0.2422\n",
      "Epoch [3/50], Step [240/777], Loss: 0.7322\n",
      "Epoch [3/50], Step [250/777], Loss: 0.2572\n",
      "Epoch [3/50], Step [260/777], Loss: 0.1516\n",
      "Epoch [3/50], Step [270/777], Loss: 0.1567\n",
      "Epoch [3/50], Step [280/777], Loss: 0.0418\n",
      "Epoch [3/50], Step [290/777], Loss: 0.2740\n",
      "Epoch [3/50], Step [300/777], Loss: 0.0473\n",
      "Epoch [3/50], Step [310/777], Loss: 0.2503\n",
      "Epoch [3/50], Step [320/777], Loss: 0.3212\n",
      "Epoch [3/50], Step [330/777], Loss: 0.1953\n",
      "Epoch [3/50], Step [340/777], Loss: 0.3559\n",
      "Epoch [3/50], Step [350/777], Loss: 0.1713\n",
      "Epoch [3/50], Step [360/777], Loss: 0.0693\n",
      "Epoch [3/50], Step [370/777], Loss: 0.6176\n",
      "Epoch [3/50], Step [380/777], Loss: 0.1565\n",
      "Epoch [3/50], Step [390/777], Loss: 0.0440\n",
      "Epoch [3/50], Step [400/777], Loss: 0.1449\n",
      "Epoch [3/50], Step [410/777], Loss: 0.4355\n",
      "Epoch [3/50], Step [420/777], Loss: 0.0978\n",
      "Epoch [3/50], Step [430/777], Loss: 0.1938\n",
      "Epoch [3/50], Step [440/777], Loss: 0.0309\n",
      "Epoch [3/50], Step [450/777], Loss: 0.0518\n",
      "Epoch [3/50], Step [460/777], Loss: 0.6182\n",
      "Epoch [3/50], Step [470/777], Loss: 0.2246\n",
      "Epoch [3/50], Step [480/777], Loss: 0.0711\n",
      "Epoch [3/50], Step [490/777], Loss: 0.2569\n",
      "Epoch [3/50], Step [500/777], Loss: 0.0558\n",
      "Epoch [3/50], Step [510/777], Loss: 0.1166\n",
      "Epoch [3/50], Step [520/777], Loss: 0.3208\n",
      "Epoch [3/50], Step [530/777], Loss: 0.0867\n",
      "Epoch [3/50], Step [540/777], Loss: 0.0872\n",
      "Epoch [3/50], Step [550/777], Loss: 0.2640\n",
      "Epoch [3/50], Step [560/777], Loss: 0.0431\n",
      "Epoch [3/50], Step [570/777], Loss: 0.0610\n",
      "Epoch [3/50], Step [580/777], Loss: 0.0338\n",
      "Epoch [3/50], Step [590/777], Loss: 0.0624\n",
      "Epoch [3/50], Step [600/777], Loss: 0.0209\n",
      "Epoch [3/50], Step [610/777], Loss: 0.0318\n",
      "Epoch [3/50], Step [620/777], Loss: 0.1031\n",
      "Epoch [3/50], Step [630/777], Loss: 0.0879\n",
      "Epoch [3/50], Step [640/777], Loss: 0.3824\n",
      "Epoch [3/50], Step [650/777], Loss: 0.8948\n",
      "Epoch [3/50], Step [660/777], Loss: 0.3252\n",
      "Epoch [3/50], Step [670/777], Loss: 0.4832\n",
      "Epoch [3/50], Step [680/777], Loss: 0.1327\n",
      "Epoch [3/50], Step [690/777], Loss: 0.1274\n",
      "Epoch [3/50], Step [700/777], Loss: 0.1406\n",
      "Epoch [3/50], Step [710/777], Loss: 0.0363\n",
      "Epoch [3/50], Step [720/777], Loss: 0.0505\n",
      "Epoch [3/50], Step [730/777], Loss: 0.2729\n",
      "Epoch [3/50], Step [740/777], Loss: 0.0317\n",
      "Epoch [3/50], Step [750/777], Loss: 0.1114\n",
      "Epoch [3/50], Step [760/777], Loss: 0.2297\n",
      "Epoch [3/50], Step [770/777], Loss: 0.4137\n",
      "Epoch [3/50], Train Loss: 0.2037, Val Loss: 0.2064, Val Accuracy: 0.9508\n",
      "Epoch [4/50], Step [10/777], Loss: 0.0475\n",
      "Epoch [4/50], Step [20/777], Loss: 0.1000\n",
      "Epoch [4/50], Step [30/777], Loss: 0.1103\n",
      "Epoch [4/50], Step [40/777], Loss: 0.2709\n",
      "Epoch [4/50], Step [50/777], Loss: 0.5174\n",
      "Epoch [4/50], Step [60/777], Loss: 0.2108\n",
      "Epoch [4/50], Step [70/777], Loss: 0.0828\n",
      "Epoch [4/50], Step [80/777], Loss: 0.3675\n",
      "Epoch [4/50], Step [90/777], Loss: 0.0613\n",
      "Epoch [4/50], Step [100/777], Loss: 0.0831\n",
      "Epoch [4/50], Step [110/777], Loss: 0.3214\n",
      "Epoch [4/50], Step [120/777], Loss: 0.0841\n",
      "Epoch [4/50], Step [130/777], Loss: 0.1647\n",
      "Epoch [4/50], Step [140/777], Loss: 0.0331\n",
      "Epoch [4/50], Step [150/777], Loss: 0.0435\n",
      "Epoch [4/50], Step [160/777], Loss: 0.1648\n",
      "Epoch [4/50], Step [170/777], Loss: 0.0262\n",
      "Epoch [4/50], Step [180/777], Loss: 0.3065\n",
      "Epoch [4/50], Step [190/777], Loss: 0.0427\n",
      "Epoch [4/50], Step [200/777], Loss: 0.0387\n",
      "Epoch [4/50], Step [210/777], Loss: 0.0408\n",
      "Epoch [4/50], Step [220/777], Loss: 0.0396\n",
      "Epoch [4/50], Step [230/777], Loss: 0.0632\n",
      "Epoch [4/50], Step [240/777], Loss: 0.0203\n",
      "Epoch [4/50], Step [250/777], Loss: 0.0530\n",
      "Epoch [4/50], Step [260/777], Loss: 0.0381\n",
      "Epoch [4/50], Step [270/777], Loss: 0.2813\n",
      "Epoch [4/50], Step [280/777], Loss: 0.0435\n",
      "Epoch [4/50], Step [290/777], Loss: 0.0647\n",
      "Epoch [4/50], Step [300/777], Loss: 0.0568\n",
      "Epoch [4/50], Step [310/777], Loss: 0.2164\n",
      "Epoch [4/50], Step [320/777], Loss: 0.0311\n",
      "Epoch [4/50], Step [330/777], Loss: 0.2813\n",
      "Epoch [4/50], Step [340/777], Loss: 0.0450\n",
      "Epoch [4/50], Step [350/777], Loss: 0.7961\n",
      "Epoch [4/50], Step [360/777], Loss: 0.0325\n",
      "Epoch [4/50], Step [370/777], Loss: 0.2999\n",
      "Epoch [4/50], Step [380/777], Loss: 0.0189\n",
      "Epoch [4/50], Step [390/777], Loss: 0.0550\n",
      "Epoch [4/50], Step [400/777], Loss: 0.0547\n",
      "Epoch [4/50], Step [410/777], Loss: 0.1574\n",
      "Epoch [4/50], Step [420/777], Loss: 0.1926\n",
      "Epoch [4/50], Step [430/777], Loss: 0.0732\n",
      "Epoch [4/50], Step [440/777], Loss: 0.2810\n",
      "Epoch [4/50], Step [450/777], Loss: 0.2479\n",
      "Epoch [4/50], Step [460/777], Loss: 0.0345\n",
      "Epoch [4/50], Step [470/777], Loss: 0.0222\n",
      "Epoch [4/50], Step [480/777], Loss: 0.1078\n",
      "Epoch [4/50], Step [490/777], Loss: 0.2084\n",
      "Epoch [4/50], Step [500/777], Loss: 0.0781\n",
      "Epoch [4/50], Step [510/777], Loss: 0.4295\n",
      "Epoch [4/50], Step [520/777], Loss: 0.3965\n",
      "Epoch [4/50], Step [530/777], Loss: 0.0505\n",
      "Epoch [4/50], Step [540/777], Loss: 0.3193\n",
      "Epoch [4/50], Step [550/777], Loss: 0.1588\n",
      "Epoch [4/50], Step [560/777], Loss: 0.0262\n",
      "Epoch [4/50], Step [570/777], Loss: 0.1971\n",
      "Epoch [4/50], Step [580/777], Loss: 0.0316\n",
      "Epoch [4/50], Step [590/777], Loss: 0.0104\n",
      "Epoch [4/50], Step [600/777], Loss: 0.3184\n",
      "Epoch [4/50], Step [610/777], Loss: 0.1015\n",
      "Epoch [4/50], Step [620/777], Loss: 0.3677\n",
      "Epoch [4/50], Step [630/777], Loss: 0.3785\n",
      "Epoch [4/50], Step [640/777], Loss: 0.1112\n",
      "Epoch [4/50], Step [650/777], Loss: 0.0469\n",
      "Epoch [4/50], Step [660/777], Loss: 0.4322\n",
      "Epoch [4/50], Step [670/777], Loss: 0.0652\n",
      "Epoch [4/50], Step [680/777], Loss: 0.1109\n",
      "Epoch [4/50], Step [690/777], Loss: 0.0665\n",
      "Epoch [4/50], Step [700/777], Loss: 0.2453\n",
      "Epoch [4/50], Step [710/777], Loss: 0.0215\n",
      "Epoch [4/50], Step [720/777], Loss: 0.0363\n",
      "Epoch [4/50], Step [730/777], Loss: 0.0422\n",
      "Epoch [4/50], Step [740/777], Loss: 0.0372\n",
      "Epoch [4/50], Step [750/777], Loss: 0.0352\n",
      "Epoch [4/50], Step [760/777], Loss: 0.4558\n",
      "Epoch [4/50], Step [770/777], Loss: 0.0492\n",
      "Epoch [4/50], Train Loss: 0.1789, Val Loss: 0.2083, Val Accuracy: 0.9508\n",
      "Epoch [5/50], Step [10/777], Loss: 0.0141\n",
      "Epoch [5/50], Step [20/777], Loss: 0.0177\n",
      "Epoch [5/50], Step [30/777], Loss: 0.1712\n",
      "Epoch [5/50], Step [40/777], Loss: 0.0368\n",
      "Epoch [5/50], Step [50/777], Loss: 0.1543\n",
      "Epoch [5/50], Step [60/777], Loss: 0.0296\n",
      "Epoch [5/50], Step [70/777], Loss: 0.1572\n",
      "Epoch [5/50], Step [80/777], Loss: 0.0387\n",
      "Epoch [5/50], Step [90/777], Loss: 0.0221\n",
      "Epoch [5/50], Step [100/777], Loss: 0.0659\n",
      "Epoch [5/50], Step [110/777], Loss: 0.2216\n",
      "Epoch [5/50], Step [120/777], Loss: 0.0477\n",
      "Epoch [5/50], Step [130/777], Loss: 0.0309\n",
      "Epoch [5/50], Step [140/777], Loss: 0.0519\n",
      "Epoch [5/50], Step [150/777], Loss: 0.1667\n",
      "Epoch [5/50], Step [160/777], Loss: 0.0596\n",
      "Epoch [5/50], Step [170/777], Loss: 0.2208\n",
      "Epoch [5/50], Step [180/777], Loss: 0.1993\n",
      "Epoch [5/50], Step [190/777], Loss: 0.0118\n",
      "Epoch [5/50], Step [200/777], Loss: 0.6654\n",
      "Epoch [5/50], Step [210/777], Loss: 0.1371\n",
      "Epoch [5/50], Step [220/777], Loss: 0.0839\n",
      "Epoch [5/50], Step [230/777], Loss: 0.3889\n",
      "Epoch [5/50], Step [240/777], Loss: 0.0230\n",
      "Epoch [5/50], Step [250/777], Loss: 0.0303\n",
      "Epoch [5/50], Step [260/777], Loss: 0.0505\n",
      "Epoch [5/50], Step [270/777], Loss: 0.0372\n",
      "Epoch [5/50], Step [280/777], Loss: 0.2620\n",
      "Epoch [5/50], Step [290/777], Loss: 0.0781\n",
      "Epoch [5/50], Step [300/777], Loss: 0.5180\n",
      "Epoch [5/50], Step [310/777], Loss: 0.0469\n",
      "Epoch [5/50], Step [320/777], Loss: 0.0379\n",
      "Epoch [5/50], Step [330/777], Loss: 0.0704\n",
      "Epoch [5/50], Step [340/777], Loss: 0.2031\n",
      "Epoch [5/50], Step [350/777], Loss: 0.2287\n",
      "Epoch [5/50], Step [360/777], Loss: 0.0341\n",
      "Epoch [5/50], Step [370/777], Loss: 0.3911\n",
      "Epoch [5/50], Step [380/777], Loss: 0.0322\n",
      "Epoch [5/50], Step [390/777], Loss: 0.0691\n",
      "Epoch [5/50], Step [400/777], Loss: 0.2225\n",
      "Epoch [5/50], Step [410/777], Loss: 0.0402\n",
      "Epoch [5/50], Step [420/777], Loss: 0.0645\n",
      "Epoch [5/50], Step [430/777], Loss: 0.0418\n",
      "Epoch [5/50], Step [440/777], Loss: 0.3499\n",
      "Epoch [5/50], Step [450/777], Loss: 0.1842\n",
      "Epoch [5/50], Step [460/777], Loss: 0.0370\n",
      "Epoch [5/50], Step [470/777], Loss: 0.0224\n",
      "Epoch [5/50], Step [480/777], Loss: 0.0197\n",
      "Epoch [5/50], Step [490/777], Loss: 0.0510\n",
      "Epoch [5/50], Step [500/777], Loss: 0.5946\n",
      "Epoch [5/50], Step [510/777], Loss: 0.3021\n",
      "Epoch [5/50], Step [520/777], Loss: 0.1688\n",
      "Epoch [5/50], Step [530/777], Loss: 0.2874\n",
      "Epoch [5/50], Step [540/777], Loss: 0.3278\n",
      "Epoch [5/50], Step [550/777], Loss: 0.0118\n",
      "Epoch [5/50], Step [560/777], Loss: 0.0687\n",
      "Epoch [5/50], Step [570/777], Loss: 0.0085\n",
      "Epoch [5/50], Step [580/777], Loss: 0.0066\n",
      "Epoch [5/50], Step [590/777], Loss: 0.0261\n",
      "Epoch [5/50], Step [600/777], Loss: 0.1165\n",
      "Epoch [5/50], Step [610/777], Loss: 0.0960\n",
      "Epoch [5/50], Step [620/777], Loss: 0.2424\n",
      "Epoch [5/50], Step [630/777], Loss: 0.0207\n",
      "Epoch [5/50], Step [640/777], Loss: 0.0168\n",
      "Epoch [5/50], Step [650/777], Loss: 0.3964\n",
      "Epoch [5/50], Step [660/777], Loss: 0.0915\n",
      "Epoch [5/50], Step [670/777], Loss: 0.0171\n",
      "Epoch [5/50], Step [680/777], Loss: 0.0253\n",
      "Epoch [5/50], Step [690/777], Loss: 0.1935\n",
      "Epoch [5/50], Step [700/777], Loss: 0.0168\n",
      "Epoch [5/50], Step [710/777], Loss: 0.0063\n",
      "Epoch [5/50], Step [720/777], Loss: 0.2509\n",
      "Epoch [5/50], Step [730/777], Loss: 0.0179\n",
      "Epoch [5/50], Step [740/777], Loss: 0.2232\n",
      "Epoch [5/50], Step [750/777], Loss: 0.0620\n",
      "Epoch [5/50], Step [760/777], Loss: 0.0285\n",
      "Epoch [5/50], Step [770/777], Loss: 0.1745\n",
      "Epoch [5/50], Train Loss: 0.1522, Val Loss: 0.1943, Val Accuracy: 0.9523\n",
      "Epoch [6/50], Step [10/777], Loss: 0.0527\n",
      "Epoch [6/50], Step [20/777], Loss: 0.0230\n",
      "Epoch [6/50], Step [30/777], Loss: 0.1885\n",
      "Epoch [6/50], Step [40/777], Loss: 0.0561\n",
      "Epoch [6/50], Step [50/777], Loss: 0.3246\n",
      "Epoch [6/50], Step [60/777], Loss: 0.5883\n",
      "Epoch [6/50], Step [70/777], Loss: 0.3690\n",
      "Epoch [6/50], Step [80/777], Loss: 0.2690\n",
      "Epoch [6/50], Step [90/777], Loss: 0.0211\n",
      "Epoch [6/50], Step [100/777], Loss: 0.0133\n",
      "Epoch [6/50], Step [110/777], Loss: 0.0051\n",
      "Epoch [6/50], Step [120/777], Loss: 0.0340\n",
      "Epoch [6/50], Step [130/777], Loss: 0.4427\n",
      "Epoch [6/50], Step [140/777], Loss: 0.0631\n",
      "Epoch [6/50], Step [150/777], Loss: 0.0391\n",
      "Epoch [6/50], Step [160/777], Loss: 0.0238\n",
      "Epoch [6/50], Step [170/777], Loss: 0.0611\n",
      "Epoch [6/50], Step [180/777], Loss: 0.0262\n",
      "Epoch [6/50], Step [190/777], Loss: 0.4568\n",
      "Epoch [6/50], Step [200/777], Loss: 0.3255\n",
      "Epoch [6/50], Step [210/777], Loss: 0.0922\n",
      "Epoch [6/50], Step [220/777], Loss: 0.0402\n",
      "Epoch [6/50], Step [230/777], Loss: 0.0395\n",
      "Epoch [6/50], Step [240/777], Loss: 0.0224\n",
      "Epoch [6/50], Step [250/777], Loss: 0.0177\n",
      "Epoch [6/50], Step [260/777], Loss: 0.2508\n",
      "Epoch [6/50], Step [270/777], Loss: 0.1937\n",
      "Epoch [6/50], Step [280/777], Loss: 0.2069\n",
      "Epoch [6/50], Step [290/777], Loss: 0.4620\n",
      "Epoch [6/50], Step [300/777], Loss: 0.0145\n",
      "Epoch [6/50], Step [310/777], Loss: 0.0885\n",
      "Epoch [6/50], Step [320/777], Loss: 0.0573\n",
      "Epoch [6/50], Step [330/777], Loss: 0.2264\n",
      "Epoch [6/50], Step [340/777], Loss: 0.4394\n",
      "Epoch [6/50], Step [350/777], Loss: 0.0405\n",
      "Epoch [6/50], Step [360/777], Loss: 0.2244\n",
      "Epoch [6/50], Step [370/777], Loss: 0.2408\n",
      "Epoch [6/50], Step [380/777], Loss: 0.3893\n",
      "Epoch [6/50], Step [390/777], Loss: 0.0140\n",
      "Epoch [6/50], Step [400/777], Loss: 0.0512\n",
      "Epoch [6/50], Step [410/777], Loss: 0.2209\n",
      "Epoch [6/50], Step [420/777], Loss: 0.0121\n",
      "Epoch [6/50], Step [430/777], Loss: 0.2646\n",
      "Epoch [6/50], Step [440/777], Loss: 0.3858\n",
      "Epoch [6/50], Step [450/777], Loss: 0.0272\n",
      "Epoch [6/50], Step [460/777], Loss: 0.0233\n",
      "Epoch [6/50], Step [470/777], Loss: 0.1938\n",
      "Epoch [6/50], Step [480/777], Loss: 0.0666\n",
      "Epoch [6/50], Step [490/777], Loss: 0.0231\n",
      "Epoch [6/50], Step [500/777], Loss: 0.2982\n",
      "Epoch [6/50], Step [510/777], Loss: 0.0070\n",
      "Epoch [6/50], Step [520/777], Loss: 0.0189\n",
      "Epoch [6/50], Step [530/777], Loss: 0.0637\n",
      "Epoch [6/50], Step [540/777], Loss: 0.0292\n",
      "Epoch [6/50], Step [550/777], Loss: 0.2426\n",
      "Epoch [6/50], Step [560/777], Loss: 0.0190\n",
      "Epoch [6/50], Step [570/777], Loss: 0.0691\n",
      "Epoch [6/50], Step [580/777], Loss: 0.0212\n",
      "Epoch [6/50], Step [590/777], Loss: 0.1812\n",
      "Epoch [6/50], Step [600/777], Loss: 0.1615\n",
      "Epoch [6/50], Step [610/777], Loss: 0.0136\n",
      "Epoch [6/50], Step [620/777], Loss: 0.0550\n",
      "Epoch [6/50], Step [630/777], Loss: 0.0423\n",
      "Epoch [6/50], Step [640/777], Loss: 0.0168\n",
      "Epoch [6/50], Step [650/777], Loss: 0.0920\n",
      "Epoch [6/50], Step [660/777], Loss: 0.0325\n",
      "Epoch [6/50], Step [670/777], Loss: 0.0356\n",
      "Epoch [6/50], Step [680/777], Loss: 0.0389\n",
      "Epoch [6/50], Step [690/777], Loss: 0.1631\n",
      "Epoch [6/50], Step [700/777], Loss: 0.0318\n",
      "Epoch [6/50], Step [710/777], Loss: 0.3834\n",
      "Epoch [6/50], Step [720/777], Loss: 0.0134\n",
      "Epoch [6/50], Step [730/777], Loss: 0.1351\n",
      "Epoch [6/50], Step [740/777], Loss: 0.0597\n",
      "Epoch [6/50], Step [750/777], Loss: 0.0215\n",
      "Epoch [6/50], Step [760/777], Loss: 0.1656\n",
      "Epoch [6/50], Step [770/777], Loss: 0.0460\n",
      "Epoch [6/50], Train Loss: 0.1431, Val Loss: 0.2099, Val Accuracy: 0.9478\n",
      "Epoch [7/50], Step [10/777], Loss: 0.1318\n",
      "Epoch [7/50], Step [20/777], Loss: 0.1936\n",
      "Epoch [7/50], Step [30/777], Loss: 0.0167\n",
      "Epoch [7/50], Step [40/777], Loss: 0.0197\n",
      "Epoch [7/50], Step [50/777], Loss: 0.0124\n",
      "Epoch [7/50], Step [60/777], Loss: 0.3359\n",
      "Epoch [7/50], Step [70/777], Loss: 0.0183\n",
      "Epoch [7/50], Step [80/777], Loss: 0.0656\n",
      "Epoch [7/50], Step [90/777], Loss: 0.0787\n",
      "Epoch [7/50], Step [100/777], Loss: 0.0199\n",
      "Epoch [7/50], Step [110/777], Loss: 0.0147\n",
      "Epoch [7/50], Step [120/777], Loss: 0.0423\n",
      "Epoch [7/50], Step [130/777], Loss: 0.2633\n",
      "Epoch [7/50], Step [140/777], Loss: 0.0114\n",
      "Epoch [7/50], Step [150/777], Loss: 0.0228\n",
      "Epoch [7/50], Step [160/777], Loss: 0.1052\n",
      "Epoch [7/50], Step [170/777], Loss: 0.0249\n",
      "Epoch [7/50], Step [180/777], Loss: 0.3110\n",
      "Epoch [7/50], Step [190/777], Loss: 0.0323\n",
      "Epoch [7/50], Step [200/777], Loss: 0.1916\n",
      "Epoch [7/50], Step [210/777], Loss: 0.0583\n",
      "Epoch [7/50], Step [220/777], Loss: 0.1386\n",
      "Epoch [7/50], Step [230/777], Loss: 0.0118\n",
      "Epoch [7/50], Step [240/777], Loss: 0.0237\n",
      "Epoch [7/50], Step [250/777], Loss: 0.0079\n",
      "Epoch [7/50], Step [260/777], Loss: 0.0059\n",
      "Epoch [7/50], Step [270/777], Loss: 0.5072\n",
      "Epoch [7/50], Step [280/777], Loss: 0.0130\n",
      "Epoch [7/50], Step [290/777], Loss: 0.0105\n",
      "Epoch [7/50], Step [300/777], Loss: 0.1956\n",
      "Epoch [7/50], Step [310/777], Loss: 0.1417\n",
      "Epoch [7/50], Step [320/777], Loss: 0.1706\n",
      "Epoch [7/50], Step [330/777], Loss: 0.1324\n",
      "Epoch [7/50], Step [340/777], Loss: 0.0676\n",
      "Epoch [7/50], Step [350/777], Loss: 0.0250\n",
      "Epoch [7/50], Step [360/777], Loss: 0.0181\n",
      "Epoch [7/50], Step [370/777], Loss: 0.2247\n",
      "Epoch [7/50], Step [380/777], Loss: 0.0538\n",
      "Epoch [7/50], Step [390/777], Loss: 0.0330\n",
      "Epoch [7/50], Step [400/777], Loss: 0.1508\n",
      "Epoch [7/50], Step [410/777], Loss: 0.0368\n",
      "Epoch [7/50], Step [420/777], Loss: 0.1663\n",
      "Epoch [7/50], Step [430/777], Loss: 0.1271\n",
      "Epoch [7/50], Step [440/777], Loss: 0.2845\n",
      "Epoch [7/50], Step [450/777], Loss: 0.0125\n",
      "Epoch [7/50], Step [460/777], Loss: 0.0074\n",
      "Epoch [7/50], Step [470/777], Loss: 0.2530\n",
      "Epoch [7/50], Step [480/777], Loss: 0.3834\n",
      "Epoch [7/50], Step [490/777], Loss: 0.0115\n",
      "Epoch [7/50], Step [500/777], Loss: 0.0153\n",
      "Epoch [7/50], Step [510/777], Loss: 0.0304\n",
      "Epoch [7/50], Step [520/777], Loss: 0.1206\n",
      "Epoch [7/50], Step [530/777], Loss: 0.0425\n",
      "Epoch [7/50], Step [540/777], Loss: 0.0200\n",
      "Epoch [7/50], Step [550/777], Loss: 0.3044\n",
      "Epoch [7/50], Step [560/777], Loss: 0.1599\n",
      "Epoch [7/50], Step [570/777], Loss: 0.2320\n",
      "Epoch [7/50], Step [580/777], Loss: 0.3314\n",
      "Epoch [7/50], Step [590/777], Loss: 0.0271\n",
      "Epoch [7/50], Step [600/777], Loss: 0.0164\n",
      "Epoch [7/50], Step [610/777], Loss: 0.0416\n",
      "Epoch [7/50], Step [620/777], Loss: 0.0246\n",
      "Epoch [7/50], Step [630/777], Loss: 0.0369\n",
      "Epoch [7/50], Step [640/777], Loss: 0.2808\n",
      "Epoch [7/50], Step [650/777], Loss: 0.0266\n",
      "Epoch [7/50], Step [660/777], Loss: 0.1479\n",
      "Epoch [7/50], Step [670/777], Loss: 0.2483\n",
      "Epoch [7/50], Step [680/777], Loss: 0.0176\n",
      "Epoch [7/50], Step [690/777], Loss: 0.0322\n",
      "Epoch [7/50], Step [700/777], Loss: 0.2445\n",
      "Epoch [7/50], Step [710/777], Loss: 0.0111\n",
      "Epoch [7/50], Step [720/777], Loss: 0.0115\n",
      "Epoch [7/50], Step [730/777], Loss: 0.0118\n",
      "Epoch [7/50], Step [740/777], Loss: 0.3455\n",
      "Epoch [7/50], Step [750/777], Loss: 0.0568\n",
      "Epoch [7/50], Step [760/777], Loss: 0.0329\n",
      "Epoch [7/50], Step [770/777], Loss: 0.0204\n",
      "Epoch [7/50], Train Loss: 0.0938, Val Loss: 0.1792, Val Accuracy: 0.9557\n",
      "Model saved with validation accuracy: 0.9557\n",
      "Epoch [8/50], Step [10/777], Loss: 0.0341\n",
      "Epoch [8/50], Step [20/777], Loss: 0.0069\n",
      "Epoch [8/50], Step [30/777], Loss: 0.0118\n",
      "Epoch [8/50], Step [40/777], Loss: 0.0146\n",
      "Epoch [8/50], Step [50/777], Loss: 0.0074\n",
      "Epoch [8/50], Step [60/777], Loss: 0.2070\n",
      "Epoch [8/50], Step [70/777], Loss: 0.0107\n",
      "Epoch [8/50], Step [80/777], Loss: 0.0326\n",
      "Epoch [8/50], Step [90/777], Loss: 0.0079\n",
      "Epoch [8/50], Step [100/777], Loss: 0.0229\n",
      "Epoch [8/50], Step [110/777], Loss: 0.0281\n",
      "Epoch [8/50], Step [120/777], Loss: 0.2319\n",
      "Epoch [8/50], Step [130/777], Loss: 0.0373\n",
      "Epoch [8/50], Step [140/777], Loss: 0.0160\n",
      "Epoch [8/50], Step [150/777], Loss: 0.0114\n",
      "Epoch [8/50], Step [160/777], Loss: 0.0288\n",
      "Epoch [8/50], Step [170/777], Loss: 0.0189\n",
      "Epoch [8/50], Step [180/777], Loss: 0.0181\n",
      "Epoch [8/50], Step [190/777], Loss: 0.0077\n",
      "Epoch [8/50], Step [200/777], Loss: 0.0171\n",
      "Epoch [8/50], Step [210/777], Loss: 0.0983\n",
      "Epoch [8/50], Step [220/777], Loss: 0.0176\n",
      "Epoch [8/50], Step [230/777], Loss: 0.1924\n",
      "Epoch [8/50], Step [240/777], Loss: 0.0544\n",
      "Epoch [8/50], Step [250/777], Loss: 0.0058\n",
      "Epoch [8/50], Step [260/777], Loss: 0.0093\n",
      "Epoch [8/50], Step [270/777], Loss: 0.0215\n",
      "Epoch [8/50], Step [280/777], Loss: 0.0071\n",
      "Epoch [8/50], Step [290/777], Loss: 0.0170\n",
      "Epoch [8/50], Step [300/777], Loss: 0.0102\n",
      "Epoch [8/50], Step [310/777], Loss: 0.0159\n",
      "Epoch [8/50], Step [320/777], Loss: 0.0232\n",
      "Epoch [8/50], Step [330/777], Loss: 0.4086\n",
      "Epoch [8/50], Step [340/777], Loss: 0.1062\n",
      "Epoch [8/50], Step [350/777], Loss: 0.1816\n",
      "Epoch [8/50], Step [360/777], Loss: 0.1785\n",
      "Epoch [8/50], Step [370/777], Loss: 0.1045\n",
      "Epoch [8/50], Step [380/777], Loss: 0.0497\n",
      "Epoch [8/50], Step [390/777], Loss: 0.0589\n",
      "Epoch [8/50], Step [400/777], Loss: 0.0361\n",
      "Epoch [8/50], Step [410/777], Loss: 0.0032\n",
      "Epoch [8/50], Step [420/777], Loss: 0.0163\n",
      "Epoch [8/50], Step [430/777], Loss: 0.2317\n",
      "Epoch [8/50], Step [440/777], Loss: 0.0616\n",
      "Epoch [8/50], Step [450/777], Loss: 0.0051\n",
      "Epoch [8/50], Step [460/777], Loss: 0.2776\n",
      "Epoch [8/50], Step [470/777], Loss: 0.0163\n",
      "Epoch [8/50], Step [480/777], Loss: 0.0573\n",
      "Epoch [8/50], Step [490/777], Loss: 0.0043\n",
      "Epoch [8/50], Step [500/777], Loss: 0.0120\n",
      "Epoch [8/50], Step [510/777], Loss: 0.0103\n",
      "Epoch [8/50], Step [520/777], Loss: 0.0199\n",
      "Epoch [8/50], Step [530/777], Loss: 0.0093\n",
      "Epoch [8/50], Step [540/777], Loss: 0.0211\n",
      "Epoch [8/50], Step [550/777], Loss: 0.0100\n",
      "Epoch [8/50], Step [560/777], Loss: 0.2997\n",
      "Epoch [8/50], Step [570/777], Loss: 0.0125\n",
      "Epoch [8/50], Step [580/777], Loss: 0.0149\n",
      "Epoch [8/50], Step [590/777], Loss: 0.0084\n",
      "Epoch [8/50], Step [600/777], Loss: 0.1571\n",
      "Epoch [8/50], Step [610/777], Loss: 0.0192\n",
      "Epoch [8/50], Step [620/777], Loss: 0.0248\n",
      "Epoch [8/50], Step [630/777], Loss: 0.0040\n",
      "Epoch [8/50], Step [640/777], Loss: 0.3362\n",
      "Epoch [8/50], Step [650/777], Loss: 0.0077\n",
      "Epoch [8/50], Step [660/777], Loss: 0.0545\n",
      "Epoch [8/50], Step [670/777], Loss: 0.1050\n",
      "Epoch [8/50], Step [680/777], Loss: 0.0194\n",
      "Epoch [8/50], Step [690/777], Loss: 0.2322\n",
      "Epoch [8/50], Step [700/777], Loss: 0.0143\n",
      "Epoch [8/50], Step [710/777], Loss: 0.3135\n",
      "Epoch [8/50], Step [720/777], Loss: 0.0134\n",
      "Epoch [8/50], Step [730/777], Loss: 0.0800\n",
      "Epoch [8/50], Step [740/777], Loss: 0.0032\n",
      "Epoch [8/50], Step [750/777], Loss: 0.1908\n",
      "Epoch [8/50], Step [760/777], Loss: 0.0948\n",
      "Epoch [8/50], Step [770/777], Loss: 0.0150\n",
      "Epoch [8/50], Train Loss: 0.0665, Val Loss: 0.1900, Val Accuracy: 0.9545\n",
      "Epoch [9/50], Step [10/777], Loss: 0.0144\n",
      "Epoch [9/50], Step [20/777], Loss: 0.0098\n",
      "Epoch [9/50], Step [30/777], Loss: 0.0036\n",
      "Epoch [9/50], Step [40/777], Loss: 0.0044\n",
      "Epoch [9/50], Step [50/777], Loss: 0.1286\n",
      "Epoch [9/50], Step [60/777], Loss: 0.0342\n",
      "Epoch [9/50], Step [70/777], Loss: 0.0358\n",
      "Epoch [9/50], Step [80/777], Loss: 0.1780\n",
      "Epoch [9/50], Step [90/777], Loss: 0.0129\n",
      "Epoch [9/50], Step [100/777], Loss: 0.0047\n",
      "Epoch [9/50], Step [110/777], Loss: 0.0996\n",
      "Epoch [9/50], Step [120/777], Loss: 0.0317\n",
      "Epoch [9/50], Step [130/777], Loss: 0.0067\n",
      "Epoch [9/50], Step [140/777], Loss: 0.0117\n",
      "Epoch [9/50], Step [150/777], Loss: 0.0087\n",
      "Epoch [9/50], Step [160/777], Loss: 0.0180\n",
      "Epoch [9/50], Step [170/777], Loss: 0.0057\n",
      "Epoch [9/50], Step [180/777], Loss: 0.0050\n",
      "Epoch [9/50], Step [190/777], Loss: 0.2175\n",
      "Epoch [9/50], Step [200/777], Loss: 0.0856\n",
      "Epoch [9/50], Step [210/777], Loss: 0.2195\n",
      "Epoch [9/50], Step [220/777], Loss: 0.0077\n",
      "Epoch [9/50], Step [230/777], Loss: 0.0402\n",
      "Epoch [9/50], Step [240/777], Loss: 0.0060\n",
      "Epoch [9/50], Step [250/777], Loss: 0.0480\n",
      "Epoch [9/50], Step [260/777], Loss: 0.0783\n",
      "Epoch [9/50], Step [270/777], Loss: 0.2036\n",
      "Epoch [9/50], Step [280/777], Loss: 0.0031\n",
      "Epoch [9/50], Step [290/777], Loss: 0.0045\n",
      "Epoch [9/50], Step [300/777], Loss: 0.0088\n",
      "Epoch [9/50], Step [310/777], Loss: 0.1694\n",
      "Epoch [9/50], Step [320/777], Loss: 0.0118\n",
      "Epoch [9/50], Step [330/777], Loss: 0.0470\n",
      "Epoch [9/50], Step [340/777], Loss: 0.0070\n",
      "Epoch [9/50], Step [350/777], Loss: 0.0054\n",
      "Epoch [9/50], Step [360/777], Loss: 0.2791\n",
      "Epoch [9/50], Step [370/777], Loss: 0.0080\n",
      "Epoch [9/50], Step [380/777], Loss: 0.1216\n",
      "Epoch [9/50], Step [390/777], Loss: 0.0146\n",
      "Epoch [9/50], Step [400/777], Loss: 0.0066\n",
      "Epoch [9/50], Step [410/777], Loss: 0.1275\n",
      "Epoch [9/50], Step [420/777], Loss: 0.0285\n",
      "Epoch [9/50], Step [430/777], Loss: 0.0473\n",
      "Epoch [9/50], Step [440/777], Loss: 0.0213\n",
      "Epoch [9/50], Step [450/777], Loss: 0.0036\n",
      "Epoch [9/50], Step [460/777], Loss: 0.0133\n",
      "Epoch [9/50], Step [470/777], Loss: 0.0090\n",
      "Epoch [9/50], Step [480/777], Loss: 0.0135\n",
      "Epoch [9/50], Step [490/777], Loss: 0.0123\n",
      "Epoch [9/50], Step [500/777], Loss: 0.0519\n",
      "Epoch [9/50], Step [510/777], Loss: 0.0084\n",
      "Epoch [9/50], Step [520/777], Loss: 0.1234\n",
      "Epoch [9/50], Step [530/777], Loss: 0.0013\n",
      "Epoch [9/50], Step [540/777], Loss: 0.0546\n",
      "Epoch [9/50], Step [550/777], Loss: 0.0021\n",
      "Epoch [9/50], Step [560/777], Loss: 0.0021\n",
      "Epoch [9/50], Step [570/777], Loss: 0.1072\n",
      "Epoch [9/50], Step [580/777], Loss: 0.0040\n",
      "Epoch [9/50], Step [590/777], Loss: 0.0396\n",
      "Epoch [9/50], Step [600/777], Loss: 0.5848\n",
      "Epoch [9/50], Step [610/777], Loss: 0.0915\n",
      "Epoch [9/50], Step [620/777], Loss: 0.0215\n",
      "Epoch [9/50], Step [630/777], Loss: 0.0050\n",
      "Epoch [9/50], Step [640/777], Loss: 0.0051\n",
      "Epoch [9/50], Step [650/777], Loss: 0.0033\n",
      "Epoch [9/50], Step [660/777], Loss: 0.0039\n",
      "Epoch [9/50], Step [670/777], Loss: 0.2232\n",
      "Epoch [9/50], Step [680/777], Loss: 0.3936\n",
      "Epoch [9/50], Step [690/777], Loss: 0.0182\n",
      "Epoch [9/50], Step [700/777], Loss: 0.0012\n",
      "Epoch [9/50], Step [710/777], Loss: 0.0032\n",
      "Epoch [9/50], Step [720/777], Loss: 0.0468\n",
      "Epoch [9/50], Step [730/777], Loss: 0.0054\n",
      "Epoch [9/50], Step [740/777], Loss: 0.0435\n",
      "Epoch [9/50], Step [750/777], Loss: 0.0204\n",
      "Epoch [9/50], Step [760/777], Loss: 0.0073\n",
      "Epoch [9/50], Step [770/777], Loss: 0.0065\n",
      "Epoch [9/50], Train Loss: 0.0446, Val Loss: 0.2163, Val Accuracy: 0.9545\n",
      "Epoch [10/50], Step [10/777], Loss: 0.0292\n",
      "Epoch [10/50], Step [20/777], Loss: 0.0165\n",
      "Epoch [10/50], Step [30/777], Loss: 0.0677\n",
      "Epoch [10/50], Step [40/777], Loss: 0.0036\n",
      "Epoch [10/50], Step [50/777], Loss: 0.0085\n",
      "Epoch [10/50], Step [60/777], Loss: 0.0047\n",
      "Epoch [10/50], Step [70/777], Loss: 0.0034\n",
      "Epoch [10/50], Step [80/777], Loss: 0.0032\n",
      "Epoch [10/50], Step [90/777], Loss: 0.0817\n",
      "Epoch [10/50], Step [100/777], Loss: 0.0137\n",
      "Epoch [10/50], Step [110/777], Loss: 0.0316\n",
      "Epoch [10/50], Step [120/777], Loss: 0.0194\n",
      "Epoch [10/50], Step [130/777], Loss: 0.0029\n",
      "Epoch [10/50], Step [140/777], Loss: 0.0178\n",
      "Epoch [10/50], Step [150/777], Loss: 0.0063\n",
      "Epoch [10/50], Step [160/777], Loss: 0.0068\n",
      "Epoch [10/50], Step [170/777], Loss: 0.0021\n",
      "Epoch [10/50], Step [180/777], Loss: 0.0288\n",
      "Epoch [10/50], Step [190/777], Loss: 0.0065\n",
      "Epoch [10/50], Step [200/777], Loss: 0.0403\n",
      "Epoch [10/50], Step [210/777], Loss: 0.0039\n",
      "Epoch [10/50], Step [220/777], Loss: 0.0044\n",
      "Epoch [10/50], Step [230/777], Loss: 0.0045\n",
      "Epoch [10/50], Step [240/777], Loss: 0.0023\n",
      "Epoch [10/50], Step [250/777], Loss: 0.0132\n",
      "Epoch [10/50], Step [260/777], Loss: 0.0162\n",
      "Epoch [10/50], Step [270/777], Loss: 0.0007\n",
      "Epoch [10/50], Step [280/777], Loss: 0.0100\n",
      "Epoch [10/50], Step [290/777], Loss: 0.0020\n",
      "Epoch [10/50], Step [300/777], Loss: 0.0301\n",
      "Epoch [10/50], Step [310/777], Loss: 0.0031\n",
      "Epoch [10/50], Step [320/777], Loss: 0.0246\n",
      "Epoch [10/50], Step [330/777], Loss: 0.0041\n",
      "Epoch [10/50], Step [340/777], Loss: 0.0029\n",
      "Epoch [10/50], Step [350/777], Loss: 0.1132\n",
      "Epoch [10/50], Step [360/777], Loss: 0.0476\n",
      "Epoch [10/50], Step [370/777], Loss: 0.0298\n",
      "Epoch [10/50], Step [380/777], Loss: 0.1440\n",
      "Epoch [10/50], Step [390/777], Loss: 0.0270\n",
      "Epoch [10/50], Step [400/777], Loss: 0.0071\n",
      "Epoch [10/50], Step [410/777], Loss: 0.0045\n",
      "Epoch [10/50], Step [420/777], Loss: 0.0034\n",
      "Epoch [10/50], Step [430/777], Loss: 0.0071\n",
      "Epoch [10/50], Step [440/777], Loss: 0.0030\n",
      "Epoch [10/50], Step [450/777], Loss: 0.0061\n",
      "Epoch [10/50], Step [460/777], Loss: 0.0981\n",
      "Epoch [10/50], Step [470/777], Loss: 0.0709\n",
      "Epoch [10/50], Step [480/777], Loss: 0.0188\n",
      "Epoch [10/50], Step [490/777], Loss: 0.0166\n",
      "Epoch [10/50], Step [500/777], Loss: 0.0056\n",
      "Epoch [10/50], Step [510/777], Loss: 0.0170\n",
      "Epoch [10/50], Step [520/777], Loss: 0.0106\n",
      "Epoch [10/50], Step [530/777], Loss: 0.0112\n",
      "Epoch [10/50], Step [540/777], Loss: 0.0033\n",
      "Epoch [10/50], Step [550/777], Loss: 0.0046\n",
      "Epoch [10/50], Step [560/777], Loss: 0.0222\n",
      "Epoch [10/50], Step [570/777], Loss: 0.0058\n",
      "Epoch [10/50], Step [580/777], Loss: 0.0364\n",
      "Epoch [10/50], Step [590/777], Loss: 0.0715\n",
      "Epoch [10/50], Step [600/777], Loss: 0.0119\n",
      "Epoch [10/50], Step [610/777], Loss: 0.0714\n",
      "Epoch [10/50], Step [620/777], Loss: 0.0087\n",
      "Epoch [10/50], Step [630/777], Loss: 0.0142\n",
      "Epoch [10/50], Step [640/777], Loss: 0.0060\n",
      "Epoch [10/50], Step [650/777], Loss: 0.0044\n",
      "Epoch [10/50], Step [660/777], Loss: 0.0334\n",
      "Epoch [10/50], Step [670/777], Loss: 0.0024\n",
      "Epoch [10/50], Step [680/777], Loss: 0.0029\n",
      "Epoch [10/50], Step [690/777], Loss: 0.0021\n",
      "Epoch [10/50], Step [700/777], Loss: 0.0054\n",
      "Epoch [10/50], Step [710/777], Loss: 0.0036\n",
      "Epoch [10/50], Step [720/777], Loss: 0.0026\n",
      "Epoch [10/50], Step [730/777], Loss: 0.0056\n",
      "Epoch [10/50], Step [740/777], Loss: 0.1648\n",
      "Epoch [10/50], Step [750/777], Loss: 0.0011\n",
      "Epoch [10/50], Step [760/777], Loss: 0.0881\n",
      "Epoch [10/50], Step [770/777], Loss: 0.0016\n",
      "Epoch [10/50], Train Loss: 0.0325, Val Loss: 0.2464, Val Accuracy: 0.9553\n",
      "Epoch [11/50], Step [10/777], Loss: 0.0041\n",
      "Epoch [11/50], Step [20/777], Loss: 0.1283\n",
      "Epoch [11/50], Step [30/777], Loss: 0.0065\n",
      "Epoch [11/50], Step [40/777], Loss: 0.0031\n",
      "Epoch [11/50], Step [50/777], Loss: 0.2591\n",
      "Epoch [11/50], Step [60/777], Loss: 0.0066\n",
      "Epoch [11/50], Step [70/777], Loss: 0.0318\n",
      "Epoch [11/50], Step [80/777], Loss: 0.0021\n",
      "Epoch [11/50], Step [90/777], Loss: 0.0745\n",
      "Epoch [11/50], Step [100/777], Loss: 0.0110\n",
      "Epoch [11/50], Step [110/777], Loss: 0.0043\n",
      "Epoch [11/50], Step [120/777], Loss: 0.0021\n",
      "Epoch [11/50], Step [130/777], Loss: 0.0024\n",
      "Epoch [11/50], Step [140/777], Loss: 0.0036\n",
      "Epoch [11/50], Step [150/777], Loss: 0.2444\n",
      "Epoch [11/50], Step [160/777], Loss: 0.0199\n",
      "Epoch [11/50], Step [170/777], Loss: 0.0136\n",
      "Epoch [11/50], Step [180/777], Loss: 0.0054\n",
      "Epoch [11/50], Step [190/777], Loss: 0.0012\n",
      "Epoch [11/50], Step [200/777], Loss: 0.0430\n",
      "Epoch [11/50], Step [210/777], Loss: 0.0116\n",
      "Epoch [11/50], Step [220/777], Loss: 0.0194\n",
      "Epoch [11/50], Step [230/777], Loss: 0.0048\n",
      "Epoch [11/50], Step [240/777], Loss: 0.0155\n",
      "Epoch [11/50], Step [250/777], Loss: 0.0067\n",
      "Epoch [11/50], Step [260/777], Loss: 0.0035\n",
      "Epoch [11/50], Step [270/777], Loss: 0.0011\n",
      "Epoch [11/50], Step [280/777], Loss: 0.1544\n",
      "Epoch [11/50], Step [290/777], Loss: 0.0015\n",
      "Epoch [11/50], Step [300/777], Loss: 0.0020\n",
      "Epoch [11/50], Step [310/777], Loss: 0.0128\n",
      "Epoch [11/50], Step [320/777], Loss: 0.0233\n",
      "Epoch [11/50], Step [330/777], Loss: 0.0034\n",
      "Epoch [11/50], Step [340/777], Loss: 0.0035\n",
      "Epoch [11/50], Step [350/777], Loss: 0.3386\n",
      "Epoch [11/50], Step [360/777], Loss: 0.0022\n",
      "Epoch [11/50], Step [370/777], Loss: 0.0086\n",
      "Epoch [11/50], Step [380/777], Loss: 0.0040\n",
      "Epoch [11/50], Step [390/777], Loss: 0.0060\n",
      "Epoch [11/50], Step [400/777], Loss: 0.0051\n",
      "Epoch [11/50], Step [410/777], Loss: 0.0025\n",
      "Epoch [11/50], Step [420/777], Loss: 0.0027\n",
      "Epoch [11/50], Step [430/777], Loss: 0.0023\n",
      "Epoch [11/50], Step [440/777], Loss: 0.0061\n",
      "Epoch [11/50], Step [450/777], Loss: 0.0053\n",
      "Epoch [11/50], Step [460/777], Loss: 0.0013\n",
      "Epoch [11/50], Step [470/777], Loss: 0.0266\n",
      "Epoch [11/50], Step [480/777], Loss: 0.0247\n",
      "Epoch [11/50], Step [490/777], Loss: 0.0263\n",
      "Epoch [11/50], Step [500/777], Loss: 0.0048\n",
      "Epoch [11/50], Step [510/777], Loss: 0.0216\n",
      "Epoch [11/50], Step [520/777], Loss: 0.0013\n",
      "Epoch [11/50], Step [530/777], Loss: 0.0963\n",
      "Epoch [11/50], Step [540/777], Loss: 0.0014\n",
      "Epoch [11/50], Step [550/777], Loss: 0.0017\n",
      "Epoch [11/50], Step [560/777], Loss: 0.0034\n",
      "Epoch [11/50], Step [570/777], Loss: 0.0099\n",
      "Epoch [11/50], Step [580/777], Loss: 0.0016\n",
      "Epoch [11/50], Step [590/777], Loss: 0.0046\n",
      "Epoch [11/50], Step [600/777], Loss: 0.0349\n",
      "Epoch [11/50], Step [610/777], Loss: 0.0012\n",
      "Epoch [11/50], Step [620/777], Loss: 0.0085\n",
      "Epoch [11/50], Step [630/777], Loss: 0.0019\n",
      "Epoch [11/50], Step [640/777], Loss: 0.0150\n",
      "Epoch [11/50], Step [650/777], Loss: 0.0790\n",
      "Epoch [11/50], Step [660/777], Loss: 0.1131\n",
      "Epoch [11/50], Step [670/777], Loss: 0.0347\n",
      "Epoch [11/50], Step [680/777], Loss: 0.0056\n",
      "Epoch [11/50], Step [690/777], Loss: 0.0015\n",
      "Epoch [11/50], Step [700/777], Loss: 0.0040\n",
      "Epoch [11/50], Step [710/777], Loss: 0.0033\n",
      "Epoch [11/50], Step [720/777], Loss: 0.0112\n",
      "Epoch [11/50], Step [730/777], Loss: 0.0906\n",
      "Epoch [11/50], Step [740/777], Loss: 0.0435\n",
      "Epoch [11/50], Step [750/777], Loss: 0.0015\n",
      "Epoch [11/50], Step [760/777], Loss: 0.0225\n",
      "Epoch [11/50], Step [770/777], Loss: 0.0078\n",
      "Epoch [11/50], Train Loss: 0.0180, Val Loss: 0.2816, Val Accuracy: 0.9542\n",
      "Epoch [12/50], Step [10/777], Loss: 0.0007\n",
      "Epoch [12/50], Step [20/777], Loss: 0.0075\n",
      "Epoch [12/50], Step [30/777], Loss: 0.0070\n",
      "Epoch [12/50], Step [40/777], Loss: 0.0104\n",
      "Epoch [12/50], Step [50/777], Loss: 0.0217\n",
      "Epoch [12/50], Step [60/777], Loss: 0.0010\n",
      "Epoch [12/50], Step [70/777], Loss: 0.0086\n",
      "Epoch [12/50], Step [80/777], Loss: 0.0011\n",
      "Epoch [12/50], Step [90/777], Loss: 0.0071\n",
      "Epoch [12/50], Step [100/777], Loss: 0.0309\n",
      "Epoch [12/50], Step [110/777], Loss: 0.0009\n",
      "Epoch [12/50], Step [120/777], Loss: 0.0017\n",
      "Epoch [12/50], Step [130/777], Loss: 0.0017\n",
      "Epoch [12/50], Step [140/777], Loss: 0.0187\n",
      "Epoch [12/50], Step [150/777], Loss: 0.0010\n",
      "Epoch [12/50], Step [160/777], Loss: 0.0018\n",
      "Epoch [12/50], Step [170/777], Loss: 0.0080\n",
      "Epoch [12/50], Step [180/777], Loss: 0.0034\n",
      "Epoch [12/50], Step [190/777], Loss: 0.0020\n",
      "Epoch [12/50], Step [200/777], Loss: 0.0202\n",
      "Epoch [12/50], Step [210/777], Loss: 0.2877\n",
      "Epoch [12/50], Step [220/777], Loss: 0.0012\n",
      "Epoch [12/50], Step [230/777], Loss: 0.0211\n",
      "Epoch [12/50], Step [240/777], Loss: 0.0040\n",
      "Epoch [12/50], Step [250/777], Loss: 0.0005\n",
      "Epoch [12/50], Step [260/777], Loss: 0.0055\n",
      "Epoch [12/50], Step [270/777], Loss: 0.0030\n",
      "Epoch [12/50], Step [280/777], Loss: 0.0028\n",
      "Epoch [12/50], Step [290/777], Loss: 0.0094\n",
      "Epoch [12/50], Step [300/777], Loss: 0.0032\n",
      "Epoch [12/50], Step [310/777], Loss: 0.0014\n",
      "Epoch [12/50], Step [320/777], Loss: 0.0023\n",
      "Epoch [12/50], Step [330/777], Loss: 0.0030\n",
      "Epoch [12/50], Step [340/777], Loss: 0.0045\n",
      "Epoch [12/50], Step [350/777], Loss: 0.0017\n",
      "Epoch [12/50], Step [360/777], Loss: 0.0044\n",
      "Epoch [12/50], Step [370/777], Loss: 0.0086\n",
      "Epoch [12/50], Step [380/777], Loss: 0.0042\n",
      "Epoch [12/50], Step [390/777], Loss: 0.0026\n",
      "Epoch [12/50], Step [400/777], Loss: 0.0023\n",
      "Epoch [12/50], Step [410/777], Loss: 0.0015\n",
      "Epoch [12/50], Step [420/777], Loss: 0.1177\n",
      "Epoch [12/50], Step [430/777], Loss: 0.0090\n",
      "Epoch [12/50], Step [440/777], Loss: 0.0035\n",
      "Epoch [12/50], Step [450/777], Loss: 0.0035\n",
      "Epoch [12/50], Step [460/777], Loss: 0.0496\n",
      "Epoch [12/50], Step [470/777], Loss: 0.0025\n",
      "Epoch [12/50], Step [480/777], Loss: 0.0020\n",
      "Epoch [12/50], Step [490/777], Loss: 0.0093\n",
      "Epoch [12/50], Step [500/777], Loss: 0.0012\n",
      "Epoch [12/50], Step [510/777], Loss: 0.0010\n",
      "Epoch [12/50], Step [520/777], Loss: 0.0012\n",
      "Epoch [12/50], Step [530/777], Loss: 0.0009\n",
      "Epoch [12/50], Step [540/777], Loss: 0.0027\n",
      "Epoch [12/50], Step [550/777], Loss: 0.0056\n",
      "Epoch [12/50], Step [560/777], Loss: 0.0062\n",
      "Epoch [12/50], Step [570/777], Loss: 0.0047\n",
      "Epoch [12/50], Step [580/777], Loss: 0.0033\n",
      "Epoch [12/50], Step [590/777], Loss: 0.0041\n",
      "Epoch [12/50], Step [600/777], Loss: 0.0005\n",
      "Epoch [12/50], Step [610/777], Loss: 0.0046\n",
      "Epoch [12/50], Step [620/777], Loss: 0.0096\n",
      "Epoch [12/50], Step [630/777], Loss: 0.0051\n",
      "Epoch [12/50], Step [640/777], Loss: 0.0023\n",
      "Epoch [12/50], Step [650/777], Loss: 0.0201\n",
      "Epoch [12/50], Step [660/777], Loss: 0.0036\n",
      "Epoch [12/50], Step [670/777], Loss: 0.0010\n",
      "Epoch [12/50], Step [680/777], Loss: 0.0042\n",
      "Epoch [12/50], Step [690/777], Loss: 0.0020\n",
      "Epoch [12/50], Step [700/777], Loss: 0.0018\n",
      "Epoch [12/50], Step [710/777], Loss: 0.0026\n",
      "Epoch [12/50], Step [720/777], Loss: 0.0030\n",
      "Epoch [12/50], Step [730/777], Loss: 0.0019\n",
      "Epoch [12/50], Step [740/777], Loss: 0.0030\n",
      "Epoch [12/50], Step [750/777], Loss: 0.0007\n",
      "Epoch [12/50], Step [760/777], Loss: 0.0655\n",
      "Epoch [12/50], Step [770/777], Loss: 0.0032\n",
      "Epoch [12/50], Train Loss: 0.0114, Val Loss: 0.2769, Val Accuracy: 0.9530\n",
      "Early stopping triggered after 12 epochs\n",
      "\n",
      "Evaluating ResNet-50 on test set...\n",
      "\n",
      "ResNet-50 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DJI       0.97      0.86      0.92       200\n",
      "   FutabaT14       0.96      0.89      0.92       548\n",
      "    FutabaT7       0.99      0.92      0.96        93\n",
      "    Graupner       1.00      0.97      0.99       107\n",
      "       Noise       0.93      0.99      0.96      1314\n",
      "     Taranis       1.00      0.99      0.99       268\n",
      "     Turnigy       0.99      0.96      0.98       133\n",
      "\n",
      "    accuracy                           0.95      2663\n",
      "   macro avg       0.98      0.94      0.96      2663\n",
      "weighted avg       0.95      0.95      0.95      2663\n",
      "\n",
      "\n",
      "ResNet-50 Multi-class ROC AUC Score: 0.9906\n",
      "\n",
      "Analyzing ResNet-50 performance by SNR levels...\n",
      "\n",
      "ResNet-50 Performance by SNR level:\n",
      "SNR (dB) | Accuracy | Samples\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ResNet-50 Total Number of Parameters: 23,522,375\n",
      "ResNet-50 Average Inference Time per Sample: 5.656 ms\n",
      "ResNet-50 FLOPs: 4,131,708,928.0 (4.13 G)\n",
      "ResNet-50 MACs: 23,522,375.0 (23.52 M)\n",
      "\n",
      "✅ ResNet-50 Accuracy for class 'DJI': 86.50%\n",
      "✅ ResNet-50 Accuracy for class 'FutabaT14': 89.23%\n",
      "✅ ResNet-50 Accuracy for class 'FutabaT7': 92.47%\n",
      "✅ ResNet-50 Accuracy for class 'Graupner': 97.20%\n",
      "✅ ResNet-50 Accuracy for class 'Noise': 98.63%\n",
      "✅ ResNet-50 Accuracy for class 'Taranis': 98.51%\n",
      "✅ ResNet-50 Accuracy for class 'Turnigy': 96.24%\n",
      "\n",
      "✅ ResNet-50 Test Set Accuracy: 0.954\n",
      "📊 ResNet-50 Model Size: 89.73 MB\n",
      "\n",
      "Key characteristics of ResNet-50:\n",
      "- Deep residual network with skip connections to solve the vanishing gradient problem\n",
      "- 50-layer architecture with bottleneck blocks\n",
      "- Well-established architecture with strong performance\n",
      "- Uses batch normalization after each convolutional layer\n",
      "- Popular backbone for many computer vision tasks\n",
      "\n",
      "Metrics saved to resnet_50_drone_rf_metrics.json\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING MOBILENETV2\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13.6M/13.6M [00:00<00:00, 144MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training and Evaluating MobileNetV2\n",
      "==================================================\n",
      "\n",
      "MobileNetV2 Summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]             864\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "             ReLU6-3         [-1, 32, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]             288\n",
      "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
      "             ReLU6-6         [-1, 32, 112, 112]               0\n",
      "            Conv2d-7         [-1, 16, 112, 112]             512\n",
      "       BatchNorm2d-8         [-1, 16, 112, 112]              32\n",
      "  InvertedResidual-9         [-1, 16, 112, 112]               0\n",
      "           Conv2d-10         [-1, 96, 112, 112]           1,536\n",
      "      BatchNorm2d-11         [-1, 96, 112, 112]             192\n",
      "            ReLU6-12         [-1, 96, 112, 112]               0\n",
      "           Conv2d-13           [-1, 96, 56, 56]             864\n",
      "      BatchNorm2d-14           [-1, 96, 56, 56]             192\n",
      "            ReLU6-15           [-1, 96, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      " InvertedResidual-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-20          [-1, 144, 56, 56]             288\n",
      "            ReLU6-21          [-1, 144, 56, 56]               0\n",
      "           Conv2d-22          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-23          [-1, 144, 56, 56]             288\n",
      "            ReLU6-24          [-1, 144, 56, 56]               0\n",
      "           Conv2d-25           [-1, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-26           [-1, 24, 56, 56]              48\n",
      " InvertedResidual-27           [-1, 24, 56, 56]               0\n",
      "           Conv2d-28          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-29          [-1, 144, 56, 56]             288\n",
      "            ReLU6-30          [-1, 144, 56, 56]               0\n",
      "           Conv2d-31          [-1, 144, 28, 28]           1,296\n",
      "      BatchNorm2d-32          [-1, 144, 28, 28]             288\n",
      "            ReLU6-33          [-1, 144, 28, 28]               0\n",
      "           Conv2d-34           [-1, 32, 28, 28]           4,608\n",
      "      BatchNorm2d-35           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-36           [-1, 32, 28, 28]               0\n",
      "           Conv2d-37          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-38          [-1, 192, 28, 28]             384\n",
      "            ReLU6-39          [-1, 192, 28, 28]               0\n",
      "           Conv2d-40          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-41          [-1, 192, 28, 28]             384\n",
      "            ReLU6-42          [-1, 192, 28, 28]               0\n",
      "           Conv2d-43           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-44           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-45           [-1, 32, 28, 28]               0\n",
      "           Conv2d-46          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-47          [-1, 192, 28, 28]             384\n",
      "            ReLU6-48          [-1, 192, 28, 28]               0\n",
      "           Conv2d-49          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-50          [-1, 192, 28, 28]             384\n",
      "            ReLU6-51          [-1, 192, 28, 28]               0\n",
      "           Conv2d-52           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-53           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-54           [-1, 32, 28, 28]               0\n",
      "           Conv2d-55          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-56          [-1, 192, 28, 28]             384\n",
      "            ReLU6-57          [-1, 192, 28, 28]               0\n",
      "           Conv2d-58          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-59          [-1, 192, 14, 14]             384\n",
      "            ReLU6-60          [-1, 192, 14, 14]               0\n",
      "           Conv2d-61           [-1, 64, 14, 14]          12,288\n",
      "      BatchNorm2d-62           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-63           [-1, 64, 14, 14]               0\n",
      "           Conv2d-64          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-65          [-1, 384, 14, 14]             768\n",
      "            ReLU6-66          [-1, 384, 14, 14]               0\n",
      "           Conv2d-67          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-68          [-1, 384, 14, 14]             768\n",
      "            ReLU6-69          [-1, 384, 14, 14]               0\n",
      "           Conv2d-70           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-71           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-72           [-1, 64, 14, 14]               0\n",
      "           Conv2d-73          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-74          [-1, 384, 14, 14]             768\n",
      "            ReLU6-75          [-1, 384, 14, 14]               0\n",
      "           Conv2d-76          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-77          [-1, 384, 14, 14]             768\n",
      "            ReLU6-78          [-1, 384, 14, 14]               0\n",
      "           Conv2d-79           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-80           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-81           [-1, 64, 14, 14]               0\n",
      "           Conv2d-82          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-83          [-1, 384, 14, 14]             768\n",
      "            ReLU6-84          [-1, 384, 14, 14]               0\n",
      "           Conv2d-85          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-86          [-1, 384, 14, 14]             768\n",
      "            ReLU6-87          [-1, 384, 14, 14]               0\n",
      "           Conv2d-88           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-89           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-90           [-1, 64, 14, 14]               0\n",
      "           Conv2d-91          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-92          [-1, 384, 14, 14]             768\n",
      "            ReLU6-93          [-1, 384, 14, 14]               0\n",
      "           Conv2d-94          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-95          [-1, 384, 14, 14]             768\n",
      "            ReLU6-96          [-1, 384, 14, 14]               0\n",
      "           Conv2d-97           [-1, 96, 14, 14]          36,864\n",
      "      BatchNorm2d-98           [-1, 96, 14, 14]             192\n",
      " InvertedResidual-99           [-1, 96, 14, 14]               0\n",
      "          Conv2d-100          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-101          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-102          [-1, 576, 14, 14]               0\n",
      "          Conv2d-103          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-104          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-105          [-1, 576, 14, 14]               0\n",
      "          Conv2d-106           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-107           [-1, 96, 14, 14]             192\n",
      "InvertedResidual-108           [-1, 96, 14, 14]               0\n",
      "          Conv2d-109          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-110          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-111          [-1, 576, 14, 14]               0\n",
      "          Conv2d-112          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-113          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-114          [-1, 576, 14, 14]               0\n",
      "          Conv2d-115           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-116           [-1, 96, 14, 14]             192\n",
      "InvertedResidual-117           [-1, 96, 14, 14]               0\n",
      "          Conv2d-118          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-119          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-120          [-1, 576, 14, 14]               0\n",
      "          Conv2d-121            [-1, 576, 7, 7]           5,184\n",
      "     BatchNorm2d-122            [-1, 576, 7, 7]           1,152\n",
      "           ReLU6-123            [-1, 576, 7, 7]               0\n",
      "          Conv2d-124            [-1, 160, 7, 7]          92,160\n",
      "     BatchNorm2d-125            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-126            [-1, 160, 7, 7]               0\n",
      "          Conv2d-127            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-128            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-129            [-1, 960, 7, 7]               0\n",
      "          Conv2d-130            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-131            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-132            [-1, 960, 7, 7]               0\n",
      "          Conv2d-133            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-134            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-135            [-1, 160, 7, 7]               0\n",
      "          Conv2d-136            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-137            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-138            [-1, 960, 7, 7]               0\n",
      "          Conv2d-139            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-140            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-141            [-1, 960, 7, 7]               0\n",
      "          Conv2d-142            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-143            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-144            [-1, 160, 7, 7]               0\n",
      "          Conv2d-145            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-146            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-147            [-1, 960, 7, 7]               0\n",
      "          Conv2d-148            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-149            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-150            [-1, 960, 7, 7]               0\n",
      "          Conv2d-151            [-1, 320, 7, 7]         307,200\n",
      "     BatchNorm2d-152            [-1, 320, 7, 7]             640\n",
      "InvertedResidual-153            [-1, 320, 7, 7]               0\n",
      "          Conv2d-154           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-155           [-1, 1280, 7, 7]           2,560\n",
      "           ReLU6-156           [-1, 1280, 7, 7]               0\n",
      "         Dropout-157                 [-1, 1280]               0\n",
      "          Linear-158                    [-1, 7]           8,967\n",
      "================================================================\n",
      "Total params: 2,232,839\n",
      "Trainable params: 2,232,839\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 152.86\n",
      "Params size (MB): 8.52\n",
      "Estimated Total Size (MB): 161.95\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Starting training MobileNetV2...\n",
      "Epoch [1/50], Step [10/777], Loss: 1.6144\n",
      "Epoch [1/50], Step [20/777], Loss: 1.1902\n",
      "Epoch [1/50], Step [30/777], Loss: 1.8142\n",
      "Epoch [1/50], Step [40/777], Loss: 1.1103\n",
      "Epoch [1/50], Step [50/777], Loss: 1.3875\n",
      "Epoch [1/50], Step [60/777], Loss: 0.9388\n",
      "Epoch [1/50], Step [70/777], Loss: 0.8158\n",
      "Epoch [1/50], Step [80/777], Loss: 0.8743\n",
      "Epoch [1/50], Step [90/777], Loss: 0.6502\n",
      "Epoch [1/50], Step [100/777], Loss: 0.9291\n",
      "Epoch [1/50], Step [110/777], Loss: 0.8993\n",
      "Epoch [1/50], Step [120/777], Loss: 0.5743\n",
      "Epoch [1/50], Step [130/777], Loss: 0.7426\n",
      "Epoch [1/50], Step [140/777], Loss: 0.6678\n",
      "Epoch [1/50], Step [150/777], Loss: 0.5224\n",
      "Epoch [1/50], Step [160/777], Loss: 0.6738\n",
      "Epoch [1/50], Step [170/777], Loss: 0.6879\n",
      "Epoch [1/50], Step [180/777], Loss: 0.5566\n",
      "Epoch [1/50], Step [190/777], Loss: 0.6256\n",
      "Epoch [1/50], Step [200/777], Loss: 0.6119\n",
      "Epoch [1/50], Step [210/777], Loss: 0.5872\n",
      "Epoch [1/50], Step [220/777], Loss: 0.3201\n",
      "Epoch [1/50], Step [230/777], Loss: 0.5688\n",
      "Epoch [1/50], Step [240/777], Loss: 0.3438\n",
      "Epoch [1/50], Step [250/777], Loss: 0.5858\n",
      "Epoch [1/50], Step [260/777], Loss: 0.3066\n",
      "Epoch [1/50], Step [270/777], Loss: 0.9425\n",
      "Epoch [1/50], Step [280/777], Loss: 0.1114\n",
      "Epoch [1/50], Step [290/777], Loss: 0.8360\n",
      "Epoch [1/50], Step [300/777], Loss: 0.3169\n",
      "Epoch [1/50], Step [310/777], Loss: 0.5635\n",
      "Epoch [1/50], Step [320/777], Loss: 0.7449\n",
      "Epoch [1/50], Step [330/777], Loss: 0.4663\n",
      "Epoch [1/50], Step [340/777], Loss: 0.1485\n",
      "Epoch [1/50], Step [350/777], Loss: 0.1570\n",
      "Epoch [1/50], Step [360/777], Loss: 0.5237\n",
      "Epoch [1/50], Step [370/777], Loss: 0.7656\n",
      "Epoch [1/50], Step [380/777], Loss: 0.7705\n",
      "Epoch [1/50], Step [390/777], Loss: 0.7596\n",
      "Epoch [1/50], Step [400/777], Loss: 0.3839\n",
      "Epoch [1/50], Step [410/777], Loss: 0.1501\n",
      "Epoch [1/50], Step [420/777], Loss: 0.9004\n",
      "Epoch [1/50], Step [430/777], Loss: 0.5132\n",
      "Epoch [1/50], Step [440/777], Loss: 0.4783\n",
      "Epoch [1/50], Step [450/777], Loss: 0.4800\n",
      "Epoch [1/50], Step [460/777], Loss: 0.5907\n",
      "Epoch [1/50], Step [470/777], Loss: 0.3804\n",
      "Epoch [1/50], Step [480/777], Loss: 0.5087\n",
      "Epoch [1/50], Step [490/777], Loss: 0.2478\n",
      "Epoch [1/50], Step [500/777], Loss: 0.1211\n",
      "Epoch [1/50], Step [510/777], Loss: 0.4739\n",
      "Epoch [1/50], Step [520/777], Loss: 0.2730\n",
      "Epoch [1/50], Step [530/777], Loss: 0.7992\n",
      "Epoch [1/50], Step [540/777], Loss: 0.2683\n",
      "Epoch [1/50], Step [550/777], Loss: 0.1481\n",
      "Epoch [1/50], Step [560/777], Loss: 0.4267\n",
      "Epoch [1/50], Step [570/777], Loss: 0.3132\n",
      "Epoch [1/50], Step [580/777], Loss: 0.5284\n",
      "Epoch [1/50], Step [590/777], Loss: 0.6685\n",
      "Epoch [1/50], Step [600/777], Loss: 0.2179\n",
      "Epoch [1/50], Step [610/777], Loss: 0.3701\n",
      "Epoch [1/50], Step [620/777], Loss: 0.1509\n",
      "Epoch [1/50], Step [630/777], Loss: 0.3900\n",
      "Epoch [1/50], Step [640/777], Loss: 0.3067\n",
      "Epoch [1/50], Step [650/777], Loss: 0.1133\n",
      "Epoch [1/50], Step [660/777], Loss: 0.2319\n",
      "Epoch [1/50], Step [670/777], Loss: 0.1935\n",
      "Epoch [1/50], Step [680/777], Loss: 0.1371\n",
      "Epoch [1/50], Step [690/777], Loss: 0.4856\n",
      "Epoch [1/50], Step [700/777], Loss: 0.5135\n",
      "Epoch [1/50], Step [710/777], Loss: 0.3489\n",
      "Epoch [1/50], Step [720/777], Loss: 0.3309\n",
      "Epoch [1/50], Step [730/777], Loss: 0.4973\n",
      "Epoch [1/50], Step [740/777], Loss: 0.2735\n",
      "Epoch [1/50], Step [750/777], Loss: 0.3573\n",
      "Epoch [1/50], Step [760/777], Loss: 0.5089\n",
      "Epoch [1/50], Step [770/777], Loss: 0.3663\n",
      "Epoch [1/50], Train Loss: 0.5637, Val Loss: 0.2616, Val Accuracy: 0.9309\n",
      "Model saved with validation accuracy: 0.9309\n",
      "Epoch [2/50], Step [10/777], Loss: 0.2985\n",
      "Epoch [2/50], Step [20/777], Loss: 0.0891\n",
      "Epoch [2/50], Step [30/777], Loss: 0.0803\n",
      "Epoch [2/50], Step [40/777], Loss: 0.0861\n",
      "Epoch [2/50], Step [50/777], Loss: 0.2857\n",
      "Epoch [2/50], Step [60/777], Loss: 0.2615\n",
      "Epoch [2/50], Step [70/777], Loss: 0.4036\n",
      "Epoch [2/50], Step [80/777], Loss: 0.3545\n",
      "Epoch [2/50], Step [90/777], Loss: 0.2063\n",
      "Epoch [2/50], Step [100/777], Loss: 0.2809\n",
      "Epoch [2/50], Step [110/777], Loss: 0.2931\n",
      "Epoch [2/50], Step [120/777], Loss: 0.2799\n",
      "Epoch [2/50], Step [130/777], Loss: 0.5702\n",
      "Epoch [2/50], Step [140/777], Loss: 0.1564\n",
      "Epoch [2/50], Step [150/777], Loss: 0.2815\n",
      "Epoch [2/50], Step [160/777], Loss: 0.1842\n",
      "Epoch [2/50], Step [170/777], Loss: 0.3722\n",
      "Epoch [2/50], Step [180/777], Loss: 0.3469\n",
      "Epoch [2/50], Step [190/777], Loss: 0.6421\n",
      "Epoch [2/50], Step [200/777], Loss: 0.1844\n",
      "Epoch [2/50], Step [210/777], Loss: 0.0895\n",
      "Epoch [2/50], Step [220/777], Loss: 0.3262\n",
      "Epoch [2/50], Step [230/777], Loss: 0.0447\n",
      "Epoch [2/50], Step [240/777], Loss: 0.2442\n",
      "Epoch [2/50], Step [250/777], Loss: 0.8352\n",
      "Epoch [2/50], Step [260/777], Loss: 0.0952\n",
      "Epoch [2/50], Step [270/777], Loss: 0.0772\n",
      "Epoch [2/50], Step [280/777], Loss: 0.0877\n",
      "Epoch [2/50], Step [290/777], Loss: 0.0309\n",
      "Epoch [2/50], Step [300/777], Loss: 0.0511\n",
      "Epoch [2/50], Step [310/777], Loss: 0.2341\n",
      "Epoch [2/50], Step [320/777], Loss: 0.0894\n",
      "Epoch [2/50], Step [330/777], Loss: 0.0504\n",
      "Epoch [2/50], Step [340/777], Loss: 0.3127\n",
      "Epoch [2/50], Step [350/777], Loss: 0.2766\n",
      "Epoch [2/50], Step [360/777], Loss: 0.0856\n",
      "Epoch [2/50], Step [370/777], Loss: 0.0702\n",
      "Epoch [2/50], Step [380/777], Loss: 0.3895\n",
      "Epoch [2/50], Step [390/777], Loss: 0.0539\n",
      "Epoch [2/50], Step [400/777], Loss: 0.2599\n",
      "Epoch [2/50], Step [410/777], Loss: 0.1796\n",
      "Epoch [2/50], Step [420/777], Loss: 0.2577\n",
      "Epoch [2/50], Step [430/777], Loss: 0.3245\n",
      "Epoch [2/50], Step [440/777], Loss: 0.3386\n",
      "Epoch [2/50], Step [450/777], Loss: 0.3167\n",
      "Epoch [2/50], Step [460/777], Loss: 0.2940\n",
      "Epoch [2/50], Step [470/777], Loss: 0.2188\n",
      "Epoch [2/50], Step [480/777], Loss: 0.0708\n",
      "Epoch [2/50], Step [490/777], Loss: 0.1291\n",
      "Epoch [2/50], Step [500/777], Loss: 0.3301\n",
      "Epoch [2/50], Step [510/777], Loss: 0.2201\n",
      "Epoch [2/50], Step [520/777], Loss: 0.0544\n",
      "Epoch [2/50], Step [530/777], Loss: 0.0647\n",
      "Epoch [2/50], Step [540/777], Loss: 0.4230\n",
      "Epoch [2/50], Step [550/777], Loss: 0.5161\n",
      "Epoch [2/50], Step [560/777], Loss: 0.1849\n",
      "Epoch [2/50], Step [570/777], Loss: 0.3820\n",
      "Epoch [2/50], Step [580/777], Loss: 0.3569\n",
      "Epoch [2/50], Step [590/777], Loss: 0.3775\n",
      "Epoch [2/50], Step [600/777], Loss: 0.2882\n",
      "Epoch [2/50], Step [610/777], Loss: 0.3911\n",
      "Epoch [2/50], Step [620/777], Loss: 0.5567\n",
      "Epoch [2/50], Step [630/777], Loss: 0.3787\n",
      "Epoch [2/50], Step [640/777], Loss: 0.2616\n",
      "Epoch [2/50], Step [650/777], Loss: 0.0525\n",
      "Epoch [2/50], Step [660/777], Loss: 0.5741\n",
      "Epoch [2/50], Step [670/777], Loss: 1.4081\n",
      "Epoch [2/50], Step [680/777], Loss: 0.1281\n",
      "Epoch [2/50], Step [690/777], Loss: 0.2410\n",
      "Epoch [2/50], Step [700/777], Loss: 0.1011\n",
      "Epoch [2/50], Step [710/777], Loss: 0.4182\n",
      "Epoch [2/50], Step [720/777], Loss: 0.2751\n",
      "Epoch [2/50], Step [730/777], Loss: 0.3156\n",
      "Epoch [2/50], Step [740/777], Loss: 0.0896\n",
      "Epoch [2/50], Step [750/777], Loss: 0.2136\n",
      "Epoch [2/50], Step [760/777], Loss: 0.2833\n",
      "Epoch [2/50], Step [770/777], Loss: 0.5483\n",
      "Epoch [2/50], Train Loss: 0.2865, Val Loss: 0.2269, Val Accuracy: 0.9433\n",
      "Model saved with validation accuracy: 0.9433\n",
      "Epoch [3/50], Step [10/777], Loss: 0.4008\n",
      "Epoch [3/50], Step [20/777], Loss: 0.1417\n",
      "Epoch [3/50], Step [30/777], Loss: 0.2726\n",
      "Epoch [3/50], Step [40/777], Loss: 0.0488\n",
      "Epoch [3/50], Step [50/777], Loss: 0.0268\n",
      "Epoch [3/50], Step [60/777], Loss: 0.0581\n",
      "Epoch [3/50], Step [70/777], Loss: 0.3389\n",
      "Epoch [3/50], Step [80/777], Loss: 0.1631\n",
      "Epoch [3/50], Step [90/777], Loss: 0.0484\n",
      "Epoch [3/50], Step [100/777], Loss: 0.3418\n",
      "Epoch [3/50], Step [110/777], Loss: 0.0655\n",
      "Epoch [3/50], Step [120/777], Loss: 0.1623\n",
      "Epoch [3/50], Step [130/777], Loss: 0.1008\n",
      "Epoch [3/50], Step [140/777], Loss: 0.2199\n",
      "Epoch [3/50], Step [150/777], Loss: 0.0704\n",
      "Epoch [3/50], Step [160/777], Loss: 0.1297\n",
      "Epoch [3/50], Step [170/777], Loss: 0.0417\n",
      "Epoch [3/50], Step [180/777], Loss: 0.1086\n",
      "Epoch [3/50], Step [190/777], Loss: 0.0796\n",
      "Epoch [3/50], Step [200/777], Loss: 0.2806\n",
      "Epoch [3/50], Step [210/777], Loss: 0.0318\n",
      "Epoch [3/50], Step [220/777], Loss: 0.4553\n",
      "Epoch [3/50], Step [230/777], Loss: 0.0329\n",
      "Epoch [3/50], Step [240/777], Loss: 0.0721\n",
      "Epoch [3/50], Step [250/777], Loss: 0.0852\n",
      "Epoch [3/50], Step [260/777], Loss: 0.0911\n",
      "Epoch [3/50], Step [270/777], Loss: 0.4443\n",
      "Epoch [3/50], Step [280/777], Loss: 0.0293\n",
      "Epoch [3/50], Step [290/777], Loss: 0.0429\n",
      "Epoch [3/50], Step [300/777], Loss: 0.0422\n",
      "Epoch [3/50], Step [310/777], Loss: 0.4882\n",
      "Epoch [3/50], Step [320/777], Loss: 0.3957\n",
      "Epoch [3/50], Step [330/777], Loss: 0.2313\n",
      "Epoch [3/50], Step [340/777], Loss: 0.0866\n",
      "Epoch [3/50], Step [350/777], Loss: 0.6087\n",
      "Epoch [3/50], Step [360/777], Loss: 0.0905\n",
      "Epoch [3/50], Step [370/777], Loss: 0.1934\n",
      "Epoch [3/50], Step [380/777], Loss: 0.2580\n",
      "Epoch [3/50], Step [390/777], Loss: 0.3225\n",
      "Epoch [3/50], Step [400/777], Loss: 0.1611\n",
      "Epoch [3/50], Step [410/777], Loss: 0.3730\n",
      "Epoch [3/50], Step [420/777], Loss: 0.2733\n",
      "Epoch [3/50], Step [430/777], Loss: 0.0254\n",
      "Epoch [3/50], Step [440/777], Loss: 0.2217\n",
      "Epoch [3/50], Step [450/777], Loss: 0.1341\n",
      "Epoch [3/50], Step [460/777], Loss: 0.5397\n",
      "Epoch [3/50], Step [470/777], Loss: 0.0820\n",
      "Epoch [3/50], Step [480/777], Loss: 0.0586\n",
      "Epoch [3/50], Step [490/777], Loss: 0.3661\n",
      "Epoch [3/50], Step [500/777], Loss: 0.4726\n",
      "Epoch [3/50], Step [510/777], Loss: 0.2116\n",
      "Epoch [3/50], Step [520/777], Loss: 0.0701\n",
      "Epoch [3/50], Step [530/777], Loss: 0.0639\n",
      "Epoch [3/50], Step [540/777], Loss: 0.0280\n",
      "Epoch [3/50], Step [550/777], Loss: 0.3783\n",
      "Epoch [3/50], Step [560/777], Loss: 0.0806\n",
      "Epoch [3/50], Step [570/777], Loss: 0.0784\n",
      "Epoch [3/50], Step [580/777], Loss: 0.2741\n",
      "Epoch [3/50], Step [590/777], Loss: 0.0279\n",
      "Epoch [3/50], Step [600/777], Loss: 0.1933\n",
      "Epoch [3/50], Step [610/777], Loss: 0.0514\n",
      "Epoch [3/50], Step [620/777], Loss: 0.0723\n",
      "Epoch [3/50], Step [630/777], Loss: 0.5951\n",
      "Epoch [3/50], Step [640/777], Loss: 0.0819\n",
      "Epoch [3/50], Step [650/777], Loss: 0.5502\n",
      "Epoch [3/50], Step [660/777], Loss: 0.1721\n",
      "Epoch [3/50], Step [670/777], Loss: 0.0747\n",
      "Epoch [3/50], Step [680/777], Loss: 0.7096\n",
      "Epoch [3/50], Step [690/777], Loss: 0.3076\n",
      "Epoch [3/50], Step [700/777], Loss: 0.0572\n",
      "Epoch [3/50], Step [710/777], Loss: 0.2424\n",
      "Epoch [3/50], Step [720/777], Loss: 0.1336\n",
      "Epoch [3/50], Step [730/777], Loss: 0.2127\n",
      "Epoch [3/50], Step [740/777], Loss: 0.1859\n",
      "Epoch [3/50], Step [750/777], Loss: 0.0706\n",
      "Epoch [3/50], Step [760/777], Loss: 0.0382\n",
      "Epoch [3/50], Step [770/777], Loss: 0.1772\n",
      "Epoch [3/50], Train Loss: 0.2297, Val Loss: 0.2206, Val Accuracy: 0.9421\n",
      "Epoch [4/50], Step [10/777], Loss: 0.0459\n",
      "Epoch [4/50], Step [20/777], Loss: 0.0649\n",
      "Epoch [4/50], Step [30/777], Loss: 0.0940\n",
      "Epoch [4/50], Step [40/777], Loss: 0.0868\n",
      "Epoch [4/50], Step [50/777], Loss: 0.3212\n",
      "Epoch [4/50], Step [60/777], Loss: 0.4743\n",
      "Epoch [4/50], Step [70/777], Loss: 0.3326\n",
      "Epoch [4/50], Step [80/777], Loss: 0.3075\n",
      "Epoch [4/50], Step [90/777], Loss: 0.0230\n",
      "Epoch [4/50], Step [100/777], Loss: 0.0437\n",
      "Epoch [4/50], Step [110/777], Loss: 0.3164\n",
      "Epoch [4/50], Step [120/777], Loss: 0.0646\n",
      "Epoch [4/50], Step [130/777], Loss: 0.7215\n",
      "Epoch [4/50], Step [140/777], Loss: 0.1174\n",
      "Epoch [4/50], Step [150/777], Loss: 0.0805\n",
      "Epoch [4/50], Step [160/777], Loss: 0.0467\n",
      "Epoch [4/50], Step [170/777], Loss: 0.1325\n",
      "Epoch [4/50], Step [180/777], Loss: 0.0182\n",
      "Epoch [4/50], Step [190/777], Loss: 0.9802\n",
      "Epoch [4/50], Step [200/777], Loss: 0.0221\n",
      "Epoch [4/50], Step [210/777], Loss: 0.3140\n",
      "Epoch [4/50], Step [220/777], Loss: 0.0373\n",
      "Epoch [4/50], Step [230/777], Loss: 0.1828\n",
      "Epoch [4/50], Step [240/777], Loss: 0.0952\n",
      "Epoch [4/50], Step [250/777], Loss: 0.1720\n",
      "Epoch [4/50], Step [260/777], Loss: 0.1205\n",
      "Epoch [4/50], Step [270/777], Loss: 0.0405\n",
      "Epoch [4/50], Step [280/777], Loss: 0.1821\n",
      "Epoch [4/50], Step [290/777], Loss: 0.3373\n",
      "Epoch [4/50], Step [300/777], Loss: 0.0364\n",
      "Epoch [4/50], Step [310/777], Loss: 0.0709\n",
      "Epoch [4/50], Step [320/777], Loss: 0.6084\n",
      "Epoch [4/50], Step [330/777], Loss: 0.1360\n",
      "Epoch [4/50], Step [340/777], Loss: 0.1333\n",
      "Epoch [4/50], Step [350/777], Loss: 0.5234\n",
      "Epoch [4/50], Step [360/777], Loss: 0.0666\n",
      "Epoch [4/50], Step [370/777], Loss: 0.0724\n",
      "Epoch [4/50], Step [380/777], Loss: 0.1384\n",
      "Epoch [4/50], Step [390/777], Loss: 1.1574\n",
      "Epoch [4/50], Step [400/777], Loss: 0.1930\n",
      "Epoch [4/50], Step [410/777], Loss: 0.5313\n",
      "Epoch [4/50], Step [420/777], Loss: 0.5623\n",
      "Epoch [4/50], Step [430/777], Loss: 0.0485\n",
      "Epoch [4/50], Step [440/777], Loss: 0.1803\n",
      "Epoch [4/50], Step [450/777], Loss: 0.2564\n",
      "Epoch [4/50], Step [460/777], Loss: 0.3325\n",
      "Epoch [4/50], Step [470/777], Loss: 0.1207\n",
      "Epoch [4/50], Step [480/777], Loss: 0.0585\n",
      "Epoch [4/50], Step [490/777], Loss: 0.0364\n",
      "Epoch [4/50], Step [500/777], Loss: 0.0317\n",
      "Epoch [4/50], Step [510/777], Loss: 0.0318\n",
      "Epoch [4/50], Step [520/777], Loss: 0.1627\n",
      "Epoch [4/50], Step [530/777], Loss: 0.2677\n",
      "Epoch [4/50], Step [540/777], Loss: 0.0496\n",
      "Epoch [4/50], Step [550/777], Loss: 0.1500\n",
      "Epoch [4/50], Step [560/777], Loss: 0.0372\n",
      "Epoch [4/50], Step [570/777], Loss: 0.1439\n",
      "Epoch [4/50], Step [580/777], Loss: 0.5965\n",
      "Epoch [4/50], Step [590/777], Loss: 0.0793\n",
      "Epoch [4/50], Step [600/777], Loss: 0.2508\n",
      "Epoch [4/50], Step [610/777], Loss: 0.0593\n",
      "Epoch [4/50], Step [620/777], Loss: 0.0117\n",
      "Epoch [4/50], Step [630/777], Loss: 0.1905\n",
      "Epoch [4/50], Step [640/777], Loss: 0.5084\n",
      "Epoch [4/50], Step [650/777], Loss: 0.5125\n",
      "Epoch [4/50], Step [660/777], Loss: 0.2408\n",
      "Epoch [4/50], Step [670/777], Loss: 0.0377\n",
      "Epoch [4/50], Step [680/777], Loss: 0.2839\n",
      "Epoch [4/50], Step [690/777], Loss: 0.2084\n",
      "Epoch [4/50], Step [700/777], Loss: 0.0453\n",
      "Epoch [4/50], Step [710/777], Loss: 0.0361\n",
      "Epoch [4/50], Step [720/777], Loss: 0.0372\n",
      "Epoch [4/50], Step [730/777], Loss: 0.0157\n",
      "Epoch [4/50], Step [740/777], Loss: 0.2121\n",
      "Epoch [4/50], Step [750/777], Loss: 0.0384\n",
      "Epoch [4/50], Step [760/777], Loss: 0.0691\n",
      "Epoch [4/50], Step [770/777], Loss: 0.0803\n",
      "Epoch [4/50], Train Loss: 0.2011, Val Loss: 0.2018, Val Accuracy: 0.9444\n",
      "Model saved with validation accuracy: 0.9444\n",
      "Epoch [5/50], Step [10/777], Loss: 0.0838\n",
      "Epoch [5/50], Step [20/777], Loss: 0.0890\n",
      "Epoch [5/50], Step [30/777], Loss: 0.2132\n",
      "Epoch [5/50], Step [40/777], Loss: 0.3342\n",
      "Epoch [5/50], Step [50/777], Loss: 0.0538\n",
      "Epoch [5/50], Step [60/777], Loss: 0.0211\n",
      "Epoch [5/50], Step [70/777], Loss: 0.6612\n",
      "Epoch [5/50], Step [80/777], Loss: 0.3303\n",
      "Epoch [5/50], Step [90/777], Loss: 0.0405\n",
      "Epoch [5/50], Step [100/777], Loss: 0.3435\n",
      "Epoch [5/50], Step [110/777], Loss: 0.3577\n",
      "Epoch [5/50], Step [120/777], Loss: 0.0663\n",
      "Epoch [5/50], Step [130/777], Loss: 0.1140\n",
      "Epoch [5/50], Step [140/777], Loss: 0.0246\n",
      "Epoch [5/50], Step [150/777], Loss: 0.4022\n",
      "Epoch [5/50], Step [160/777], Loss: 0.3113\n",
      "Epoch [5/50], Step [170/777], Loss: 0.3159\n",
      "Epoch [5/50], Step [180/777], Loss: 0.2975\n",
      "Epoch [5/50], Step [190/777], Loss: 0.0399\n",
      "Epoch [5/50], Step [200/777], Loss: 0.0493\n",
      "Epoch [5/50], Step [210/777], Loss: 0.3180\n",
      "Epoch [5/50], Step [220/777], Loss: 0.6230\n",
      "Epoch [5/50], Step [230/777], Loss: 0.0461\n",
      "Epoch [5/50], Step [240/777], Loss: 0.0254\n",
      "Epoch [5/50], Step [250/777], Loss: 0.0207\n",
      "Epoch [5/50], Step [260/777], Loss: 0.0256\n",
      "Epoch [5/50], Step [270/777], Loss: 0.1541\n",
      "Epoch [5/50], Step [280/777], Loss: 0.5329\n",
      "Epoch [5/50], Step [290/777], Loss: 0.2062\n",
      "Epoch [5/50], Step [300/777], Loss: 0.0733\n",
      "Epoch [5/50], Step [310/777], Loss: 0.0248\n",
      "Epoch [5/50], Step [320/777], Loss: 0.0332\n",
      "Epoch [5/50], Step [330/777], Loss: 0.2009\n",
      "Epoch [5/50], Step [340/777], Loss: 0.4338\n",
      "Epoch [5/50], Step [350/777], Loss: 0.0788\n",
      "Epoch [5/50], Step [360/777], Loss: 0.3366\n",
      "Epoch [5/50], Step [370/777], Loss: 0.3040\n",
      "Epoch [5/50], Step [380/777], Loss: 0.0144\n",
      "Epoch [5/50], Step [390/777], Loss: 0.1051\n",
      "Epoch [5/50], Step [400/777], Loss: 0.0655\n",
      "Epoch [5/50], Step [410/777], Loss: 0.0355\n",
      "Epoch [5/50], Step [420/777], Loss: 0.1241\n",
      "Epoch [5/50], Step [430/777], Loss: 0.1563\n",
      "Epoch [5/50], Step [440/777], Loss: 0.3770\n",
      "Epoch [5/50], Step [450/777], Loss: 0.0435\n",
      "Epoch [5/50], Step [460/777], Loss: 0.5760\n",
      "Epoch [5/50], Step [470/777], Loss: 0.1788\n",
      "Epoch [5/50], Step [480/777], Loss: 0.0793\n",
      "Epoch [5/50], Step [490/777], Loss: 0.0988\n",
      "Epoch [5/50], Step [500/777], Loss: 0.3182\n",
      "Epoch [5/50], Step [510/777], Loss: 0.0696\n",
      "Epoch [5/50], Step [520/777], Loss: 0.1501\n",
      "Epoch [5/50], Step [530/777], Loss: 0.0431\n",
      "Epoch [5/50], Step [540/777], Loss: 0.0491\n",
      "Epoch [5/50], Step [550/777], Loss: 0.6759\n",
      "Epoch [5/50], Step [560/777], Loss: 0.0494\n",
      "Epoch [5/50], Step [570/777], Loss: 0.0231\n",
      "Epoch [5/50], Step [580/777], Loss: 0.1760\n",
      "Epoch [5/50], Step [590/777], Loss: 0.0832\n",
      "Epoch [5/50], Step [600/777], Loss: 0.0541\n",
      "Epoch [5/50], Step [610/777], Loss: 0.0354\n",
      "Epoch [5/50], Step [620/777], Loss: 0.0325\n",
      "Epoch [5/50], Step [630/777], Loss: 0.0159\n",
      "Epoch [5/50], Step [640/777], Loss: 0.1810\n",
      "Epoch [5/50], Step [650/777], Loss: 0.0887\n",
      "Epoch [5/50], Step [660/777], Loss: 0.6069\n",
      "Epoch [5/50], Step [670/777], Loss: 0.9803\n",
      "Epoch [5/50], Step [680/777], Loss: 0.0478\n",
      "Epoch [5/50], Step [690/777], Loss: 0.0811\n",
      "Epoch [5/50], Step [700/777], Loss: 0.3030\n",
      "Epoch [5/50], Step [710/777], Loss: 0.2813\n",
      "Epoch [5/50], Step [720/777], Loss: 0.0959\n",
      "Epoch [5/50], Step [730/777], Loss: 0.0449\n",
      "Epoch [5/50], Step [740/777], Loss: 0.2471\n",
      "Epoch [5/50], Step [750/777], Loss: 0.1338\n",
      "Epoch [5/50], Step [760/777], Loss: 0.0231\n",
      "Epoch [5/50], Step [770/777], Loss: 0.4194\n",
      "Epoch [5/50], Train Loss: 0.1649, Val Loss: 0.2193, Val Accuracy: 0.9463\n",
      "Model saved with validation accuracy: 0.9463\n",
      "Epoch [6/50], Step [10/777], Loss: 0.0970\n",
      "Epoch [6/50], Step [20/777], Loss: 0.0510\n",
      "Epoch [6/50], Step [30/777], Loss: 0.0595\n",
      "Epoch [6/50], Step [40/777], Loss: 0.0251\n",
      "Epoch [6/50], Step [50/777], Loss: 0.0306\n",
      "Epoch [6/50], Step [60/777], Loss: 0.0370\n",
      "Epoch [6/50], Step [70/777], Loss: 0.0158\n",
      "Epoch [6/50], Step [80/777], Loss: 0.0758\n",
      "Epoch [6/50], Step [90/777], Loss: 0.0194\n",
      "Epoch [6/50], Step [100/777], Loss: 0.3033\n",
      "Epoch [6/50], Step [110/777], Loss: 0.0767\n",
      "Epoch [6/50], Step [120/777], Loss: 0.0767\n",
      "Epoch [6/50], Step [130/777], Loss: 0.0113\n",
      "Epoch [6/50], Step [140/777], Loss: 0.1030\n",
      "Epoch [6/50], Step [150/777], Loss: 0.0533\n",
      "Epoch [6/50], Step [160/777], Loss: 0.5254\n",
      "Epoch [6/50], Step [170/777], Loss: 0.1338\n",
      "Epoch [6/50], Step [180/777], Loss: 0.0358\n",
      "Epoch [6/50], Step [190/777], Loss: 0.0428\n",
      "Epoch [6/50], Step [200/777], Loss: 0.1377\n",
      "Epoch [6/50], Step [210/777], Loss: 0.1851\n",
      "Epoch [6/50], Step [220/777], Loss: 0.0825\n",
      "Epoch [6/50], Step [230/777], Loss: 0.2327\n",
      "Epoch [6/50], Step [240/777], Loss: 0.4126\n",
      "Epoch [6/50], Step [250/777], Loss: 0.0234\n",
      "Epoch [6/50], Step [260/777], Loss: 0.1756\n",
      "Epoch [6/50], Step [270/777], Loss: 0.7023\n",
      "Epoch [6/50], Step [280/777], Loss: 0.0765\n",
      "Epoch [6/50], Step [290/777], Loss: 0.1262\n",
      "Epoch [6/50], Step [300/777], Loss: 0.0152\n",
      "Epoch [6/50], Step [310/777], Loss: 0.0276\n",
      "Epoch [6/50], Step [320/777], Loss: 0.1076\n",
      "Epoch [6/50], Step [330/777], Loss: 0.0248\n",
      "Epoch [6/50], Step [340/777], Loss: 0.0488\n",
      "Epoch [6/50], Step [350/777], Loss: 0.0287\n",
      "Epoch [6/50], Step [360/777], Loss: 0.0543\n",
      "Epoch [6/50], Step [370/777], Loss: 0.0153\n",
      "Epoch [6/50], Step [380/777], Loss: 0.2859\n",
      "Epoch [6/50], Step [390/777], Loss: 0.0997\n",
      "Epoch [6/50], Step [400/777], Loss: 0.0768\n",
      "Epoch [6/50], Step [410/777], Loss: 0.2549\n",
      "Epoch [6/50], Step [420/777], Loss: 0.0549\n",
      "Epoch [6/50], Step [430/777], Loss: 0.0941\n",
      "Epoch [6/50], Step [440/777], Loss: 0.1007\n",
      "Epoch [6/50], Step [450/777], Loss: 0.0304\n",
      "Epoch [6/50], Step [460/777], Loss: 0.0077\n",
      "Epoch [6/50], Step [470/777], Loss: 0.0153\n",
      "Epoch [6/50], Step [480/777], Loss: 0.0195\n",
      "Epoch [6/50], Step [490/777], Loss: 0.0160\n",
      "Epoch [6/50], Step [500/777], Loss: 0.1212\n",
      "Epoch [6/50], Step [510/777], Loss: 0.1649\n",
      "Epoch [6/50], Step [520/777], Loss: 0.0617\n",
      "Epoch [6/50], Step [530/777], Loss: 0.0349\n",
      "Epoch [6/50], Step [540/777], Loss: 0.0551\n",
      "Epoch [6/50], Step [550/777], Loss: 0.0216\n",
      "Epoch [6/50], Step [560/777], Loss: 0.0538\n",
      "Epoch [6/50], Step [570/777], Loss: 0.0452\n",
      "Epoch [6/50], Step [580/777], Loss: 0.2196\n",
      "Epoch [6/50], Step [590/777], Loss: 0.2860\n",
      "Epoch [6/50], Step [600/777], Loss: 0.5312\n",
      "Epoch [6/50], Step [610/777], Loss: 0.4446\n",
      "Epoch [6/50], Step [620/777], Loss: 0.0274\n",
      "Epoch [6/50], Step [630/777], Loss: 0.5081\n",
      "Epoch [6/50], Step [640/777], Loss: 0.5301\n",
      "Epoch [6/50], Step [650/777], Loss: 0.0270\n",
      "Epoch [6/50], Step [660/777], Loss: 0.1073\n",
      "Epoch [6/50], Step [670/777], Loss: 0.3910\n",
      "Epoch [6/50], Step [680/777], Loss: 0.1730\n",
      "Epoch [6/50], Step [690/777], Loss: 0.0556\n",
      "Epoch [6/50], Step [700/777], Loss: 0.0711\n",
      "Epoch [6/50], Step [710/777], Loss: 0.1031\n",
      "Epoch [6/50], Step [720/777], Loss: 0.1337\n",
      "Epoch [6/50], Step [730/777], Loss: 0.0350\n",
      "Epoch [6/50], Step [740/777], Loss: 0.4481\n",
      "Epoch [6/50], Step [750/777], Loss: 0.0488\n",
      "Epoch [6/50], Step [760/777], Loss: 0.3137\n",
      "Epoch [6/50], Step [770/777], Loss: 0.2862\n",
      "Epoch [6/50], Train Loss: 0.1452, Val Loss: 0.2092, Val Accuracy: 0.9511\n",
      "Model saved with validation accuracy: 0.9511\n",
      "Epoch [7/50], Step [10/777], Loss: 0.0237\n",
      "Epoch [7/50], Step [20/777], Loss: 0.2194\n",
      "Epoch [7/50], Step [30/777], Loss: 0.0698\n",
      "Epoch [7/50], Step [40/777], Loss: 0.0472\n",
      "Epoch [7/50], Step [50/777], Loss: 0.0314\n",
      "Epoch [7/50], Step [60/777], Loss: 0.0632\n",
      "Epoch [7/50], Step [70/777], Loss: 0.1428\n",
      "Epoch [7/50], Step [80/777], Loss: 0.0655\n",
      "Epoch [7/50], Step [90/777], Loss: 0.3366\n",
      "Epoch [7/50], Step [100/777], Loss: 0.2557\n",
      "Epoch [7/50], Step [110/777], Loss: 0.4969\n",
      "Epoch [7/50], Step [120/777], Loss: 0.0439\n",
      "Epoch [7/50], Step [130/777], Loss: 0.3853\n",
      "Epoch [7/50], Step [140/777], Loss: 0.0102\n",
      "Epoch [7/50], Step [150/777], Loss: 0.0277\n",
      "Epoch [7/50], Step [160/777], Loss: 0.0587\n",
      "Epoch [7/50], Step [170/777], Loss: 0.0933\n",
      "Epoch [7/50], Step [180/777], Loss: 0.3096\n",
      "Epoch [7/50], Step [190/777], Loss: 0.2150\n",
      "Epoch [7/50], Step [200/777], Loss: 0.0100\n",
      "Epoch [7/50], Step [210/777], Loss: 0.0100\n",
      "Epoch [7/50], Step [220/777], Loss: 0.1023\n",
      "Epoch [7/50], Step [230/777], Loss: 0.0081\n",
      "Epoch [7/50], Step [240/777], Loss: 0.0816\n",
      "Epoch [7/50], Step [250/777], Loss: 0.0373\n",
      "Epoch [7/50], Step [260/777], Loss: 0.3150\n",
      "Epoch [7/50], Step [270/777], Loss: 0.1100\n",
      "Epoch [7/50], Step [280/777], Loss: 0.2539\n",
      "Epoch [7/50], Step [290/777], Loss: 0.0919\n",
      "Epoch [7/50], Step [300/777], Loss: 0.0292\n",
      "Epoch [7/50], Step [310/777], Loss: 0.1560\n",
      "Epoch [7/50], Step [320/777], Loss: 0.0662\n",
      "Epoch [7/50], Step [330/777], Loss: 0.0463\n",
      "Epoch [7/50], Step [340/777], Loss: 0.0138\n",
      "Epoch [7/50], Step [350/777], Loss: 0.0207\n",
      "Epoch [7/50], Step [360/777], Loss: 0.2207\n",
      "Epoch [7/50], Step [370/777], Loss: 0.1805\n",
      "Epoch [7/50], Step [380/777], Loss: 0.0246\n",
      "Epoch [7/50], Step [390/777], Loss: 0.0327\n",
      "Epoch [7/50], Step [400/777], Loss: 0.5018\n",
      "Epoch [7/50], Step [410/777], Loss: 0.2389\n",
      "Epoch [7/50], Step [420/777], Loss: 0.1286\n",
      "Epoch [7/50], Step [430/777], Loss: 0.0154\n",
      "Epoch [7/50], Step [440/777], Loss: 0.0174\n",
      "Epoch [7/50], Step [450/777], Loss: 0.0168\n",
      "Epoch [7/50], Step [460/777], Loss: 0.2002\n",
      "Epoch [7/50], Step [470/777], Loss: 0.0185\n",
      "Epoch [7/50], Step [480/777], Loss: 0.0356\n",
      "Epoch [7/50], Step [490/777], Loss: 0.2847\n",
      "Epoch [7/50], Step [500/777], Loss: 0.3458\n",
      "Epoch [7/50], Step [510/777], Loss: 0.2983\n",
      "Epoch [7/50], Step [520/777], Loss: 0.1998\n",
      "Epoch [7/50], Step [530/777], Loss: 0.1256\n",
      "Epoch [7/50], Step [540/777], Loss: 0.0094\n",
      "Epoch [7/50], Step [550/777], Loss: 0.1289\n",
      "Epoch [7/50], Step [560/777], Loss: 0.0960\n",
      "Epoch [7/50], Step [570/777], Loss: 0.0481\n",
      "Epoch [7/50], Step [580/777], Loss: 0.0203\n",
      "Epoch [7/50], Step [590/777], Loss: 0.0079\n",
      "Epoch [7/50], Step [600/777], Loss: 0.0575\n",
      "Epoch [7/50], Step [610/777], Loss: 0.2182\n",
      "Epoch [7/50], Step [620/777], Loss: 0.0333\n",
      "Epoch [7/50], Step [630/777], Loss: 0.0113\n",
      "Epoch [7/50], Step [640/777], Loss: 0.2437\n",
      "Epoch [7/50], Step [650/777], Loss: 0.0301\n",
      "Epoch [7/50], Step [660/777], Loss: 0.0127\n",
      "Epoch [7/50], Step [670/777], Loss: 0.1085\n",
      "Epoch [7/50], Step [680/777], Loss: 0.0278\n",
      "Epoch [7/50], Step [690/777], Loss: 0.0423\n",
      "Epoch [7/50], Step [700/777], Loss: 0.0808\n",
      "Epoch [7/50], Step [710/777], Loss: 0.4687\n",
      "Epoch [7/50], Step [720/777], Loss: 0.2984\n",
      "Epoch [7/50], Step [730/777], Loss: 0.2032\n",
      "Epoch [7/50], Step [740/777], Loss: 0.1910\n",
      "Epoch [7/50], Step [750/777], Loss: 0.1471\n",
      "Epoch [7/50], Step [760/777], Loss: 0.2113\n",
      "Epoch [7/50], Step [770/777], Loss: 0.0245\n",
      "Epoch [7/50], Train Loss: 0.1179, Val Loss: 0.2373, Val Accuracy: 0.9481\n",
      "Epoch [8/50], Step [10/777], Loss: 0.1215\n",
      "Epoch [8/50], Step [20/777], Loss: 0.0132\n",
      "Epoch [8/50], Step [30/777], Loss: 0.1333\n",
      "Epoch [8/50], Step [40/777], Loss: 0.0111\n",
      "Epoch [8/50], Step [50/777], Loss: 0.0695\n",
      "Epoch [8/50], Step [60/777], Loss: 0.0518\n",
      "Epoch [8/50], Step [70/777], Loss: 0.0399\n",
      "Epoch [8/50], Step [80/777], Loss: 0.0249\n",
      "Epoch [8/50], Step [90/777], Loss: 0.2042\n",
      "Epoch [8/50], Step [100/777], Loss: 0.1846\n",
      "Epoch [8/50], Step [110/777], Loss: 0.0447\n",
      "Epoch [8/50], Step [120/777], Loss: 0.1600\n",
      "Epoch [8/50], Step [130/777], Loss: 0.0057\n",
      "Epoch [8/50], Step [140/777], Loss: 0.0572\n",
      "Epoch [8/50], Step [150/777], Loss: 0.0075\n",
      "Epoch [8/50], Step [160/777], Loss: 0.0147\n",
      "Epoch [8/50], Step [170/777], Loss: 0.0245\n",
      "Epoch [8/50], Step [180/777], Loss: 0.0427\n",
      "Epoch [8/50], Step [190/777], Loss: 0.1908\n",
      "Epoch [8/50], Step [200/777], Loss: 0.0789\n",
      "Epoch [8/50], Step [210/777], Loss: 0.2494\n",
      "Epoch [8/50], Step [220/777], Loss: 0.3635\n",
      "Epoch [8/50], Step [230/777], Loss: 0.0262\n",
      "Epoch [8/50], Step [240/777], Loss: 0.6564\n",
      "Epoch [8/50], Step [250/777], Loss: 0.0878\n",
      "Epoch [8/50], Step [260/777], Loss: 0.0134\n",
      "Epoch [8/50], Step [270/777], Loss: 0.0670\n",
      "Epoch [8/50], Step [280/777], Loss: 0.1364\n",
      "Epoch [8/50], Step [290/777], Loss: 0.0705\n",
      "Epoch [8/50], Step [300/777], Loss: 0.0520\n",
      "Epoch [8/50], Step [310/777], Loss: 0.2846\n",
      "Epoch [8/50], Step [320/777], Loss: 0.1404\n",
      "Epoch [8/50], Step [330/777], Loss: 0.1101\n",
      "Epoch [8/50], Step [340/777], Loss: 0.0508\n",
      "Epoch [8/50], Step [350/777], Loss: 0.0937\n",
      "Epoch [8/50], Step [360/777], Loss: 0.0167\n",
      "Epoch [8/50], Step [370/777], Loss: 0.0294\n",
      "Epoch [8/50], Step [380/777], Loss: 0.0197\n",
      "Epoch [8/50], Step [390/777], Loss: 0.0171\n",
      "Epoch [8/50], Step [400/777], Loss: 0.0333\n",
      "Epoch [8/50], Step [410/777], Loss: 0.0321\n",
      "Epoch [8/50], Step [420/777], Loss: 0.0193\n",
      "Epoch [8/50], Step [430/777], Loss: 0.2889\n",
      "Epoch [8/50], Step [440/777], Loss: 0.0269\n",
      "Epoch [8/50], Step [450/777], Loss: 0.0616\n",
      "Epoch [8/50], Step [460/777], Loss: 0.0612\n",
      "Epoch [8/50], Step [470/777], Loss: 0.2777\n",
      "Epoch [8/50], Step [480/777], Loss: 0.0099\n",
      "Epoch [8/50], Step [490/777], Loss: 0.2030\n",
      "Epoch [8/50], Step [500/777], Loss: 0.0051\n",
      "Epoch [8/50], Step [510/777], Loss: 0.1479\n",
      "Epoch [8/50], Step [520/777], Loss: 0.1068\n",
      "Epoch [8/50], Step [530/777], Loss: 0.0714\n",
      "Epoch [8/50], Step [540/777], Loss: 0.0200\n",
      "Epoch [8/50], Step [550/777], Loss: 0.1344\n",
      "Epoch [8/50], Step [560/777], Loss: 0.2315\n",
      "Epoch [8/50], Step [570/777], Loss: 0.0405\n",
      "Epoch [8/50], Step [580/777], Loss: 0.0974\n",
      "Epoch [8/50], Step [590/777], Loss: 0.1377\n",
      "Epoch [8/50], Step [600/777], Loss: 0.0139\n",
      "Epoch [8/50], Step [610/777], Loss: 0.2887\n",
      "Epoch [8/50], Step [620/777], Loss: 0.4687\n",
      "Epoch [8/50], Step [630/777], Loss: 0.0192\n",
      "Epoch [8/50], Step [640/777], Loss: 0.1528\n",
      "Epoch [8/50], Step [650/777], Loss: 0.2515\n",
      "Epoch [8/50], Step [660/777], Loss: 0.0209\n",
      "Epoch [8/50], Step [670/777], Loss: 0.1117\n",
      "Epoch [8/50], Step [680/777], Loss: 0.1233\n",
      "Epoch [8/50], Step [690/777], Loss: 0.0178\n",
      "Epoch [8/50], Step [700/777], Loss: 0.0273\n",
      "Epoch [8/50], Step [710/777], Loss: 0.0128\n",
      "Epoch [8/50], Step [720/777], Loss: 0.0505\n",
      "Epoch [8/50], Step [730/777], Loss: 0.0326\n",
      "Epoch [8/50], Step [740/777], Loss: 0.1262\n",
      "Epoch [8/50], Step [750/777], Loss: 0.0789\n",
      "Epoch [8/50], Step [760/777], Loss: 0.0402\n",
      "Epoch [8/50], Step [770/777], Loss: 0.1015\n",
      "Epoch [8/50], Train Loss: 0.0963, Val Loss: 0.2380, Val Accuracy: 0.9481\n",
      "Epoch [9/50], Step [10/777], Loss: 0.0577\n",
      "Epoch [9/50], Step [20/777], Loss: 0.0184\n",
      "Epoch [9/50], Step [30/777], Loss: 0.0778\n",
      "Epoch [9/50], Step [40/777], Loss: 0.0150\n",
      "Epoch [9/50], Step [50/777], Loss: 0.0172\n",
      "Epoch [9/50], Step [60/777], Loss: 0.2483\n",
      "Epoch [9/50], Step [70/777], Loss: 0.1337\n",
      "Epoch [9/50], Step [80/777], Loss: 0.0267\n",
      "Epoch [9/50], Step [90/777], Loss: 0.2274\n",
      "Epoch [9/50], Step [100/777], Loss: 0.0138\n",
      "Epoch [9/50], Step [110/777], Loss: 0.0274\n",
      "Epoch [9/50], Step [120/777], Loss: 0.0422\n",
      "Epoch [9/50], Step [130/777], Loss: 0.1582\n",
      "Epoch [9/50], Step [140/777], Loss: 0.0072\n",
      "Epoch [9/50], Step [150/777], Loss: 0.0343\n",
      "Epoch [9/50], Step [160/777], Loss: 0.0514\n",
      "Epoch [9/50], Step [170/777], Loss: 0.0240\n",
      "Epoch [9/50], Step [180/777], Loss: 0.1583\n",
      "Epoch [9/50], Step [190/777], Loss: 0.2288\n",
      "Epoch [9/50], Step [200/777], Loss: 0.0944\n",
      "Epoch [9/50], Step [210/777], Loss: 0.0408\n",
      "Epoch [9/50], Step [220/777], Loss: 0.0107\n",
      "Epoch [9/50], Step [230/777], Loss: 0.0207\n",
      "Epoch [9/50], Step [240/777], Loss: 0.0087\n",
      "Epoch [9/50], Step [250/777], Loss: 0.0315\n",
      "Epoch [9/50], Step [260/777], Loss: 0.0230\n",
      "Epoch [9/50], Step [270/777], Loss: 0.0059\n",
      "Epoch [9/50], Step [280/777], Loss: 0.0089\n",
      "Epoch [9/50], Step [290/777], Loss: 0.0029\n",
      "Epoch [9/50], Step [300/777], Loss: 0.0541\n",
      "Epoch [9/50], Step [310/777], Loss: 0.0627\n",
      "Epoch [9/50], Step [320/777], Loss: 0.0050\n",
      "Epoch [9/50], Step [330/777], Loss: 0.2017\n",
      "Epoch [9/50], Step [340/777], Loss: 0.1038\n",
      "Epoch [9/50], Step [350/777], Loss: 0.0247\n",
      "Epoch [9/50], Step [360/777], Loss: 0.2334\n",
      "Epoch [9/50], Step [370/777], Loss: 0.0080\n",
      "Epoch [9/50], Step [380/777], Loss: 0.0344\n",
      "Epoch [9/50], Step [390/777], Loss: 0.0169\n",
      "Epoch [9/50], Step [400/777], Loss: 0.0102\n",
      "Epoch [9/50], Step [410/777], Loss: 0.0174\n",
      "Epoch [9/50], Step [420/777], Loss: 0.1345\n",
      "Epoch [9/50], Step [430/777], Loss: 0.0064\n",
      "Epoch [9/50], Step [440/777], Loss: 0.0057\n",
      "Epoch [9/50], Step [450/777], Loss: 0.1246\n",
      "Epoch [9/50], Step [460/777], Loss: 0.0447\n",
      "Epoch [9/50], Step [470/777], Loss: 0.0028\n",
      "Epoch [9/50], Step [480/777], Loss: 0.0919\n",
      "Epoch [9/50], Step [490/777], Loss: 0.0030\n",
      "Epoch [9/50], Step [500/777], Loss: 0.0072\n",
      "Epoch [9/50], Step [510/777], Loss: 0.0031\n",
      "Epoch [9/50], Step [520/777], Loss: 0.0297\n",
      "Epoch [9/50], Step [530/777], Loss: 0.0054\n",
      "Epoch [9/50], Step [540/777], Loss: 0.0123\n",
      "Epoch [9/50], Step [550/777], Loss: 0.0199\n",
      "Epoch [9/50], Step [560/777], Loss: 0.0111\n",
      "Epoch [9/50], Step [570/777], Loss: 0.0046\n",
      "Epoch [9/50], Step [580/777], Loss: 0.0358\n",
      "Epoch [9/50], Step [590/777], Loss: 0.0041\n",
      "Epoch [9/50], Step [600/777], Loss: 0.0465\n",
      "Epoch [9/50], Step [610/777], Loss: 0.1182\n",
      "Epoch [9/50], Step [620/777], Loss: 0.0189\n",
      "Epoch [9/50], Step [630/777], Loss: 0.0132\n",
      "Epoch [9/50], Step [640/777], Loss: 0.0035\n",
      "Epoch [9/50], Step [650/777], Loss: 0.0211\n",
      "Epoch [9/50], Step [660/777], Loss: 0.0049\n",
      "Epoch [9/50], Step [670/777], Loss: 0.0186\n",
      "Epoch [9/50], Step [680/777], Loss: 0.0021\n",
      "Epoch [9/50], Step [690/777], Loss: 0.0110\n",
      "Epoch [9/50], Step [700/777], Loss: 0.0107\n",
      "Epoch [9/50], Step [710/777], Loss: 0.0099\n",
      "Epoch [9/50], Step [720/777], Loss: 0.0129\n",
      "Epoch [9/50], Step [730/777], Loss: 0.0069\n",
      "Epoch [9/50], Step [740/777], Loss: 0.0145\n",
      "Epoch [9/50], Step [750/777], Loss: 0.0013\n",
      "Epoch [9/50], Step [760/777], Loss: 0.0077\n",
      "Epoch [9/50], Step [770/777], Loss: 0.4362\n",
      "Epoch [9/50], Train Loss: 0.0441, Val Loss: 0.2557, Val Accuracy: 0.9406\n",
      "Epoch [10/50], Step [10/777], Loss: 0.0416\n",
      "Epoch [10/50], Step [20/777], Loss: 0.0096\n",
      "Epoch [10/50], Step [30/777], Loss: 0.0011\n",
      "Epoch [10/50], Step [40/777], Loss: 0.0387\n",
      "Epoch [10/50], Step [50/777], Loss: 0.0048\n",
      "Epoch [10/50], Step [60/777], Loss: 0.0056\n",
      "Epoch [10/50], Step [70/777], Loss: 0.0019\n",
      "Epoch [10/50], Step [80/777], Loss: 0.0055\n",
      "Epoch [10/50], Step [90/777], Loss: 0.0344\n",
      "Epoch [10/50], Step [100/777], Loss: 0.0111\n",
      "Epoch [10/50], Step [110/777], Loss: 0.0025\n",
      "Epoch [10/50], Step [120/777], Loss: 0.0090\n",
      "Epoch [10/50], Step [130/777], Loss: 0.0318\n",
      "Epoch [10/50], Step [140/777], Loss: 0.0051\n",
      "Epoch [10/50], Step [150/777], Loss: 0.0201\n",
      "Epoch [10/50], Step [160/777], Loss: 0.0032\n",
      "Epoch [10/50], Step [170/777], Loss: 0.0022\n",
      "Epoch [10/50], Step [180/777], Loss: 0.0355\n",
      "Epoch [10/50], Step [190/777], Loss: 0.0694\n",
      "Epoch [10/50], Step [200/777], Loss: 0.0045\n",
      "Epoch [10/50], Step [210/777], Loss: 0.0174\n",
      "Epoch [10/50], Step [220/777], Loss: 0.0400\n",
      "Epoch [10/50], Step [230/777], Loss: 0.0020\n",
      "Epoch [10/50], Step [240/777], Loss: 0.0870\n",
      "Epoch [10/50], Step [250/777], Loss: 0.0022\n",
      "Epoch [10/50], Step [260/777], Loss: 0.0106\n",
      "Epoch [10/50], Step [270/777], Loss: 0.0113\n",
      "Epoch [10/50], Step [280/777], Loss: 0.0095\n",
      "Epoch [10/50], Step [290/777], Loss: 0.0105\n",
      "Epoch [10/50], Step [300/777], Loss: 0.1463\n",
      "Epoch [10/50], Step [310/777], Loss: 0.1249\n",
      "Epoch [10/50], Step [320/777], Loss: 0.0010\n",
      "Epoch [10/50], Step [330/777], Loss: 0.0066\n",
      "Epoch [10/50], Step [340/777], Loss: 0.0010\n",
      "Epoch [10/50], Step [350/777], Loss: 0.0030\n",
      "Epoch [10/50], Step [360/777], Loss: 0.0075\n",
      "Epoch [10/50], Step [370/777], Loss: 0.0081\n",
      "Epoch [10/50], Step [380/777], Loss: 0.0102\n",
      "Epoch [10/50], Step [390/777], Loss: 0.0038\n",
      "Epoch [10/50], Step [400/777], Loss: 0.0013\n",
      "Epoch [10/50], Step [410/777], Loss: 0.0864\n",
      "Epoch [10/50], Step [420/777], Loss: 0.0094\n",
      "Epoch [10/50], Step [430/777], Loss: 0.0435\n",
      "Epoch [10/50], Step [440/777], Loss: 0.0283\n",
      "Epoch [10/50], Step [450/777], Loss: 0.0042\n",
      "Epoch [10/50], Step [460/777], Loss: 0.0372\n",
      "Epoch [10/50], Step [470/777], Loss: 0.0440\n",
      "Epoch [10/50], Step [480/777], Loss: 0.0037\n",
      "Epoch [10/50], Step [490/777], Loss: 0.0178\n",
      "Epoch [10/50], Step [500/777], Loss: 0.1041\n",
      "Epoch [10/50], Step [510/777], Loss: 0.0136\n",
      "Epoch [10/50], Step [520/777], Loss: 0.0018\n",
      "Epoch [10/50], Step [530/777], Loss: 0.0021\n",
      "Epoch [10/50], Step [540/777], Loss: 0.0015\n",
      "Epoch [10/50], Step [550/777], Loss: 0.0053\n",
      "Epoch [10/50], Step [560/777], Loss: 0.0077\n",
      "Epoch [10/50], Step [570/777], Loss: 0.0962\n",
      "Epoch [10/50], Step [580/777], Loss: 0.0073\n",
      "Epoch [10/50], Step [590/777], Loss: 0.0244\n",
      "Epoch [10/50], Step [600/777], Loss: 0.0153\n",
      "Epoch [10/50], Step [610/777], Loss: 0.1838\n",
      "Epoch [10/50], Step [620/777], Loss: 0.0101\n",
      "Epoch [10/50], Step [630/777], Loss: 0.0101\n",
      "Epoch [10/50], Step [640/777], Loss: 0.0812\n",
      "Epoch [10/50], Step [650/777], Loss: 0.0141\n",
      "Epoch [10/50], Step [660/777], Loss: 0.0026\n",
      "Epoch [10/50], Step [670/777], Loss: 0.0292\n",
      "Epoch [10/50], Step [680/777], Loss: 0.0247\n",
      "Epoch [10/50], Step [690/777], Loss: 0.0092\n",
      "Epoch [10/50], Step [700/777], Loss: 0.0029\n",
      "Epoch [10/50], Step [710/777], Loss: 0.0142\n",
      "Epoch [10/50], Step [720/777], Loss: 0.0162\n",
      "Epoch [10/50], Step [730/777], Loss: 0.0087\n",
      "Epoch [10/50], Step [740/777], Loss: 0.0433\n",
      "Epoch [10/50], Step [750/777], Loss: 0.0035\n",
      "Epoch [10/50], Step [760/777], Loss: 0.0028\n",
      "Epoch [10/50], Step [770/777], Loss: 0.0045\n",
      "Epoch [10/50], Train Loss: 0.0299, Val Loss: 0.2640, Val Accuracy: 0.9406\n",
      "Epoch [11/50], Step [10/777], Loss: 0.0106\n",
      "Epoch [11/50], Step [20/777], Loss: 0.1320\n",
      "Epoch [11/50], Step [30/777], Loss: 0.0054\n",
      "Epoch [11/50], Step [40/777], Loss: 0.0038\n",
      "Epoch [11/50], Step [50/777], Loss: 0.0167\n",
      "Epoch [11/50], Step [60/777], Loss: 0.0063\n",
      "Epoch [11/50], Step [70/777], Loss: 0.0201\n",
      "Epoch [11/50], Step [80/777], Loss: 0.0064\n",
      "Epoch [11/50], Step [90/777], Loss: 0.0071\n",
      "Epoch [11/50], Step [100/777], Loss: 0.0046\n",
      "Epoch [11/50], Step [110/777], Loss: 0.0034\n",
      "Epoch [11/50], Step [120/777], Loss: 0.0038\n",
      "Epoch [11/50], Step [130/777], Loss: 0.1449\n",
      "Epoch [11/50], Step [140/777], Loss: 0.0012\n",
      "Epoch [11/50], Step [150/777], Loss: 0.1511\n",
      "Epoch [11/50], Step [160/777], Loss: 0.0257\n",
      "Epoch [11/50], Step [170/777], Loss: 0.0440\n",
      "Epoch [11/50], Step [180/777], Loss: 0.0344\n",
      "Epoch [11/50], Step [190/777], Loss: 0.0021\n",
      "Epoch [11/50], Step [200/777], Loss: 0.0438\n",
      "Epoch [11/50], Step [210/777], Loss: 0.0151\n",
      "Epoch [11/50], Step [220/777], Loss: 0.0219\n",
      "Epoch [11/50], Step [230/777], Loss: 0.0028\n",
      "Epoch [11/50], Step [240/777], Loss: 0.0032\n",
      "Epoch [11/50], Step [250/777], Loss: 0.0160\n",
      "Epoch [11/50], Step [260/777], Loss: 0.0014\n",
      "Epoch [11/50], Step [270/777], Loss: 0.0138\n",
      "Epoch [11/50], Step [280/777], Loss: 0.0812\n",
      "Epoch [11/50], Step [290/777], Loss: 0.0029\n",
      "Epoch [11/50], Step [300/777], Loss: 0.0063\n",
      "Epoch [11/50], Step [310/777], Loss: 0.0025\n",
      "Epoch [11/50], Step [320/777], Loss: 0.0060\n",
      "Epoch [11/50], Step [330/777], Loss: 0.0554\n",
      "Epoch [11/50], Step [340/777], Loss: 0.0148\n",
      "Epoch [11/50], Step [350/777], Loss: 0.0026\n",
      "Epoch [11/50], Step [360/777], Loss: 0.0037\n",
      "Epoch [11/50], Step [370/777], Loss: 0.0053\n",
      "Epoch [11/50], Step [380/777], Loss: 0.0136\n",
      "Epoch [11/50], Step [390/777], Loss: 0.0013\n",
      "Epoch [11/50], Step [400/777], Loss: 0.0013\n",
      "Epoch [11/50], Step [410/777], Loss: 0.0020\n",
      "Epoch [11/50], Step [420/777], Loss: 0.0045\n",
      "Epoch [11/50], Step [430/777], Loss: 0.0024\n",
      "Epoch [11/50], Step [440/777], Loss: 0.0037\n",
      "Epoch [11/50], Step [450/777], Loss: 0.0482\n",
      "Epoch [11/50], Step [460/777], Loss: 0.0122\n",
      "Epoch [11/50], Step [470/777], Loss: 0.0034\n",
      "Epoch [11/50], Step [480/777], Loss: 0.0013\n",
      "Epoch [11/50], Step [490/777], Loss: 0.0030\n",
      "Epoch [11/50], Step [500/777], Loss: 0.0030\n",
      "Epoch [11/50], Step [510/777], Loss: 0.0074\n",
      "Epoch [11/50], Step [520/777], Loss: 0.0051\n",
      "Epoch [11/50], Step [530/777], Loss: 0.0136\n",
      "Epoch [11/50], Step [540/777], Loss: 0.0069\n",
      "Epoch [11/50], Step [550/777], Loss: 0.0060\n",
      "Epoch [11/50], Step [560/777], Loss: 0.0177\n",
      "Epoch [11/50], Step [570/777], Loss: 0.0229\n",
      "Epoch [11/50], Step [580/777], Loss: 0.0893\n",
      "Epoch [11/50], Step [590/777], Loss: 0.0486\n",
      "Epoch [11/50], Step [600/777], Loss: 0.0450\n",
      "Epoch [11/50], Step [610/777], Loss: 0.0115\n",
      "Epoch [11/50], Step [620/777], Loss: 0.0028\n",
      "Epoch [11/50], Step [630/777], Loss: 0.0025\n",
      "Epoch [11/50], Step [640/777], Loss: 0.0020\n",
      "Epoch [11/50], Step [650/777], Loss: 0.0018\n",
      "Epoch [11/50], Step [660/777], Loss: 0.0040\n",
      "Epoch [11/50], Step [670/777], Loss: 0.0067\n",
      "Epoch [11/50], Step [680/777], Loss: 0.0026\n",
      "Epoch [11/50], Step [690/777], Loss: 0.0065\n",
      "Epoch [11/50], Step [700/777], Loss: 0.0070\n",
      "Epoch [11/50], Step [710/777], Loss: 0.0042\n",
      "Epoch [11/50], Step [720/777], Loss: 0.0016\n",
      "Epoch [11/50], Step [730/777], Loss: 0.0042\n",
      "Epoch [11/50], Step [740/777], Loss: 0.0187\n",
      "Epoch [11/50], Step [750/777], Loss: 0.0100\n",
      "Epoch [11/50], Step [760/777], Loss: 0.0043\n",
      "Epoch [11/50], Step [770/777], Loss: 0.0013\n",
      "Epoch [11/50], Train Loss: 0.0197, Val Loss: 0.2978, Val Accuracy: 0.9440\n",
      "Early stopping triggered after 11 epochs\n",
      "\n",
      "Evaluating MobileNetV2 on test set...\n",
      "\n",
      "MobileNetV2 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DJI       0.97      0.85      0.91       200\n",
      "   FutabaT14       0.98      0.87      0.92       548\n",
      "    FutabaT7       0.99      0.91      0.95        93\n",
      "    Graupner       0.96      0.98      0.97       107\n",
      "       Noise       0.92      0.99      0.96      1314\n",
      "     Taranis       1.00      0.99      0.99       268\n",
      "     Turnigy       0.98      0.95      0.97       133\n",
      "\n",
      "    accuracy                           0.95      2663\n",
      "   macro avg       0.97      0.94      0.95      2663\n",
      "weighted avg       0.95      0.95      0.95      2663\n",
      "\n",
      "\n",
      "MobileNetV2 Multi-class ROC AUC Score: 0.9894\n",
      "\n",
      "Analyzing MobileNetV2 performance by SNR levels...\n",
      "\n",
      "MobileNetV2 Performance by SNR level:\n",
      "SNR (dB) | Accuracy | Samples\n",
      "------------------------------\n",
      "\n",
      "MobileNetV2 Total Number of Parameters: 2,232,839\n",
      "MobileNetV2 Average Inference Time per Sample: 5.223 ms\n",
      "MobileNetV2 FLOPs: 326,215,680.0 (326.22 M)\n",
      "MobileNetV2 MACs: 2,232,839.0 (2.23 M)\n",
      "\n",
      "✅ MobileNetV2 Accuracy for class 'DJI': 85.50%\n",
      "✅ MobileNetV2 Accuracy for class 'FutabaT14': 87.41%\n",
      "✅ MobileNetV2 Accuracy for class 'FutabaT7': 91.40%\n",
      "✅ MobileNetV2 Accuracy for class 'Graupner': 98.13%\n",
      "✅ MobileNetV2 Accuracy for class 'Noise': 98.86%\n",
      "✅ MobileNetV2 Accuracy for class 'Taranis': 98.88%\n",
      "✅ MobileNetV2 Accuracy for class 'Turnigy': 95.49%\n",
      "\n",
      "✅ MobileNetV2 Test Set Accuracy: 0.950\n",
      "📊 MobileNetV2 Model Size: 8.52 MB\n",
      "\n",
      "Key characteristics of MobileNetV2:\n",
      "- Designed specifically for mobile and edge devices\n",
      "- Uses inverted residual blocks with linear bottlenecks\n",
      "- Employs depthwise separable convolutions for efficiency\n",
      "- Excellent trade-off between accuracy and computational cost\n",
      "- Well-suited for real-time applications with limited resources\n",
      "\n",
      "Metrics saved to mobilenetv2_drone_rf_metrics.json\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING INCEPTION-V3\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
      "100%|██████████| 104M/104M [00:00<00:00, 222MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training and Evaluating Inception-v3\n",
      "==================================================\n",
      "\n",
      "Inception-v3 Summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 149, 149]             864\n",
      "       BatchNorm2d-2         [-1, 32, 149, 149]              64\n",
      "       BasicConv2d-3         [-1, 32, 149, 149]               0\n",
      "            Conv2d-4         [-1, 32, 147, 147]           9,216\n",
      "       BatchNorm2d-5         [-1, 32, 147, 147]              64\n",
      "       BasicConv2d-6         [-1, 32, 147, 147]               0\n",
      "            Conv2d-7         [-1, 64, 147, 147]          18,432\n",
      "       BatchNorm2d-8         [-1, 64, 147, 147]             128\n",
      "       BasicConv2d-9         [-1, 64, 147, 147]               0\n",
      "        MaxPool2d-10           [-1, 64, 73, 73]               0\n",
      "           Conv2d-11           [-1, 80, 73, 73]           5,120\n",
      "      BatchNorm2d-12           [-1, 80, 73, 73]             160\n",
      "      BasicConv2d-13           [-1, 80, 73, 73]               0\n",
      "           Conv2d-14          [-1, 192, 71, 71]         138,240\n",
      "      BatchNorm2d-15          [-1, 192, 71, 71]             384\n",
      "      BasicConv2d-16          [-1, 192, 71, 71]               0\n",
      "        MaxPool2d-17          [-1, 192, 35, 35]               0\n",
      "           Conv2d-18           [-1, 64, 35, 35]          12,288\n",
      "      BatchNorm2d-19           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-20           [-1, 64, 35, 35]               0\n",
      "           Conv2d-21           [-1, 48, 35, 35]           9,216\n",
      "      BatchNorm2d-22           [-1, 48, 35, 35]              96\n",
      "      BasicConv2d-23           [-1, 48, 35, 35]               0\n",
      "           Conv2d-24           [-1, 64, 35, 35]          76,800\n",
      "      BatchNorm2d-25           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-26           [-1, 64, 35, 35]               0\n",
      "           Conv2d-27           [-1, 64, 35, 35]          12,288\n",
      "      BatchNorm2d-28           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-29           [-1, 64, 35, 35]               0\n",
      "           Conv2d-30           [-1, 96, 35, 35]          55,296\n",
      "      BatchNorm2d-31           [-1, 96, 35, 35]             192\n",
      "      BasicConv2d-32           [-1, 96, 35, 35]               0\n",
      "           Conv2d-33           [-1, 96, 35, 35]          82,944\n",
      "      BatchNorm2d-34           [-1, 96, 35, 35]             192\n",
      "      BasicConv2d-35           [-1, 96, 35, 35]               0\n",
      "           Conv2d-36           [-1, 32, 35, 35]           6,144\n",
      "      BatchNorm2d-37           [-1, 32, 35, 35]              64\n",
      "      BasicConv2d-38           [-1, 32, 35, 35]               0\n",
      "       InceptionA-39          [-1, 256, 35, 35]               0\n",
      "           Conv2d-40           [-1, 64, 35, 35]          16,384\n",
      "      BatchNorm2d-41           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-42           [-1, 64, 35, 35]               0\n",
      "           Conv2d-43           [-1, 48, 35, 35]          12,288\n",
      "      BatchNorm2d-44           [-1, 48, 35, 35]              96\n",
      "      BasicConv2d-45           [-1, 48, 35, 35]               0\n",
      "           Conv2d-46           [-1, 64, 35, 35]          76,800\n",
      "      BatchNorm2d-47           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-48           [-1, 64, 35, 35]               0\n",
      "           Conv2d-49           [-1, 64, 35, 35]          16,384\n",
      "      BatchNorm2d-50           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-51           [-1, 64, 35, 35]               0\n",
      "           Conv2d-52           [-1, 96, 35, 35]          55,296\n",
      "      BatchNorm2d-53           [-1, 96, 35, 35]             192\n",
      "      BasicConv2d-54           [-1, 96, 35, 35]               0\n",
      "           Conv2d-55           [-1, 96, 35, 35]          82,944\n",
      "      BatchNorm2d-56           [-1, 96, 35, 35]             192\n",
      "      BasicConv2d-57           [-1, 96, 35, 35]               0\n",
      "           Conv2d-58           [-1, 64, 35, 35]          16,384\n",
      "      BatchNorm2d-59           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-60           [-1, 64, 35, 35]               0\n",
      "       InceptionA-61          [-1, 288, 35, 35]               0\n",
      "           Conv2d-62           [-1, 64, 35, 35]          18,432\n",
      "      BatchNorm2d-63           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-64           [-1, 64, 35, 35]               0\n",
      "           Conv2d-65           [-1, 48, 35, 35]          13,824\n",
      "      BatchNorm2d-66           [-1, 48, 35, 35]              96\n",
      "      BasicConv2d-67           [-1, 48, 35, 35]               0\n",
      "           Conv2d-68           [-1, 64, 35, 35]          76,800\n",
      "      BatchNorm2d-69           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-70           [-1, 64, 35, 35]               0\n",
      "           Conv2d-71           [-1, 64, 35, 35]          18,432\n",
      "      BatchNorm2d-72           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-73           [-1, 64, 35, 35]               0\n",
      "           Conv2d-74           [-1, 96, 35, 35]          55,296\n",
      "      BatchNorm2d-75           [-1, 96, 35, 35]             192\n",
      "      BasicConv2d-76           [-1, 96, 35, 35]               0\n",
      "           Conv2d-77           [-1, 96, 35, 35]          82,944\n",
      "      BatchNorm2d-78           [-1, 96, 35, 35]             192\n",
      "      BasicConv2d-79           [-1, 96, 35, 35]               0\n",
      "           Conv2d-80           [-1, 64, 35, 35]          18,432\n",
      "      BatchNorm2d-81           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-82           [-1, 64, 35, 35]               0\n",
      "       InceptionA-83          [-1, 288, 35, 35]               0\n",
      "           Conv2d-84          [-1, 384, 17, 17]         995,328\n",
      "      BatchNorm2d-85          [-1, 384, 17, 17]             768\n",
      "      BasicConv2d-86          [-1, 384, 17, 17]               0\n",
      "           Conv2d-87           [-1, 64, 35, 35]          18,432\n",
      "      BatchNorm2d-88           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-89           [-1, 64, 35, 35]               0\n",
      "           Conv2d-90           [-1, 96, 35, 35]          55,296\n",
      "      BatchNorm2d-91           [-1, 96, 35, 35]             192\n",
      "      BasicConv2d-92           [-1, 96, 35, 35]               0\n",
      "           Conv2d-93           [-1, 96, 17, 17]          82,944\n",
      "      BatchNorm2d-94           [-1, 96, 17, 17]             192\n",
      "      BasicConv2d-95           [-1, 96, 17, 17]               0\n",
      "       InceptionB-96          [-1, 768, 17, 17]               0\n",
      "           Conv2d-97          [-1, 192, 17, 17]         147,456\n",
      "      BatchNorm2d-98          [-1, 192, 17, 17]             384\n",
      "      BasicConv2d-99          [-1, 192, 17, 17]               0\n",
      "          Conv2d-100          [-1, 128, 17, 17]          98,304\n",
      "     BatchNorm2d-101          [-1, 128, 17, 17]             256\n",
      "     BasicConv2d-102          [-1, 128, 17, 17]               0\n",
      "          Conv2d-103          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-104          [-1, 128, 17, 17]             256\n",
      "     BasicConv2d-105          [-1, 128, 17, 17]               0\n",
      "          Conv2d-106          [-1, 192, 17, 17]         172,032\n",
      "     BatchNorm2d-107          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-108          [-1, 192, 17, 17]               0\n",
      "          Conv2d-109          [-1, 128, 17, 17]          98,304\n",
      "     BatchNorm2d-110          [-1, 128, 17, 17]             256\n",
      "     BasicConv2d-111          [-1, 128, 17, 17]               0\n",
      "          Conv2d-112          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-113          [-1, 128, 17, 17]             256\n",
      "     BasicConv2d-114          [-1, 128, 17, 17]               0\n",
      "          Conv2d-115          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-116          [-1, 128, 17, 17]             256\n",
      "     BasicConv2d-117          [-1, 128, 17, 17]               0\n",
      "          Conv2d-118          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-119          [-1, 128, 17, 17]             256\n",
      "     BasicConv2d-120          [-1, 128, 17, 17]               0\n",
      "          Conv2d-121          [-1, 192, 17, 17]         172,032\n",
      "     BatchNorm2d-122          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-123          [-1, 192, 17, 17]               0\n",
      "          Conv2d-124          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-125          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-126          [-1, 192, 17, 17]               0\n",
      "      InceptionC-127          [-1, 768, 17, 17]               0\n",
      "          Conv2d-128          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-129          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-130          [-1, 192, 17, 17]               0\n",
      "          Conv2d-131          [-1, 160, 17, 17]         122,880\n",
      "     BatchNorm2d-132          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-133          [-1, 160, 17, 17]               0\n",
      "          Conv2d-134          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-135          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-136          [-1, 160, 17, 17]               0\n",
      "          Conv2d-137          [-1, 192, 17, 17]         215,040\n",
      "     BatchNorm2d-138          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-139          [-1, 192, 17, 17]               0\n",
      "          Conv2d-140          [-1, 160, 17, 17]         122,880\n",
      "     BatchNorm2d-141          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-142          [-1, 160, 17, 17]               0\n",
      "          Conv2d-143          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-144          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-145          [-1, 160, 17, 17]               0\n",
      "          Conv2d-146          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-147          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-148          [-1, 160, 17, 17]               0\n",
      "          Conv2d-149          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-150          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-151          [-1, 160, 17, 17]               0\n",
      "          Conv2d-152          [-1, 192, 17, 17]         215,040\n",
      "     BatchNorm2d-153          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-154          [-1, 192, 17, 17]               0\n",
      "          Conv2d-155          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-156          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-157          [-1, 192, 17, 17]               0\n",
      "      InceptionC-158          [-1, 768, 17, 17]               0\n",
      "          Conv2d-159          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-160          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-161          [-1, 192, 17, 17]               0\n",
      "          Conv2d-162          [-1, 160, 17, 17]         122,880\n",
      "     BatchNorm2d-163          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-164          [-1, 160, 17, 17]               0\n",
      "          Conv2d-165          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-166          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-167          [-1, 160, 17, 17]               0\n",
      "          Conv2d-168          [-1, 192, 17, 17]         215,040\n",
      "     BatchNorm2d-169          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-170          [-1, 192, 17, 17]               0\n",
      "          Conv2d-171          [-1, 160, 17, 17]         122,880\n",
      "     BatchNorm2d-172          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-173          [-1, 160, 17, 17]               0\n",
      "          Conv2d-174          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-175          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-176          [-1, 160, 17, 17]               0\n",
      "          Conv2d-177          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-178          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-179          [-1, 160, 17, 17]               0\n",
      "          Conv2d-180          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-181          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-182          [-1, 160, 17, 17]               0\n",
      "          Conv2d-183          [-1, 192, 17, 17]         215,040\n",
      "     BatchNorm2d-184          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-185          [-1, 192, 17, 17]               0\n",
      "          Conv2d-186          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-187          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-188          [-1, 192, 17, 17]               0\n",
      "      InceptionC-189          [-1, 768, 17, 17]               0\n",
      "          Conv2d-190          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-191          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-192          [-1, 192, 17, 17]               0\n",
      "          Conv2d-193          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-194          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-195          [-1, 192, 17, 17]               0\n",
      "          Conv2d-196          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-197          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-198          [-1, 192, 17, 17]               0\n",
      "          Conv2d-199          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-200          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-201          [-1, 192, 17, 17]               0\n",
      "          Conv2d-202          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-203          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-204          [-1, 192, 17, 17]               0\n",
      "          Conv2d-205          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-206          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-207          [-1, 192, 17, 17]               0\n",
      "          Conv2d-208          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-209          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-210          [-1, 192, 17, 17]               0\n",
      "          Conv2d-211          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-212          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-213          [-1, 192, 17, 17]               0\n",
      "          Conv2d-214          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-215          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-216          [-1, 192, 17, 17]               0\n",
      "          Conv2d-217          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-218          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-219          [-1, 192, 17, 17]               0\n",
      "      InceptionC-220          [-1, 768, 17, 17]               0\n",
      "          Conv2d-221            [-1, 128, 5, 5]          98,304\n",
      "     BatchNorm2d-222            [-1, 128, 5, 5]             256\n",
      "     BasicConv2d-223            [-1, 128, 5, 5]               0\n",
      "          Conv2d-224            [-1, 768, 1, 1]       2,457,600\n",
      "     BatchNorm2d-225            [-1, 768, 1, 1]           1,536\n",
      "     BasicConv2d-226            [-1, 768, 1, 1]               0\n",
      "          Linear-227                    [-1, 7]           5,383\n",
      "    InceptionAux-228                    [-1, 7]               0\n",
      "          Conv2d-229          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-230          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-231          [-1, 192, 17, 17]               0\n",
      "          Conv2d-232            [-1, 320, 8, 8]         552,960\n",
      "     BatchNorm2d-233            [-1, 320, 8, 8]             640\n",
      "     BasicConv2d-234            [-1, 320, 8, 8]               0\n",
      "          Conv2d-235          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-236          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-237          [-1, 192, 17, 17]               0\n",
      "          Conv2d-238          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-239          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-240          [-1, 192, 17, 17]               0\n",
      "          Conv2d-241          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-242          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-243          [-1, 192, 17, 17]               0\n",
      "          Conv2d-244            [-1, 192, 8, 8]         331,776\n",
      "     BatchNorm2d-245            [-1, 192, 8, 8]             384\n",
      "     BasicConv2d-246            [-1, 192, 8, 8]               0\n",
      "      InceptionD-247           [-1, 1280, 8, 8]               0\n",
      "          Conv2d-248            [-1, 320, 8, 8]         409,600\n",
      "     BatchNorm2d-249            [-1, 320, 8, 8]             640\n",
      "     BasicConv2d-250            [-1, 320, 8, 8]               0\n",
      "          Conv2d-251            [-1, 384, 8, 8]         491,520\n",
      "     BatchNorm2d-252            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-253            [-1, 384, 8, 8]               0\n",
      "          Conv2d-254            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-255            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-256            [-1, 384, 8, 8]               0\n",
      "          Conv2d-257            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-258            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-259            [-1, 384, 8, 8]               0\n",
      "          Conv2d-260            [-1, 448, 8, 8]         573,440\n",
      "     BatchNorm2d-261            [-1, 448, 8, 8]             896\n",
      "     BasicConv2d-262            [-1, 448, 8, 8]               0\n",
      "          Conv2d-263            [-1, 384, 8, 8]       1,548,288\n",
      "     BatchNorm2d-264            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-265            [-1, 384, 8, 8]               0\n",
      "          Conv2d-266            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-267            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-268            [-1, 384, 8, 8]               0\n",
      "          Conv2d-269            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-270            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-271            [-1, 384, 8, 8]               0\n",
      "          Conv2d-272            [-1, 192, 8, 8]         245,760\n",
      "     BatchNorm2d-273            [-1, 192, 8, 8]             384\n",
      "     BasicConv2d-274            [-1, 192, 8, 8]               0\n",
      "      InceptionE-275           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-276            [-1, 320, 8, 8]         655,360\n",
      "     BatchNorm2d-277            [-1, 320, 8, 8]             640\n",
      "     BasicConv2d-278            [-1, 320, 8, 8]               0\n",
      "          Conv2d-279            [-1, 384, 8, 8]         786,432\n",
      "     BatchNorm2d-280            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-281            [-1, 384, 8, 8]               0\n",
      "          Conv2d-282            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-283            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-284            [-1, 384, 8, 8]               0\n",
      "          Conv2d-285            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-286            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-287            [-1, 384, 8, 8]               0\n",
      "          Conv2d-288            [-1, 448, 8, 8]         917,504\n",
      "     BatchNorm2d-289            [-1, 448, 8, 8]             896\n",
      "     BasicConv2d-290            [-1, 448, 8, 8]               0\n",
      "          Conv2d-291            [-1, 384, 8, 8]       1,548,288\n",
      "     BatchNorm2d-292            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-293            [-1, 384, 8, 8]               0\n",
      "          Conv2d-294            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-295            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-296            [-1, 384, 8, 8]               0\n",
      "          Conv2d-297            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-298            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-299            [-1, 384, 8, 8]               0\n",
      "          Conv2d-300            [-1, 192, 8, 8]         393,216\n",
      "     BatchNorm2d-301            [-1, 192, 8, 8]             384\n",
      "     BasicConv2d-302            [-1, 192, 8, 8]               0\n",
      "      InceptionE-303           [-1, 2048, 8, 8]               0\n",
      "AdaptiveAvgPool2d-304           [-1, 2048, 1, 1]               0\n",
      "         Dropout-305           [-1, 2048, 1, 1]               0\n",
      "          Linear-306                    [-1, 7]          14,343\n",
      "================================================================\n",
      "Total params: 24,362,990\n",
      "Trainable params: 24,362,990\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.02\n",
      "Forward/backward pass size (MB): 228.64\n",
      "Params size (MB): 92.94\n",
      "Estimated Total Size (MB): 322.60\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Starting training Inception-v3...\n",
      "Epoch [1/50], Step [10/777], Loss: 2.1116\n",
      "Epoch [1/50], Step [20/777], Loss: 1.9140\n",
      "Epoch [1/50], Step [30/777], Loss: 2.1107\n",
      "Epoch [1/50], Step [40/777], Loss: 1.5409\n",
      "Epoch [1/50], Step [50/777], Loss: 1.1924\n",
      "Epoch [1/50], Step [60/777], Loss: 1.1292\n",
      "Epoch [1/50], Step [70/777], Loss: 1.1111\n",
      "Epoch [1/50], Step [80/777], Loss: 0.8245\n",
      "Epoch [1/50], Step [90/777], Loss: 1.0627\n",
      "Epoch [1/50], Step [100/777], Loss: 0.4800\n",
      "Epoch [1/50], Step [110/777], Loss: 1.3709\n",
      "Epoch [1/50], Step [120/777], Loss: 1.0073\n",
      "Epoch [1/50], Step [130/777], Loss: 0.2277\n",
      "Epoch [1/50], Step [140/777], Loss: 0.5763\n",
      "Epoch [1/50], Step [150/777], Loss: 0.9190\n",
      "Epoch [1/50], Step [160/777], Loss: 0.7299\n",
      "Epoch [1/50], Step [170/777], Loss: 0.5027\n",
      "Epoch [1/50], Step [180/777], Loss: 0.2105\n",
      "Epoch [1/50], Step [190/777], Loss: 0.3722\n",
      "Epoch [1/50], Step [200/777], Loss: 0.6447\n",
      "Epoch [1/50], Step [210/777], Loss: 1.4002\n",
      "Epoch [1/50], Step [220/777], Loss: 0.5156\n",
      "Epoch [1/50], Step [230/777], Loss: 0.1750\n",
      "Epoch [1/50], Step [240/777], Loss: 0.4613\n",
      "Epoch [1/50], Step [250/777], Loss: 0.8321\n",
      "Epoch [1/50], Step [260/777], Loss: 0.6010\n",
      "Epoch [1/50], Step [270/777], Loss: 0.3302\n",
      "Epoch [1/50], Step [280/777], Loss: 0.1907\n",
      "Epoch [1/50], Step [290/777], Loss: 0.5648\n",
      "Epoch [1/50], Step [300/777], Loss: 0.2275\n",
      "Epoch [1/50], Step [310/777], Loss: 0.2449\n",
      "Epoch [1/50], Step [320/777], Loss: 0.3705\n",
      "Epoch [1/50], Step [330/777], Loss: 0.5468\n",
      "Epoch [1/50], Step [340/777], Loss: 0.2067\n",
      "Epoch [1/50], Step [350/777], Loss: 0.0790\n",
      "Epoch [1/50], Step [360/777], Loss: 0.6676\n",
      "Epoch [1/50], Step [370/777], Loss: 0.2791\n",
      "Epoch [1/50], Step [380/777], Loss: 0.6807\n",
      "Epoch [1/50], Step [390/777], Loss: 0.2239\n",
      "Epoch [1/50], Step [400/777], Loss: 0.4531\n",
      "Epoch [1/50], Step [410/777], Loss: 0.2744\n",
      "Epoch [1/50], Step [420/777], Loss: 0.5635\n",
      "Epoch [1/50], Step [430/777], Loss: 0.0935\n",
      "Epoch [1/50], Step [440/777], Loss: 0.8454\n",
      "Epoch [1/50], Step [450/777], Loss: 0.8103\n",
      "Epoch [1/50], Step [460/777], Loss: 0.1203\n",
      "Epoch [1/50], Step [470/777], Loss: 1.0889\n",
      "Epoch [1/50], Step [480/777], Loss: 0.3096\n",
      "Epoch [1/50], Step [490/777], Loss: 1.4507\n",
      "Epoch [1/50], Step [500/777], Loss: 0.6969\n",
      "Epoch [1/50], Step [510/777], Loss: 0.4735\n",
      "Epoch [1/50], Step [520/777], Loss: 0.6751\n",
      "Epoch [1/50], Step [530/777], Loss: 0.6314\n",
      "Epoch [1/50], Step [540/777], Loss: 0.4731\n",
      "Epoch [1/50], Step [550/777], Loss: 0.1862\n",
      "Epoch [1/50], Step [560/777], Loss: 0.8527\n",
      "Epoch [1/50], Step [570/777], Loss: 0.2460\n",
      "Epoch [1/50], Step [580/777], Loss: 0.9368\n",
      "Epoch [1/50], Step [590/777], Loss: 0.5797\n",
      "Epoch [1/50], Step [600/777], Loss: 0.1584\n",
      "Epoch [1/50], Step [610/777], Loss: 0.3916\n",
      "Epoch [1/50], Step [620/777], Loss: 0.0828\n",
      "Epoch [1/50], Step [630/777], Loss: 0.5338\n",
      "Epoch [1/50], Step [640/777], Loss: 0.5471\n",
      "Epoch [1/50], Step [650/777], Loss: 0.1477\n",
      "Epoch [1/50], Step [660/777], Loss: 0.2301\n",
      "Epoch [1/50], Step [670/777], Loss: 0.1852\n",
      "Epoch [1/50], Step [680/777], Loss: 0.0799\n",
      "Epoch [1/50], Step [690/777], Loss: 1.3796\n",
      "Epoch [1/50], Step [700/777], Loss: 0.4598\n",
      "Epoch [1/50], Step [710/777], Loss: 1.1264\n",
      "Epoch [1/50], Step [720/777], Loss: 0.7422\n",
      "Epoch [1/50], Step [730/777], Loss: 0.3027\n",
      "Epoch [1/50], Step [740/777], Loss: 0.2057\n",
      "Epoch [1/50], Step [750/777], Loss: 0.4044\n",
      "Epoch [1/50], Step [760/777], Loss: 0.3452\n",
      "Epoch [1/50], Step [770/777], Loss: 0.4620\n",
      "Epoch [1/50], Train Loss: 0.6478, Val Loss: 0.2431, Val Accuracy: 0.9376\n",
      "Model saved with validation accuracy: 0.9376\n",
      "Epoch [2/50], Step [10/777], Loss: 0.4958\n",
      "Epoch [2/50], Step [20/777], Loss: 0.3199\n",
      "Epoch [2/50], Step [30/777], Loss: 0.3585\n",
      "Epoch [2/50], Step [40/777], Loss: 0.0935\n",
      "Epoch [2/50], Step [50/777], Loss: 0.4161\n",
      "Epoch [2/50], Step [60/777], Loss: 0.4408\n",
      "Epoch [2/50], Step [70/777], Loss: 0.2284\n",
      "Epoch [2/50], Step [80/777], Loss: 0.5144\n",
      "Epoch [2/50], Step [90/777], Loss: 0.1681\n",
      "Epoch [2/50], Step [100/777], Loss: 0.0651\n",
      "Epoch [2/50], Step [110/777], Loss: 0.0804\n",
      "Epoch [2/50], Step [120/777], Loss: 0.4022\n",
      "Epoch [2/50], Step [130/777], Loss: 0.4810\n",
      "Epoch [2/50], Step [140/777], Loss: 0.0759\n",
      "Epoch [2/50], Step [150/777], Loss: 0.3113\n",
      "Epoch [2/50], Step [160/777], Loss: 0.1984\n",
      "Epoch [2/50], Step [170/777], Loss: 0.1646\n",
      "Epoch [2/50], Step [180/777], Loss: 0.0824\n",
      "Epoch [2/50], Step [190/777], Loss: 0.0713\n",
      "Epoch [2/50], Step [200/777], Loss: 0.5280\n",
      "Epoch [2/50], Step [210/777], Loss: 0.1377\n",
      "Epoch [2/50], Step [220/777], Loss: 0.4971\n",
      "Epoch [2/50], Step [230/777], Loss: 0.0570\n",
      "Epoch [2/50], Step [240/777], Loss: 0.3205\n",
      "Epoch [2/50], Step [250/777], Loss: 1.0667\n",
      "Epoch [2/50], Step [260/777], Loss: 0.4785\n",
      "Epoch [2/50], Step [270/777], Loss: 0.0724\n",
      "Epoch [2/50], Step [280/777], Loss: 0.4075\n",
      "Epoch [2/50], Step [290/777], Loss: 0.2083\n",
      "Epoch [2/50], Step [300/777], Loss: 0.1989\n",
      "Epoch [2/50], Step [310/777], Loss: 0.0509\n",
      "Epoch [2/50], Step [320/777], Loss: 0.0685\n",
      "Epoch [2/50], Step [330/777], Loss: 0.9872\n",
      "Epoch [2/50], Step [340/777], Loss: 0.4304\n",
      "Epoch [2/50], Step [350/777], Loss: 0.2008\n",
      "Epoch [2/50], Step [360/777], Loss: 0.0853\n",
      "Epoch [2/50], Step [370/777], Loss: 0.0515\n",
      "Epoch [2/50], Step [380/777], Loss: 0.7267\n",
      "Epoch [2/50], Step [390/777], Loss: 0.7908\n",
      "Epoch [2/50], Step [400/777], Loss: 0.2577\n",
      "Epoch [2/50], Step [410/777], Loss: 0.1109\n",
      "Epoch [2/50], Step [420/777], Loss: 0.1216\n",
      "Epoch [2/50], Step [430/777], Loss: 0.1789\n",
      "Epoch [2/50], Step [440/777], Loss: 0.3548\n",
      "Epoch [2/50], Step [450/777], Loss: 1.0487\n",
      "Epoch [2/50], Step [460/777], Loss: 0.1060\n",
      "Epoch [2/50], Step [470/777], Loss: 0.2810\n",
      "Epoch [2/50], Step [480/777], Loss: 0.4939\n",
      "Epoch [2/50], Step [490/777], Loss: 0.1274\n",
      "Epoch [2/50], Step [500/777], Loss: 0.4826\n",
      "Epoch [2/50], Step [510/777], Loss: 0.4011\n",
      "Epoch [2/50], Step [520/777], Loss: 0.6306\n",
      "Epoch [2/50], Step [530/777], Loss: 0.2662\n",
      "Epoch [2/50], Step [540/777], Loss: 0.0831\n",
      "Epoch [2/50], Step [550/777], Loss: 0.0980\n",
      "Epoch [2/50], Step [560/777], Loss: 0.1861\n",
      "Epoch [2/50], Step [570/777], Loss: 0.9508\n",
      "Epoch [2/50], Step [580/777], Loss: 0.9601\n",
      "Epoch [2/50], Step [590/777], Loss: 0.4355\n",
      "Epoch [2/50], Step [600/777], Loss: 0.3564\n",
      "Epoch [2/50], Step [610/777], Loss: 0.0770\n",
      "Epoch [2/50], Step [620/777], Loss: 0.6260\n",
      "Epoch [2/50], Step [630/777], Loss: 0.1445\n",
      "Epoch [2/50], Step [640/777], Loss: 0.8507\n",
      "Epoch [2/50], Step [650/777], Loss: 0.3120\n",
      "Epoch [2/50], Step [660/777], Loss: 0.7255\n",
      "Epoch [2/50], Step [670/777], Loss: 0.2556\n",
      "Epoch [2/50], Step [680/777], Loss: 0.5718\n",
      "Epoch [2/50], Step [690/777], Loss: 0.0570\n",
      "Epoch [2/50], Step [700/777], Loss: 0.0686\n",
      "Epoch [2/50], Step [710/777], Loss: 0.0970\n",
      "Epoch [2/50], Step [720/777], Loss: 0.1582\n",
      "Epoch [2/50], Step [730/777], Loss: 0.0572\n",
      "Epoch [2/50], Step [740/777], Loss: 0.1806\n",
      "Epoch [2/50], Step [750/777], Loss: 0.1581\n",
      "Epoch [2/50], Step [760/777], Loss: 0.0363\n",
      "Epoch [2/50], Step [770/777], Loss: 0.0764\n",
      "Epoch [2/50], Train Loss: 0.3284, Val Loss: 0.2086, Val Accuracy: 0.9436\n",
      "Model saved with validation accuracy: 0.9436\n",
      "Epoch [3/50], Step [10/777], Loss: 0.2637\n",
      "Epoch [3/50], Step [20/777], Loss: 0.0633\n",
      "Epoch [3/50], Step [30/777], Loss: 0.9706\n",
      "Epoch [3/50], Step [40/777], Loss: 0.3618\n",
      "Epoch [3/50], Step [50/777], Loss: 0.6296\n",
      "Epoch [3/50], Step [60/777], Loss: 0.7369\n",
      "Epoch [3/50], Step [70/777], Loss: 0.1584\n",
      "Epoch [3/50], Step [80/777], Loss: 0.0354\n",
      "Epoch [3/50], Step [90/777], Loss: 0.3621\n",
      "Epoch [3/50], Step [100/777], Loss: 0.1685\n",
      "Epoch [3/50], Step [110/777], Loss: 0.0807\n",
      "Epoch [3/50], Step [120/777], Loss: 0.3548\n",
      "Epoch [3/50], Step [130/777], Loss: 0.2035\n",
      "Epoch [3/50], Step [140/777], Loss: 0.0384\n",
      "Epoch [3/50], Step [150/777], Loss: 0.4888\n",
      "Epoch [3/50], Step [160/777], Loss: 0.4447\n",
      "Epoch [3/50], Step [170/777], Loss: 0.1297\n",
      "Epoch [3/50], Step [180/777], Loss: 0.8358\n",
      "Epoch [3/50], Step [190/777], Loss: 0.2238\n",
      "Epoch [3/50], Step [200/777], Loss: 0.0624\n",
      "Epoch [3/50], Step [210/777], Loss: 0.0604\n",
      "Epoch [3/50], Step [220/777], Loss: 0.6042\n",
      "Epoch [3/50], Step [230/777], Loss: 0.0836\n",
      "Epoch [3/50], Step [240/777], Loss: 0.3064\n",
      "Epoch [3/50], Step [250/777], Loss: 0.2816\n",
      "Epoch [3/50], Step [260/777], Loss: 0.1149\n",
      "Epoch [3/50], Step [270/777], Loss: 0.0502\n",
      "Epoch [3/50], Step [280/777], Loss: 0.3745\n",
      "Epoch [3/50], Step [290/777], Loss: 0.2115\n",
      "Epoch [3/50], Step [300/777], Loss: 0.2294\n",
      "Epoch [3/50], Step [310/777], Loss: 0.5705\n",
      "Epoch [3/50], Step [320/777], Loss: 0.5076\n",
      "Epoch [3/50], Step [330/777], Loss: 0.0350\n",
      "Epoch [3/50], Step [340/777], Loss: 0.2407\n",
      "Epoch [3/50], Step [350/777], Loss: 0.5414\n",
      "Epoch [3/50], Step [360/777], Loss: 0.8188\n",
      "Epoch [3/50], Step [370/777], Loss: 0.1079\n",
      "Epoch [3/50], Step [380/777], Loss: 0.3347\n",
      "Epoch [3/50], Step [390/777], Loss: 0.0701\n",
      "Epoch [3/50], Step [400/777], Loss: 0.0429\n",
      "Epoch [3/50], Step [410/777], Loss: 0.5217\n",
      "Epoch [3/50], Step [420/777], Loss: 0.3903\n",
      "Epoch [3/50], Step [430/777], Loss: 0.5467\n",
      "Epoch [3/50], Step [440/777], Loss: 0.6546\n",
      "Epoch [3/50], Step [450/777], Loss: 0.0544\n",
      "Epoch [3/50], Step [460/777], Loss: 0.2918\n",
      "Epoch [3/50], Step [470/777], Loss: 0.2411\n",
      "Epoch [3/50], Step [480/777], Loss: 0.5415\n",
      "Epoch [3/50], Step [490/777], Loss: 0.2056\n",
      "Epoch [3/50], Step [500/777], Loss: 0.3437\n",
      "Epoch [3/50], Step [510/777], Loss: 0.1461\n",
      "Epoch [3/50], Step [520/777], Loss: 0.3364\n",
      "Epoch [3/50], Step [530/777], Loss: 0.1141\n",
      "Epoch [3/50], Step [540/777], Loss: 0.1529\n",
      "Epoch [3/50], Step [550/777], Loss: 0.0335\n",
      "Epoch [3/50], Step [560/777], Loss: 0.0189\n",
      "Epoch [3/50], Step [570/777], Loss: 0.6460\n",
      "Epoch [3/50], Step [580/777], Loss: 0.0986\n",
      "Epoch [3/50], Step [590/777], Loss: 0.1151\n",
      "Epoch [3/50], Step [600/777], Loss: 0.0658\n",
      "Epoch [3/50], Step [610/777], Loss: 0.0754\n",
      "Epoch [3/50], Step [620/777], Loss: 0.8072\n",
      "Epoch [3/50], Step [630/777], Loss: 0.0413\n",
      "Epoch [3/50], Step [640/777], Loss: 0.3407\n",
      "Epoch [3/50], Step [650/777], Loss: 0.6139\n",
      "Epoch [3/50], Step [660/777], Loss: 0.0639\n",
      "Epoch [3/50], Step [670/777], Loss: 0.4637\n",
      "Epoch [3/50], Step [680/777], Loss: 0.0341\n",
      "Epoch [3/50], Step [690/777], Loss: 0.0550\n",
      "Epoch [3/50], Step [700/777], Loss: 0.7943\n",
      "Epoch [3/50], Step [710/777], Loss: 0.3156\n",
      "Epoch [3/50], Step [720/777], Loss: 0.0914\n",
      "Epoch [3/50], Step [730/777], Loss: 0.0402\n",
      "Epoch [3/50], Step [740/777], Loss: 0.3504\n",
      "Epoch [3/50], Step [750/777], Loss: 0.3455\n",
      "Epoch [3/50], Step [760/777], Loss: 0.3231\n",
      "Epoch [3/50], Step [770/777], Loss: 0.6815\n",
      "Epoch [3/50], Train Loss: 0.2761, Val Loss: 0.2121, Val Accuracy: 0.9451\n",
      "Model saved with validation accuracy: 0.9451\n",
      "Epoch [4/50], Step [10/777], Loss: 0.1637\n",
      "Epoch [4/50], Step [20/777], Loss: 0.0460\n",
      "Epoch [4/50], Step [30/777], Loss: 0.0741\n",
      "Epoch [4/50], Step [40/777], Loss: 0.2482\n",
      "Epoch [4/50], Step [50/777], Loss: 0.7160\n",
      "Epoch [4/50], Step [60/777], Loss: 0.2855\n",
      "Epoch [4/50], Step [70/777], Loss: 0.0438\n",
      "Epoch [4/50], Step [80/777], Loss: 0.0459\n",
      "Epoch [4/50], Step [90/777], Loss: 0.0317\n",
      "Epoch [4/50], Step [100/777], Loss: 0.1494\n",
      "Epoch [4/50], Step [110/777], Loss: 0.0483\n",
      "Epoch [4/50], Step [120/777], Loss: 0.1620\n",
      "Epoch [4/50], Step [130/777], Loss: 0.0939\n",
      "Epoch [4/50], Step [140/777], Loss: 0.0227\n",
      "Epoch [4/50], Step [150/777], Loss: 0.2801\n",
      "Epoch [4/50], Step [160/777], Loss: 0.2559\n",
      "Epoch [4/50], Step [170/777], Loss: 0.5483\n",
      "Epoch [4/50], Step [180/777], Loss: 0.0859\n",
      "Epoch [4/50], Step [190/777], Loss: 0.2211\n",
      "Epoch [4/50], Step [200/777], Loss: 0.4460\n",
      "Epoch [4/50], Step [210/777], Loss: 0.0995\n",
      "Epoch [4/50], Step [220/777], Loss: 0.0583\n",
      "Epoch [4/50], Step [230/777], Loss: 0.2603\n",
      "Epoch [4/50], Step [240/777], Loss: 0.2544\n",
      "Epoch [4/50], Step [250/777], Loss: 0.2528\n",
      "Epoch [4/50], Step [260/777], Loss: 0.2140\n",
      "Epoch [4/50], Step [270/777], Loss: 0.2674\n",
      "Epoch [4/50], Step [280/777], Loss: 0.0406\n",
      "Epoch [4/50], Step [290/777], Loss: 0.1948\n",
      "Epoch [4/50], Step [300/777], Loss: 0.0671\n",
      "Epoch [4/50], Step [310/777], Loss: 0.0584\n",
      "Epoch [4/50], Step [320/777], Loss: 0.6408\n",
      "Epoch [4/50], Step [330/777], Loss: 0.0359\n",
      "Epoch [4/50], Step [340/777], Loss: 0.0276\n",
      "Epoch [4/50], Step [350/777], Loss: 0.9166\n",
      "Epoch [4/50], Step [360/777], Loss: 0.0971\n",
      "Epoch [4/50], Step [370/777], Loss: 0.1015\n",
      "Epoch [4/50], Step [380/777], Loss: 0.0454\n",
      "Epoch [4/50], Step [390/777], Loss: 0.2921\n",
      "Epoch [4/50], Step [400/777], Loss: 0.0511\n",
      "Epoch [4/50], Step [410/777], Loss: 0.7076\n",
      "Epoch [4/50], Step [420/777], Loss: 0.0949\n",
      "Epoch [4/50], Step [430/777], Loss: 0.5278\n",
      "Epoch [4/50], Step [440/777], Loss: 0.2463\n",
      "Epoch [4/50], Step [450/777], Loss: 0.4613\n",
      "Epoch [4/50], Step [460/777], Loss: 0.2178\n",
      "Epoch [4/50], Step [470/777], Loss: 0.1125\n",
      "Epoch [4/50], Step [480/777], Loss: 0.1127\n",
      "Epoch [4/50], Step [490/777], Loss: 0.1357\n",
      "Epoch [4/50], Step [500/777], Loss: 0.4886\n",
      "Epoch [4/50], Step [510/777], Loss: 0.0657\n",
      "Epoch [4/50], Step [520/777], Loss: 0.5399\n",
      "Epoch [4/50], Step [530/777], Loss: 0.4035\n",
      "Epoch [4/50], Step [540/777], Loss: 0.0789\n",
      "Epoch [4/50], Step [550/777], Loss: 0.0420\n",
      "Epoch [4/50], Step [560/777], Loss: 0.5781\n",
      "Epoch [4/50], Step [570/777], Loss: 0.3347\n",
      "Epoch [4/50], Step [580/777], Loss: 0.0686\n",
      "Epoch [4/50], Step [590/777], Loss: 0.8347\n",
      "Epoch [4/50], Step [600/777], Loss: 0.1331\n",
      "Epoch [4/50], Step [610/777], Loss: 0.0191\n",
      "Epoch [4/50], Step [620/777], Loss: 0.2476\n",
      "Epoch [4/50], Step [630/777], Loss: 0.2290\n",
      "Epoch [4/50], Step [640/777], Loss: 0.0650\n",
      "Epoch [4/50], Step [650/777], Loss: 0.8066\n",
      "Epoch [4/50], Step [660/777], Loss: 0.1459\n",
      "Epoch [4/50], Step [670/777], Loss: 0.5531\n",
      "Epoch [4/50], Step [680/777], Loss: 0.0524\n",
      "Epoch [4/50], Step [690/777], Loss: 0.0446\n",
      "Epoch [4/50], Step [700/777], Loss: 0.2835\n",
      "Epoch [4/50], Step [710/777], Loss: 0.3267\n",
      "Epoch [4/50], Step [720/777], Loss: 0.2145\n",
      "Epoch [4/50], Step [730/777], Loss: 0.0423\n",
      "Epoch [4/50], Step [740/777], Loss: 0.1115\n",
      "Epoch [4/50], Step [750/777], Loss: 0.0904\n",
      "Epoch [4/50], Step [760/777], Loss: 0.5225\n",
      "Epoch [4/50], Step [770/777], Loss: 0.0246\n",
      "Epoch [4/50], Train Loss: 0.2257, Val Loss: 0.2398, Val Accuracy: 0.9402\n",
      "Epoch [5/50], Step [10/777], Loss: 0.0608\n",
      "Epoch [5/50], Step [20/777], Loss: 0.0439\n",
      "Epoch [5/50], Step [30/777], Loss: 0.2690\n",
      "Epoch [5/50], Step [40/777], Loss: 0.2542\n",
      "Epoch [5/50], Step [50/777], Loss: 0.0520\n",
      "Epoch [5/50], Step [60/777], Loss: 0.3313\n",
      "Epoch [5/50], Step [70/777], Loss: 0.2304\n",
      "Epoch [5/50], Step [80/777], Loss: 0.2149\n",
      "Epoch [5/50], Step [90/777], Loss: 0.1398\n",
      "Epoch [5/50], Step [100/777], Loss: 0.3237\n",
      "Epoch [5/50], Step [110/777], Loss: 0.0471\n",
      "Epoch [5/50], Step [120/777], Loss: 0.0690\n",
      "Epoch [5/50], Step [130/777], Loss: 0.0485\n",
      "Epoch [5/50], Step [140/777], Loss: 0.2654\n",
      "Epoch [5/50], Step [150/777], Loss: 0.0539\n",
      "Epoch [5/50], Step [160/777], Loss: 0.0183\n",
      "Epoch [5/50], Step [170/777], Loss: 0.0957\n",
      "Epoch [5/50], Step [180/777], Loss: 0.0559\n",
      "Epoch [5/50], Step [190/777], Loss: 0.3539\n",
      "Epoch [5/50], Step [200/777], Loss: 0.0187\n",
      "Epoch [5/50], Step [210/777], Loss: 0.5542\n",
      "Epoch [5/50], Step [220/777], Loss: 0.5035\n",
      "Epoch [5/50], Step [230/777], Loss: 0.3455\n",
      "Epoch [5/50], Step [240/777], Loss: 0.1512\n",
      "Epoch [5/50], Step [250/777], Loss: 0.2677\n",
      "Epoch [5/50], Step [260/777], Loss: 1.4001\n",
      "Epoch [5/50], Step [270/777], Loss: 0.2379\n",
      "Epoch [5/50], Step [280/777], Loss: 0.1332\n",
      "Epoch [5/50], Step [290/777], Loss: 0.2651\n",
      "Epoch [5/50], Step [300/777], Loss: 0.0253\n",
      "Epoch [5/50], Step [310/777], Loss: 0.0355\n",
      "Epoch [5/50], Step [320/777], Loss: 0.1317\n",
      "Epoch [5/50], Step [330/777], Loss: 0.4014\n",
      "Epoch [5/50], Step [340/777], Loss: 0.0419\n",
      "Epoch [5/50], Step [350/777], Loss: 0.4476\n",
      "Epoch [5/50], Step [360/777], Loss: 0.9065\n",
      "Epoch [5/50], Step [370/777], Loss: 0.7330\n",
      "Epoch [5/50], Step [380/777], Loss: 0.0449\n",
      "Epoch [5/50], Step [390/777], Loss: 0.3572\n",
      "Epoch [5/50], Step [400/777], Loss: 0.2496\n",
      "Epoch [5/50], Step [410/777], Loss: 0.0280\n",
      "Epoch [5/50], Step [420/777], Loss: 0.2863\n",
      "Epoch [5/50], Step [430/777], Loss: 0.0411\n",
      "Epoch [5/50], Step [440/777], Loss: 0.1639\n",
      "Epoch [5/50], Step [450/777], Loss: 0.3469\n",
      "Epoch [5/50], Step [460/777], Loss: 0.2562\n",
      "Epoch [5/50], Step [470/777], Loss: 0.3182\n",
      "Epoch [5/50], Step [480/777], Loss: 0.3653\n",
      "Epoch [5/50], Step [490/777], Loss: 0.1669\n",
      "Epoch [5/50], Step [500/777], Loss: 0.3088\n",
      "Epoch [5/50], Step [510/777], Loss: 0.0441\n",
      "Epoch [5/50], Step [520/777], Loss: 0.0776\n",
      "Epoch [5/50], Step [530/777], Loss: 0.0755\n",
      "Epoch [5/50], Step [540/777], Loss: 0.0535\n",
      "Epoch [5/50], Step [550/777], Loss: 0.0464\n",
      "Epoch [5/50], Step [560/777], Loss: 0.0432\n",
      "Epoch [5/50], Step [570/777], Loss: 0.1425\n",
      "Epoch [5/50], Step [580/777], Loss: 0.0500\n",
      "Epoch [5/50], Step [590/777], Loss: 0.3677\n",
      "Epoch [5/50], Step [600/777], Loss: 0.1273\n",
      "Epoch [5/50], Step [610/777], Loss: 0.0259\n",
      "Epoch [5/50], Step [620/777], Loss: 0.1382\n",
      "Epoch [5/50], Step [630/777], Loss: 0.0224\n",
      "Epoch [5/50], Step [640/777], Loss: 1.0434\n",
      "Epoch [5/50], Step [650/777], Loss: 0.0546\n",
      "Epoch [5/50], Step [660/777], Loss: 0.0656\n",
      "Epoch [5/50], Step [670/777], Loss: 0.3511\n",
      "Epoch [5/50], Step [680/777], Loss: 0.2809\n",
      "Epoch [5/50], Step [690/777], Loss: 0.0537\n",
      "Epoch [5/50], Step [700/777], Loss: 0.3624\n",
      "Epoch [5/50], Step [710/777], Loss: 0.3609\n",
      "Epoch [5/50], Step [720/777], Loss: 0.1933\n",
      "Epoch [5/50], Step [730/777], Loss: 0.4024\n",
      "Epoch [5/50], Step [740/777], Loss: 0.0398\n",
      "Epoch [5/50], Step [750/777], Loss: 0.0408\n",
      "Epoch [5/50], Step [760/777], Loss: 0.0527\n",
      "Epoch [5/50], Step [770/777], Loss: 0.0306\n",
      "Epoch [5/50], Train Loss: 0.1971, Val Loss: 0.2179, Val Accuracy: 0.9470\n",
      "Model saved with validation accuracy: 0.9470\n",
      "Epoch [6/50], Step [10/777], Loss: 0.5456\n",
      "Epoch [6/50], Step [20/777], Loss: 0.0382\n",
      "Epoch [6/50], Step [30/777], Loss: 0.3197\n",
      "Epoch [6/50], Step [40/777], Loss: 0.0201\n",
      "Epoch [6/50], Step [50/777], Loss: 0.1688\n",
      "Epoch [6/50], Step [60/777], Loss: 0.2739\n",
      "Epoch [6/50], Step [70/777], Loss: 0.2238\n",
      "Epoch [6/50], Step [80/777], Loss: 0.1848\n",
      "Epoch [6/50], Step [90/777], Loss: 0.2589\n",
      "Epoch [6/50], Step [100/777], Loss: 0.0175\n",
      "Epoch [6/50], Step [110/777], Loss: 0.2706\n",
      "Epoch [6/50], Step [120/777], Loss: 0.4711\n",
      "Epoch [6/50], Step [130/777], Loss: 0.0339\n",
      "Epoch [6/50], Step [140/777], Loss: 0.0451\n",
      "Epoch [6/50], Step [150/777], Loss: 0.0303\n",
      "Epoch [6/50], Step [160/777], Loss: 0.0518\n",
      "Epoch [6/50], Step [170/777], Loss: 0.0281\n",
      "Epoch [6/50], Step [180/777], Loss: 0.0316\n",
      "Epoch [6/50], Step [190/777], Loss: 0.0241\n",
      "Epoch [6/50], Step [200/777], Loss: 0.0553\n",
      "Epoch [6/50], Step [210/777], Loss: 0.0108\n",
      "Epoch [6/50], Step [220/777], Loss: 0.0377\n",
      "Epoch [6/50], Step [230/777], Loss: 0.0261\n",
      "Epoch [6/50], Step [240/777], Loss: 0.0268\n",
      "Epoch [6/50], Step [250/777], Loss: 0.2203\n",
      "Epoch [6/50], Step [260/777], Loss: 0.0079\n",
      "Epoch [6/50], Step [270/777], Loss: 0.0404\n",
      "Epoch [6/50], Step [280/777], Loss: 0.0417\n",
      "Epoch [6/50], Step [290/777], Loss: 0.1515\n",
      "Epoch [6/50], Step [300/777], Loss: 0.0937\n",
      "Epoch [6/50], Step [310/777], Loss: 0.2620\n",
      "Epoch [6/50], Step [320/777], Loss: 0.3343\n",
      "Epoch [6/50], Step [330/777], Loss: 0.0303\n",
      "Epoch [6/50], Step [340/777], Loss: 0.1293\n",
      "Epoch [6/50], Step [350/777], Loss: 0.0645\n",
      "Epoch [6/50], Step [360/777], Loss: 0.0501\n",
      "Epoch [6/50], Step [370/777], Loss: 0.2199\n",
      "Epoch [6/50], Step [380/777], Loss: 0.0076\n",
      "Epoch [6/50], Step [390/777], Loss: 0.1314\n",
      "Epoch [6/50], Step [400/777], Loss: 0.0692\n",
      "Epoch [6/50], Step [410/777], Loss: 0.4752\n",
      "Epoch [6/50], Step [420/777], Loss: 0.0470\n",
      "Epoch [6/50], Step [430/777], Loss: 0.3250\n",
      "Epoch [6/50], Step [440/777], Loss: 0.0373\n",
      "Epoch [6/50], Step [450/777], Loss: 0.0367\n",
      "Epoch [6/50], Step [460/777], Loss: 0.0493\n",
      "Epoch [6/50], Step [470/777], Loss: 0.2657\n",
      "Epoch [6/50], Step [480/777], Loss: 0.5420\n",
      "Epoch [6/50], Step [490/777], Loss: 0.0335\n",
      "Epoch [6/50], Step [500/777], Loss: 0.0492\n",
      "Epoch [6/50], Step [510/777], Loss: 0.0243\n",
      "Epoch [6/50], Step [520/777], Loss: 0.0419\n",
      "Epoch [6/50], Step [530/777], Loss: 0.0126\n",
      "Epoch [6/50], Step [540/777], Loss: 0.0262\n",
      "Epoch [6/50], Step [550/777], Loss: 0.0201\n",
      "Epoch [6/50], Step [560/777], Loss: 0.2062\n",
      "Epoch [6/50], Step [570/777], Loss: 0.0186\n",
      "Epoch [6/50], Step [580/777], Loss: 0.3489\n",
      "Epoch [6/50], Step [590/777], Loss: 0.2027\n",
      "Epoch [6/50], Step [600/777], Loss: 0.2621\n",
      "Epoch [6/50], Step [610/777], Loss: 0.0631\n",
      "Epoch [6/50], Step [620/777], Loss: 0.5433\n",
      "Epoch [6/50], Step [630/777], Loss: 0.2246\n",
      "Epoch [6/50], Step [640/777], Loss: 0.0261\n",
      "Epoch [6/50], Step [650/777], Loss: 0.0231\n",
      "Epoch [6/50], Step [660/777], Loss: 0.1171\n",
      "Epoch [6/50], Step [670/777], Loss: 0.0305\n",
      "Epoch [6/50], Step [680/777], Loss: 0.1368\n",
      "Epoch [6/50], Step [690/777], Loss: 0.0114\n",
      "Epoch [6/50], Step [700/777], Loss: 0.1027\n",
      "Epoch [6/50], Step [710/777], Loss: 0.0180\n",
      "Epoch [6/50], Step [720/777], Loss: 0.0467\n",
      "Epoch [6/50], Step [730/777], Loss: 0.0096\n",
      "Epoch [6/50], Step [740/777], Loss: 0.4255\n",
      "Epoch [6/50], Step [750/777], Loss: 0.1252\n",
      "Epoch [6/50], Step [760/777], Loss: 0.0506\n",
      "Epoch [6/50], Step [770/777], Loss: 0.0125\n",
      "Epoch [6/50], Train Loss: 0.1635, Val Loss: 0.2484, Val Accuracy: 0.9474\n",
      "Model saved with validation accuracy: 0.9474\n",
      "Epoch [7/50], Step [10/777], Loss: 0.1201\n",
      "Epoch [7/50], Step [20/777], Loss: 0.1641\n",
      "Epoch [7/50], Step [30/777], Loss: 0.0894\n",
      "Epoch [7/50], Step [40/777], Loss: 0.3811\n",
      "Epoch [7/50], Step [50/777], Loss: 0.3536\n",
      "Epoch [7/50], Step [60/777], Loss: 0.0162\n",
      "Epoch [7/50], Step [70/777], Loss: 0.0279\n",
      "Epoch [7/50], Step [80/777], Loss: 0.0117\n",
      "Epoch [7/50], Step [90/777], Loss: 0.0167\n",
      "Epoch [7/50], Step [100/777], Loss: 0.1110\n",
      "Epoch [7/50], Step [110/777], Loss: 0.0479\n",
      "Epoch [7/50], Step [120/777], Loss: 0.1586\n",
      "Epoch [7/50], Step [130/777], Loss: 0.0068\n",
      "Epoch [7/50], Step [140/777], Loss: 0.0242\n",
      "Epoch [7/50], Step [150/777], Loss: 0.0080\n",
      "Epoch [7/50], Step [160/777], Loss: 0.0722\n",
      "Epoch [7/50], Step [170/777], Loss: 0.0123\n",
      "Epoch [7/50], Step [180/777], Loss: 0.2019\n",
      "Epoch [7/50], Step [190/777], Loss: 0.0868\n",
      "Epoch [7/50], Step [200/777], Loss: 0.1619\n",
      "Epoch [7/50], Step [210/777], Loss: 0.0124\n",
      "Epoch [7/50], Step [220/777], Loss: 0.0227\n",
      "Epoch [7/50], Step [230/777], Loss: 0.0151\n",
      "Epoch [7/50], Step [240/777], Loss: 0.0135\n",
      "Epoch [7/50], Step [250/777], Loss: 0.0159\n",
      "Epoch [7/50], Step [260/777], Loss: 0.2344\n",
      "Epoch [7/50], Step [270/777], Loss: 0.0163\n",
      "Epoch [7/50], Step [280/777], Loss: 0.1443\n",
      "Epoch [7/50], Step [290/777], Loss: 0.0435\n",
      "Epoch [7/50], Step [300/777], Loss: 0.0027\n",
      "Epoch [7/50], Step [310/777], Loss: 0.0274\n",
      "Epoch [7/50], Step [320/777], Loss: 0.2785\n",
      "Epoch [7/50], Step [330/777], Loss: 0.0510\n",
      "Epoch [7/50], Step [340/777], Loss: 0.3134\n",
      "Epoch [7/50], Step [350/777], Loss: 0.1795\n",
      "Epoch [7/50], Step [360/777], Loss: 0.0184\n",
      "Epoch [7/50], Step [370/777], Loss: 0.3817\n",
      "Epoch [7/50], Step [380/777], Loss: 0.0181\n",
      "Epoch [7/50], Step [390/777], Loss: 0.0168\n",
      "Epoch [7/50], Step [400/777], Loss: 0.0084\n",
      "Epoch [7/50], Step [410/777], Loss: 0.0265\n",
      "Epoch [7/50], Step [420/777], Loss: 0.0047\n",
      "Epoch [7/50], Step [430/777], Loss: 0.2733\n",
      "Epoch [7/50], Step [440/777], Loss: 0.1219\n",
      "Epoch [7/50], Step [450/777], Loss: 0.0810\n",
      "Epoch [7/50], Step [460/777], Loss: 0.0298\n",
      "Epoch [7/50], Step [470/777], Loss: 0.0286\n",
      "Epoch [7/50], Step [480/777], Loss: 0.1354\n",
      "Epoch [7/50], Step [490/777], Loss: 0.0118\n",
      "Epoch [7/50], Step [500/777], Loss: 1.0313\n",
      "Epoch [7/50], Step [510/777], Loss: 0.1696\n",
      "Epoch [7/50], Step [520/777], Loss: 0.0043\n",
      "Epoch [7/50], Step [530/777], Loss: 0.0484\n",
      "Epoch [7/50], Step [540/777], Loss: 0.0417\n",
      "Epoch [7/50], Step [550/777], Loss: 0.2228\n",
      "Epoch [7/50], Step [560/777], Loss: 0.0179\n",
      "Epoch [7/50], Step [570/777], Loss: 0.0122\n",
      "Epoch [7/50], Step [580/777], Loss: 0.1174\n",
      "Epoch [7/50], Step [590/777], Loss: 0.0196\n",
      "Epoch [7/50], Step [600/777], Loss: 0.0187\n",
      "Epoch [7/50], Step [610/777], Loss: 0.0114\n",
      "Epoch [7/50], Step [620/777], Loss: 0.1117\n",
      "Epoch [7/50], Step [630/777], Loss: 0.0065\n",
      "Epoch [7/50], Step [640/777], Loss: 0.0072\n",
      "Epoch [7/50], Step [650/777], Loss: 0.0599\n",
      "Epoch [7/50], Step [660/777], Loss: 0.0102\n",
      "Epoch [7/50], Step [670/777], Loss: 0.0109\n",
      "Epoch [7/50], Step [680/777], Loss: 0.0116\n",
      "Epoch [7/50], Step [690/777], Loss: 0.0103\n",
      "Epoch [7/50], Step [700/777], Loss: 0.0100\n",
      "Epoch [7/50], Step [710/777], Loss: 0.0102\n",
      "Epoch [7/50], Step [720/777], Loss: 0.0247\n",
      "Epoch [7/50], Step [730/777], Loss: 0.0473\n",
      "Epoch [7/50], Step [740/777], Loss: 0.0403\n",
      "Epoch [7/50], Step [750/777], Loss: 0.0134\n",
      "Epoch [7/50], Step [760/777], Loss: 0.0251\n",
      "Epoch [7/50], Step [770/777], Loss: 0.0153\n",
      "Epoch [7/50], Train Loss: 0.0887, Val Loss: 0.2161, Val Accuracy: 0.9538\n",
      "Model saved with validation accuracy: 0.9538\n",
      "Epoch [8/50], Step [10/777], Loss: 0.0499\n",
      "Epoch [8/50], Step [20/777], Loss: 0.0118\n",
      "Epoch [8/50], Step [30/777], Loss: 0.0521\n",
      "Epoch [8/50], Step [40/777], Loss: 0.0241\n",
      "Epoch [8/50], Step [50/777], Loss: 0.0182\n",
      "Epoch [8/50], Step [60/777], Loss: 0.0463\n",
      "Epoch [8/50], Step [70/777], Loss: 0.0094\n",
      "Epoch [8/50], Step [80/777], Loss: 0.0044\n",
      "Epoch [8/50], Step [90/777], Loss: 0.0070\n",
      "Epoch [8/50], Step [100/777], Loss: 0.0110\n",
      "Epoch [8/50], Step [110/777], Loss: 0.0130\n",
      "Epoch [8/50], Step [120/777], Loss: 0.0113\n",
      "Epoch [8/50], Step [130/777], Loss: 0.0152\n",
      "Epoch [8/50], Step [140/777], Loss: 0.0094\n",
      "Epoch [8/50], Step [150/777], Loss: 0.2986\n",
      "Epoch [8/50], Step [160/777], Loss: 0.0083\n",
      "Epoch [8/50], Step [170/777], Loss: 0.1036\n",
      "Epoch [8/50], Step [180/777], Loss: 0.0248\n",
      "Epoch [8/50], Step [190/777], Loss: 0.0404\n",
      "Epoch [8/50], Step [200/777], Loss: 0.0074\n",
      "Epoch [8/50], Step [210/777], Loss: 0.0657\n",
      "Epoch [8/50], Step [220/777], Loss: 0.0272\n",
      "Epoch [8/50], Step [230/777], Loss: 0.2422\n",
      "Epoch [8/50], Step [240/777], Loss: 0.1039\n",
      "Epoch [8/50], Step [250/777], Loss: 0.0659\n",
      "Epoch [8/50], Step [260/777], Loss: 0.0062\n",
      "Epoch [8/50], Step [270/777], Loss: 0.0184\n",
      "Epoch [8/50], Step [280/777], Loss: 0.0524\n",
      "Epoch [8/50], Step [290/777], Loss: 0.0223\n",
      "Epoch [8/50], Step [300/777], Loss: 0.0131\n",
      "Epoch [8/50], Step [310/777], Loss: 0.0079\n",
      "Epoch [8/50], Step [320/777], Loss: 0.0072\n",
      "Epoch [8/50], Step [330/777], Loss: 0.0533\n",
      "Epoch [8/50], Step [340/777], Loss: 0.0448\n",
      "Epoch [8/50], Step [350/777], Loss: 0.0056\n",
      "Epoch [8/50], Step [360/777], Loss: 0.2860\n",
      "Epoch [8/50], Step [370/777], Loss: 0.0190\n",
      "Epoch [8/50], Step [380/777], Loss: 0.0113\n",
      "Epoch [8/50], Step [390/777], Loss: 0.0684\n",
      "Epoch [8/50], Step [400/777], Loss: 0.0136\n",
      "Epoch [8/50], Step [410/777], Loss: 0.0601\n",
      "Epoch [8/50], Step [420/777], Loss: 0.0063\n",
      "Epoch [8/50], Step [430/777], Loss: 0.0074\n",
      "Epoch [8/50], Step [440/777], Loss: 0.0414\n",
      "Epoch [8/50], Step [450/777], Loss: 0.0059\n",
      "Epoch [8/50], Step [460/777], Loss: 0.0043\n",
      "Epoch [8/50], Step [470/777], Loss: 0.0154\n",
      "Epoch [8/50], Step [480/777], Loss: 0.0098\n",
      "Epoch [8/50], Step [490/777], Loss: 0.1325\n",
      "Epoch [8/50], Step [500/777], Loss: 0.0118\n",
      "Epoch [8/50], Step [510/777], Loss: 0.0057\n",
      "Epoch [8/50], Step [520/777], Loss: 0.1650\n",
      "Epoch [8/50], Step [530/777], Loss: 0.1304\n",
      "Epoch [8/50], Step [540/777], Loss: 0.0227\n",
      "Epoch [8/50], Step [550/777], Loss: 0.0065\n",
      "Epoch [8/50], Step [560/777], Loss: 0.0195\n",
      "Epoch [8/50], Step [570/777], Loss: 0.0115\n",
      "Epoch [8/50], Step [580/777], Loss: 0.0105\n",
      "Epoch [8/50], Step [590/777], Loss: 0.0709\n",
      "Epoch [8/50], Step [600/777], Loss: 0.0056\n",
      "Epoch [8/50], Step [610/777], Loss: 0.0063\n",
      "Epoch [8/50], Step [620/777], Loss: 0.0071\n",
      "Epoch [8/50], Step [630/777], Loss: 0.0627\n",
      "Epoch [8/50], Step [640/777], Loss: 0.0048\n",
      "Epoch [8/50], Step [650/777], Loss: 0.0631\n",
      "Epoch [8/50], Step [660/777], Loss: 0.0027\n",
      "Epoch [8/50], Step [670/777], Loss: 0.0216\n",
      "Epoch [8/50], Step [680/777], Loss: 0.0188\n",
      "Epoch [8/50], Step [690/777], Loss: 0.0038\n",
      "Epoch [8/50], Step [700/777], Loss: 0.0034\n",
      "Epoch [8/50], Step [710/777], Loss: 0.0024\n",
      "Epoch [8/50], Step [720/777], Loss: 0.1314\n",
      "Epoch [8/50], Step [730/777], Loss: 0.0265\n",
      "Epoch [8/50], Step [740/777], Loss: 0.0174\n",
      "Epoch [8/50], Step [750/777], Loss: 0.0577\n",
      "Epoch [8/50], Step [760/777], Loss: 0.0037\n",
      "Epoch [8/50], Step [770/777], Loss: 0.0424\n",
      "Epoch [8/50], Train Loss: 0.0520, Val Loss: 0.2615, Val Accuracy: 0.9504\n",
      "Epoch [9/50], Step [10/777], Loss: 0.0095\n",
      "Epoch [9/50], Step [20/777], Loss: 0.0317\n",
      "Epoch [9/50], Step [30/777], Loss: 0.0336\n",
      "Epoch [9/50], Step [40/777], Loss: 0.0068\n",
      "Epoch [9/50], Step [50/777], Loss: 0.0113\n",
      "Epoch [9/50], Step [60/777], Loss: 0.0106\n",
      "Epoch [9/50], Step [70/777], Loss: 0.0175\n",
      "Epoch [9/50], Step [80/777], Loss: 0.0047\n",
      "Epoch [9/50], Step [90/777], Loss: 0.0246\n",
      "Epoch [9/50], Step [100/777], Loss: 0.2155\n",
      "Epoch [9/50], Step [110/777], Loss: 0.0180\n",
      "Epoch [9/50], Step [120/777], Loss: 0.2148\n",
      "Epoch [9/50], Step [130/777], Loss: 0.0147\n",
      "Epoch [9/50], Step [140/777], Loss: 0.0848\n",
      "Epoch [9/50], Step [150/777], Loss: 0.0133\n",
      "Epoch [9/50], Step [160/777], Loss: 0.0512\n",
      "Epoch [9/50], Step [170/777], Loss: 0.0027\n",
      "Epoch [9/50], Step [180/777], Loss: 0.0090\n",
      "Epoch [9/50], Step [190/777], Loss: 0.0128\n",
      "Epoch [9/50], Step [200/777], Loss: 0.0020\n",
      "Epoch [9/50], Step [210/777], Loss: 0.0036\n",
      "Epoch [9/50], Step [220/777], Loss: 0.0088\n",
      "Epoch [9/50], Step [230/777], Loss: 0.0078\n",
      "Epoch [9/50], Step [240/777], Loss: 0.0026\n",
      "Epoch [9/50], Step [250/777], Loss: 0.0020\n",
      "Epoch [9/50], Step [260/777], Loss: 0.0022\n",
      "Epoch [9/50], Step [270/777], Loss: 0.0900\n",
      "Epoch [9/50], Step [280/777], Loss: 0.0042\n",
      "Epoch [9/50], Step [290/777], Loss: 0.0053\n",
      "Epoch [9/50], Step [300/777], Loss: 0.0061\n",
      "Epoch [9/50], Step [310/777], Loss: 0.0056\n",
      "Epoch [9/50], Step [320/777], Loss: 0.0057\n",
      "Epoch [9/50], Step [330/777], Loss: 0.0125\n",
      "Epoch [9/50], Step [340/777], Loss: 0.0025\n",
      "Epoch [9/50], Step [350/777], Loss: 0.0053\n",
      "Epoch [9/50], Step [360/777], Loss: 0.0092\n",
      "Epoch [9/50], Step [370/777], Loss: 0.0019\n",
      "Epoch [9/50], Step [380/777], Loss: 0.0020\n",
      "Epoch [9/50], Step [390/777], Loss: 0.1507\n",
      "Epoch [9/50], Step [400/777], Loss: 0.0056\n",
      "Epoch [9/50], Step [410/777], Loss: 0.0037\n",
      "Epoch [9/50], Step [420/777], Loss: 0.0118\n",
      "Epoch [9/50], Step [430/777], Loss: 0.0088\n",
      "Epoch [9/50], Step [440/777], Loss: 0.0046\n",
      "Epoch [9/50], Step [450/777], Loss: 0.0072\n",
      "Epoch [9/50], Step [460/777], Loss: 0.2117\n",
      "Epoch [9/50], Step [470/777], Loss: 0.0897\n",
      "Epoch [9/50], Step [480/777], Loss: 0.0232\n",
      "Epoch [9/50], Step [490/777], Loss: 0.0251\n",
      "Epoch [9/50], Step [500/777], Loss: 0.0125\n",
      "Epoch [9/50], Step [510/777], Loss: 0.0151\n",
      "Epoch [9/50], Step [520/777], Loss: 0.0023\n",
      "Epoch [9/50], Step [530/777], Loss: 0.0274\n",
      "Epoch [9/50], Step [540/777], Loss: 0.0017\n",
      "Epoch [9/50], Step [550/777], Loss: 0.0106\n",
      "Epoch [9/50], Step [560/777], Loss: 0.0040\n",
      "Epoch [9/50], Step [570/777], Loss: 0.0099\n",
      "Epoch [9/50], Step [580/777], Loss: 0.0034\n",
      "Epoch [9/50], Step [590/777], Loss: 0.0037\n",
      "Epoch [9/50], Step [600/777], Loss: 0.0026\n",
      "Epoch [9/50], Step [610/777], Loss: 0.0036\n",
      "Epoch [9/50], Step [620/777], Loss: 0.0096\n",
      "Epoch [9/50], Step [630/777], Loss: 0.0026\n",
      "Epoch [9/50], Step [640/777], Loss: 0.0259\n",
      "Epoch [9/50], Step [650/777], Loss: 0.0077\n",
      "Epoch [9/50], Step [660/777], Loss: 0.0054\n",
      "Epoch [9/50], Step [670/777], Loss: 0.0030\n",
      "Epoch [9/50], Step [680/777], Loss: 0.0028\n",
      "Epoch [9/50], Step [690/777], Loss: 0.0108\n",
      "Epoch [9/50], Step [700/777], Loss: 0.0037\n",
      "Epoch [9/50], Step [710/777], Loss: 0.0040\n",
      "Epoch [9/50], Step [720/777], Loss: 0.0025\n",
      "Epoch [9/50], Step [730/777], Loss: 0.0090\n",
      "Epoch [9/50], Step [740/777], Loss: 0.0311\n",
      "Epoch [9/50], Step [750/777], Loss: 0.0061\n",
      "Epoch [9/50], Step [760/777], Loss: 0.4008\n",
      "Epoch [9/50], Step [770/777], Loss: 0.0090\n",
      "Epoch [9/50], Train Loss: 0.0338, Val Loss: 0.2939, Val Accuracy: 0.9478\n",
      "Epoch [10/50], Step [10/777], Loss: 0.0307\n",
      "Epoch [10/50], Step [20/777], Loss: 0.0039\n",
      "Epoch [10/50], Step [30/777], Loss: 0.0516\n",
      "Epoch [10/50], Step [40/777], Loss: 0.0163\n",
      "Epoch [10/50], Step [50/777], Loss: 0.0019\n",
      "Epoch [10/50], Step [60/777], Loss: 0.0096\n",
      "Epoch [10/50], Step [70/777], Loss: 0.0411\n",
      "Epoch [10/50], Step [80/777], Loss: 0.0053\n",
      "Epoch [10/50], Step [90/777], Loss: 0.1087\n",
      "Epoch [10/50], Step [100/777], Loss: 0.0057\n",
      "Epoch [10/50], Step [110/777], Loss: 0.0600\n",
      "Epoch [10/50], Step [120/777], Loss: 0.0023\n",
      "Epoch [10/50], Step [130/777], Loss: 0.0032\n",
      "Epoch [10/50], Step [140/777], Loss: 0.0163\n",
      "Epoch [10/50], Step [150/777], Loss: 0.0028\n",
      "Epoch [10/50], Step [160/777], Loss: 0.0150\n",
      "Epoch [10/50], Step [170/777], Loss: 0.0051\n",
      "Epoch [10/50], Step [180/777], Loss: 0.0057\n",
      "Epoch [10/50], Step [190/777], Loss: 0.0986\n",
      "Epoch [10/50], Step [200/777], Loss: 0.0059\n",
      "Epoch [10/50], Step [210/777], Loss: 0.0215\n",
      "Epoch [10/50], Step [220/777], Loss: 0.0019\n",
      "Epoch [10/50], Step [230/777], Loss: 0.0021\n",
      "Epoch [10/50], Step [240/777], Loss: 0.0159\n",
      "Epoch [10/50], Step [250/777], Loss: 0.0022\n",
      "Epoch [10/50], Step [260/777], Loss: 0.0151\n",
      "Epoch [10/50], Step [270/777], Loss: 0.0083\n",
      "Epoch [10/50], Step [280/777], Loss: 0.0106\n",
      "Epoch [10/50], Step [290/777], Loss: 0.0038\n",
      "Epoch [10/50], Step [300/777], Loss: 0.0021\n",
      "Epoch [10/50], Step [310/777], Loss: 0.0019\n",
      "Epoch [10/50], Step [320/777], Loss: 0.0013\n",
      "Epoch [10/50], Step [330/777], Loss: 0.0015\n",
      "Epoch [10/50], Step [340/777], Loss: 0.0199\n",
      "Epoch [10/50], Step [350/777], Loss: 0.0056\n",
      "Epoch [10/50], Step [360/777], Loss: 0.0076\n",
      "Epoch [10/50], Step [370/777], Loss: 0.1762\n",
      "Epoch [10/50], Step [380/777], Loss: 0.0088\n",
      "Epoch [10/50], Step [390/777], Loss: 0.0047\n",
      "Epoch [10/50], Step [400/777], Loss: 0.0040\n",
      "Epoch [10/50], Step [410/777], Loss: 0.0004\n",
      "Epoch [10/50], Step [420/777], Loss: 0.0030\n",
      "Epoch [10/50], Step [430/777], Loss: 0.0015\n",
      "Epoch [10/50], Step [440/777], Loss: 0.0009\n",
      "Epoch [10/50], Step [450/777], Loss: 0.0048\n",
      "Epoch [10/50], Step [460/777], Loss: 0.0050\n",
      "Epoch [10/50], Step [470/777], Loss: 0.0168\n",
      "Epoch [10/50], Step [480/777], Loss: 0.0897\n",
      "Epoch [10/50], Step [490/777], Loss: 0.0454\n",
      "Epoch [10/50], Step [500/777], Loss: 0.0013\n",
      "Epoch [10/50], Step [510/777], Loss: 0.0029\n",
      "Epoch [10/50], Step [520/777], Loss: 0.0107\n",
      "Epoch [10/50], Step [530/777], Loss: 0.0009\n",
      "Epoch [10/50], Step [540/777], Loss: 0.0292\n",
      "Epoch [10/50], Step [550/777], Loss: 0.0125\n",
      "Epoch [10/50], Step [560/777], Loss: 0.0019\n",
      "Epoch [10/50], Step [570/777], Loss: 0.0033\n",
      "Epoch [10/50], Step [580/777], Loss: 0.0053\n",
      "Epoch [10/50], Step [590/777], Loss: 0.0006\n",
      "Epoch [10/50], Step [600/777], Loss: 0.0080\n",
      "Epoch [10/50], Step [610/777], Loss: 0.0022\n",
      "Epoch [10/50], Step [620/777], Loss: 0.0060\n",
      "Epoch [10/50], Step [630/777], Loss: 0.0156\n",
      "Epoch [10/50], Step [640/777], Loss: 0.0172\n",
      "Epoch [10/50], Step [650/777], Loss: 0.0056\n",
      "Epoch [10/50], Step [660/777], Loss: 0.0074\n",
      "Epoch [10/50], Step [670/777], Loss: 0.0009\n",
      "Epoch [10/50], Step [680/777], Loss: 0.0016\n",
      "Epoch [10/50], Step [690/777], Loss: 0.0035\n",
      "Epoch [10/50], Step [700/777], Loss: 0.0170\n",
      "Epoch [10/50], Step [710/777], Loss: 0.0070\n",
      "Epoch [10/50], Step [720/777], Loss: 0.0079\n",
      "Epoch [10/50], Step [730/777], Loss: 0.0027\n",
      "Epoch [10/50], Step [740/777], Loss: 0.0881\n",
      "Epoch [10/50], Step [750/777], Loss: 0.0055\n",
      "Epoch [10/50], Step [760/777], Loss: 0.0015\n",
      "Epoch [10/50], Step [770/777], Loss: 0.0341\n",
      "Epoch [10/50], Train Loss: 0.0238, Val Loss: 0.3160, Val Accuracy: 0.9504\n",
      "Epoch [11/50], Step [10/777], Loss: 0.0029\n",
      "Epoch [11/50], Step [20/777], Loss: 0.0035\n",
      "Epoch [11/50], Step [30/777], Loss: 0.0038\n",
      "Epoch [11/50], Step [40/777], Loss: 0.0136\n",
      "Epoch [11/50], Step [50/777], Loss: 0.0068\n",
      "Epoch [11/50], Step [60/777], Loss: 0.0011\n",
      "Epoch [11/50], Step [70/777], Loss: 0.0023\n",
      "Epoch [11/50], Step [80/777], Loss: 0.0024\n",
      "Epoch [11/50], Step [90/777], Loss: 0.0015\n",
      "Epoch [11/50], Step [100/777], Loss: 0.0016\n",
      "Epoch [11/50], Step [110/777], Loss: 0.0046\n",
      "Epoch [11/50], Step [120/777], Loss: 0.0019\n",
      "Epoch [11/50], Step [130/777], Loss: 0.0036\n",
      "Epoch [11/50], Step [140/777], Loss: 0.0190\n",
      "Epoch [11/50], Step [150/777], Loss: 0.0069\n",
      "Epoch [11/50], Step [160/777], Loss: 0.0016\n",
      "Epoch [11/50], Step [170/777], Loss: 0.0157\n",
      "Epoch [11/50], Step [180/777], Loss: 0.0031\n",
      "Epoch [11/50], Step [190/777], Loss: 0.0024\n",
      "Epoch [11/50], Step [200/777], Loss: 0.0247\n",
      "Epoch [11/50], Step [210/777], Loss: 0.0201\n",
      "Epoch [11/50], Step [220/777], Loss: 0.0030\n",
      "Epoch [11/50], Step [230/777], Loss: 0.0020\n",
      "Epoch [11/50], Step [240/777], Loss: 0.0062\n",
      "Epoch [11/50], Step [250/777], Loss: 0.0036\n",
      "Epoch [11/50], Step [260/777], Loss: 0.0019\n",
      "Epoch [11/50], Step [270/777], Loss: 0.0026\n",
      "Epoch [11/50], Step [280/777], Loss: 0.0043\n",
      "Epoch [11/50], Step [290/777], Loss: 0.0026\n",
      "Epoch [11/50], Step [300/777], Loss: 0.0052\n",
      "Epoch [11/50], Step [310/777], Loss: 0.0051\n",
      "Epoch [11/50], Step [320/777], Loss: 0.0039\n",
      "Epoch [11/50], Step [330/777], Loss: 0.0069\n",
      "Epoch [11/50], Step [340/777], Loss: 0.0042\n",
      "Epoch [11/50], Step [350/777], Loss: 0.0218\n",
      "Epoch [11/50], Step [360/777], Loss: 0.0017\n",
      "Epoch [11/50], Step [370/777], Loss: 0.0028\n",
      "Epoch [11/50], Step [380/777], Loss: 0.0015\n",
      "Epoch [11/50], Step [390/777], Loss: 0.0010\n",
      "Epoch [11/50], Step [400/777], Loss: 0.0023\n",
      "Epoch [11/50], Step [410/777], Loss: 0.0038\n",
      "Epoch [11/50], Step [420/777], Loss: 0.0031\n",
      "Epoch [11/50], Step [430/777], Loss: 0.0084\n",
      "Epoch [11/50], Step [440/777], Loss: 0.0038\n",
      "Epoch [11/50], Step [450/777], Loss: 0.1772\n",
      "Epoch [11/50], Step [460/777], Loss: 0.0041\n",
      "Epoch [11/50], Step [470/777], Loss: 0.0035\n",
      "Epoch [11/50], Step [480/777], Loss: 0.0033\n",
      "Epoch [11/50], Step [490/777], Loss: 0.0032\n",
      "Epoch [11/50], Step [500/777], Loss: 0.0021\n",
      "Epoch [11/50], Step [510/777], Loss: 0.0144\n",
      "Epoch [11/50], Step [520/777], Loss: 0.0463\n",
      "Epoch [11/50], Step [530/777], Loss: 0.0016\n",
      "Epoch [11/50], Step [540/777], Loss: 0.0234\n",
      "Epoch [11/50], Step [550/777], Loss: 0.0022\n",
      "Epoch [11/50], Step [560/777], Loss: 0.0014\n",
      "Epoch [11/50], Step [570/777], Loss: 0.0095\n",
      "Epoch [11/50], Step [580/777], Loss: 0.0217\n",
      "Epoch [11/50], Step [590/777], Loss: 0.0074\n",
      "Epoch [11/50], Step [600/777], Loss: 0.0008\n",
      "Epoch [11/50], Step [610/777], Loss: 0.0031\n",
      "Epoch [11/50], Step [620/777], Loss: 0.0091\n",
      "Epoch [11/50], Step [630/777], Loss: 0.0028\n",
      "Epoch [11/50], Step [640/777], Loss: 0.0077\n",
      "Epoch [11/50], Step [650/777], Loss: 0.0034\n",
      "Epoch [11/50], Step [660/777], Loss: 0.0233\n",
      "Epoch [11/50], Step [670/777], Loss: 0.2042\n",
      "Epoch [11/50], Step [680/777], Loss: 0.0044\n",
      "Epoch [11/50], Step [690/777], Loss: 0.1142\n",
      "Epoch [11/50], Step [700/777], Loss: 0.0010\n",
      "Epoch [11/50], Step [710/777], Loss: 0.0018\n",
      "Epoch [11/50], Step [720/777], Loss: 0.0019\n",
      "Epoch [11/50], Step [730/777], Loss: 0.0356\n",
      "Epoch [11/50], Step [740/777], Loss: 0.0020\n",
      "Epoch [11/50], Step [750/777], Loss: 0.0947\n",
      "Epoch [11/50], Step [760/777], Loss: 0.0138\n",
      "Epoch [11/50], Step [770/777], Loss: 0.0064\n",
      "Epoch [11/50], Train Loss: 0.0168, Val Loss: 0.3184, Val Accuracy: 0.9496\n",
      "Epoch [12/50], Step [10/777], Loss: 0.0990\n",
      "Epoch [12/50], Step [20/777], Loss: 0.0014\n",
      "Epoch [12/50], Step [30/777], Loss: 0.0021\n",
      "Epoch [12/50], Step [40/777], Loss: 0.0090\n",
      "Epoch [12/50], Step [50/777], Loss: 0.0025\n",
      "Epoch [12/50], Step [60/777], Loss: 0.0007\n",
      "Epoch [12/50], Step [70/777], Loss: 0.0042\n",
      "Epoch [12/50], Step [80/777], Loss: 0.0493\n",
      "Epoch [12/50], Step [90/777], Loss: 0.0108\n",
      "Epoch [12/50], Step [100/777], Loss: 0.0149\n",
      "Epoch [12/50], Step [110/777], Loss: 0.0025\n",
      "Epoch [12/50], Step [120/777], Loss: 0.0105\n",
      "Epoch [12/50], Step [130/777], Loss: 0.2630\n",
      "Epoch [12/50], Step [140/777], Loss: 0.0053\n",
      "Epoch [12/50], Step [150/777], Loss: 0.0069\n",
      "Epoch [12/50], Step [160/777], Loss: 0.0399\n",
      "Epoch [12/50], Step [170/777], Loss: 0.0040\n",
      "Epoch [12/50], Step [180/777], Loss: 0.0217\n",
      "Epoch [12/50], Step [190/777], Loss: 0.0045\n",
      "Epoch [12/50], Step [200/777], Loss: 0.0074\n",
      "Epoch [12/50], Step [210/777], Loss: 0.0142\n",
      "Epoch [12/50], Step [220/777], Loss: 0.0157\n",
      "Epoch [12/50], Step [230/777], Loss: 0.0051\n",
      "Epoch [12/50], Step [240/777], Loss: 0.0020\n",
      "Epoch [12/50], Step [250/777], Loss: 0.0015\n",
      "Epoch [12/50], Step [260/777], Loss: 0.0160\n",
      "Epoch [12/50], Step [270/777], Loss: 0.0045\n",
      "Epoch [12/50], Step [280/777], Loss: 0.0111\n",
      "Epoch [12/50], Step [290/777], Loss: 0.0034\n",
      "Epoch [12/50], Step [300/777], Loss: 0.0026\n",
      "Epoch [12/50], Step [310/777], Loss: 0.0403\n",
      "Epoch [12/50], Step [320/777], Loss: 0.0026\n",
      "Epoch [12/50], Step [330/777], Loss: 0.0021\n",
      "Epoch [12/50], Step [340/777], Loss: 0.0311\n",
      "Epoch [12/50], Step [350/777], Loss: 0.0027\n",
      "Epoch [12/50], Step [360/777], Loss: 0.0709\n",
      "Epoch [12/50], Step [370/777], Loss: 0.0092\n",
      "Epoch [12/50], Step [380/777], Loss: 0.0451\n",
      "Epoch [12/50], Step [390/777], Loss: 0.0052\n",
      "Epoch [12/50], Step [400/777], Loss: 0.0041\n",
      "Epoch [12/50], Step [410/777], Loss: 0.1551\n",
      "Epoch [12/50], Step [420/777], Loss: 0.0028\n",
      "Epoch [12/50], Step [430/777], Loss: 0.0035\n",
      "Epoch [12/50], Step [440/777], Loss: 0.0033\n",
      "Epoch [12/50], Step [450/777], Loss: 0.0027\n",
      "Epoch [12/50], Step [460/777], Loss: 0.0026\n",
      "Epoch [12/50], Step [470/777], Loss: 0.0018\n",
      "Epoch [12/50], Step [480/777], Loss: 0.0009\n",
      "Epoch [12/50], Step [490/777], Loss: 0.0020\n",
      "Epoch [12/50], Step [500/777], Loss: 0.0180\n",
      "Epoch [12/50], Step [510/777], Loss: 0.0036\n",
      "Epoch [12/50], Step [520/777], Loss: 0.0698\n",
      "Epoch [12/50], Step [530/777], Loss: 0.0110\n",
      "Epoch [12/50], Step [540/777], Loss: 0.0053\n",
      "Epoch [12/50], Step [550/777], Loss: 0.0080\n",
      "Epoch [12/50], Step [560/777], Loss: 0.0031\n",
      "Epoch [12/50], Step [570/777], Loss: 0.0072\n",
      "Epoch [12/50], Step [580/777], Loss: 0.0019\n",
      "Epoch [12/50], Step [590/777], Loss: 0.0049\n",
      "Epoch [12/50], Step [600/777], Loss: 0.0088\n",
      "Epoch [12/50], Step [610/777], Loss: 0.0015\n",
      "Epoch [12/50], Step [620/777], Loss: 0.0191\n",
      "Epoch [12/50], Step [630/777], Loss: 0.0043\n",
      "Epoch [12/50], Step [640/777], Loss: 0.0053\n",
      "Epoch [12/50], Step [650/777], Loss: 0.0121\n",
      "Epoch [12/50], Step [660/777], Loss: 0.0095\n",
      "Epoch [12/50], Step [670/777], Loss: 0.0009\n",
      "Epoch [12/50], Step [680/777], Loss: 0.0043\n",
      "Epoch [12/50], Step [690/777], Loss: 0.0011\n",
      "Epoch [12/50], Step [700/777], Loss: 0.0006\n",
      "Epoch [12/50], Step [710/777], Loss: 0.0125\n",
      "Epoch [12/50], Step [720/777], Loss: 0.0024\n",
      "Epoch [12/50], Step [730/777], Loss: 0.0067\n",
      "Epoch [12/50], Step [740/777], Loss: 0.0016\n",
      "Epoch [12/50], Step [750/777], Loss: 0.0010\n",
      "Epoch [12/50], Step [760/777], Loss: 0.0065\n",
      "Epoch [12/50], Step [770/777], Loss: 0.0220\n",
      "Epoch [12/50], Train Loss: 0.0134, Val Loss: 0.3205, Val Accuracy: 0.9504\n",
      "Early stopping triggered after 12 epochs\n",
      "\n",
      "Evaluating Inception-v3 on test set...\n",
      "\n",
      "Inception-v3 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DJI       0.95      0.86      0.91       200\n",
      "   FutabaT14       0.95      0.88      0.92       548\n",
      "    FutabaT7       0.99      0.92      0.96        93\n",
      "    Graupner       0.99      0.98      0.99       107\n",
      "       Noise       0.93      0.99      0.96      1314\n",
      "     Taranis       1.00      0.98      0.99       268\n",
      "     Turnigy       1.00      0.95      0.98       133\n",
      "\n",
      "    accuracy                           0.95      2663\n",
      "   macro avg       0.97      0.94      0.96      2663\n",
      "weighted avg       0.95      0.95      0.95      2663\n",
      "\n",
      "\n",
      "Inception-v3 Multi-class ROC AUC Score: 0.9888\n",
      "\n",
      "Analyzing Inception-v3 performance by SNR levels...\n",
      "\n",
      "Inception-v3 Performance by SNR level:\n",
      "SNR (dB) | Accuracy | Samples\n",
      "------------------------------\n",
      "\n",
      "Inception-v3 Total Number of Parameters: 24,362,990\n",
      "Inception-v3 Average Inference Time per Sample: 11.958 ms\n",
      "Inception-v3 FLOPs: 5,747,185,504.0 (5.75 G)\n",
      "Inception-v3 MACs: 21,799,911.0 (21.80 M)\n",
      "\n",
      "✅ Inception-v3 Accuracy for class 'DJI': 86.50%\n",
      "✅ Inception-v3 Accuracy for class 'FutabaT14': 88.14%\n",
      "✅ Inception-v3 Accuracy for class 'FutabaT7': 92.47%\n",
      "✅ Inception-v3 Accuracy for class 'Graupner': 98.13%\n",
      "✅ Inception-v3 Accuracy for class 'Noise': 98.78%\n",
      "✅ Inception-v3 Accuracy for class 'Taranis': 97.76%\n",
      "✅ Inception-v3 Accuracy for class 'Turnigy': 95.49%\n",
      "\n",
      "✅ Inception-v3 Test Set Accuracy: 0.952\n",
      "📊 Inception-v3 Model Size: 92.94 MB\n",
      "\n",
      "Key characteristics of Inception-v3:\n",
      "- Uses multiple parallel convolutional filters of different sizes\n",
      "- Efficient 'Inception modules' capture features at multiple scales\n",
      "- Employs factorized convolutions to reduce parameters\n",
      "- Auxiliary classifiers during training to improve convergence\n",
      "- Sophisticated architecture with strong performance on complex tasks\n",
      "\n",
      "Metrics saved to inception_v3_drone_rf_metrics.json\n",
      "\n",
      "\n",
      "All models have been trained and evaluated!\n",
      "Results have been saved to individual files for each model.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from torchsummary import summary\n",
    "from thop import profile  # For FLOPs and MACs calculation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "\n",
    "# Helper function for formatting large numbers\n",
    "def format_units(num):\n",
    "    \"\"\"Format large numbers with units (K, M, G, etc.)\"\"\"\n",
    "    magnitude = 0\n",
    "    while abs(num) >= 1000:\n",
    "        magnitude += 1\n",
    "        num /= 1000.0\n",
    "    return f\"{num:.2f} {['', 'K', 'M', 'G', 'T', 'P'][magnitude]}\"\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Parameters\n",
    "batch_size = 16\n",
    "resnet_mobilenet_img_size = 224  # Standard size for ResNet and MobileNet\n",
    "inception_img_size = 299  # Inception-v3 requires 299x299 input\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-4\n",
    "split_ratio = [0.7, 0.15, 0.15]  # 70% training, 15% validation, 15% test\n",
    "\n",
    "# Dataset Directory\n",
    "dataset_dir = \"/kaggle/input/drone-data/clean_spectrograms\"\n",
    "\n",
    "# Data Transformations for ResNet and MobileNetV2\n",
    "transform_resnet_mobilenet = transforms.Compose([\n",
    "    transforms.Resize((resnet_mobilenet_img_size, resnet_mobilenet_img_size)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB if needed\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Data Transformations for Inception-v3\n",
    "transform_inception = transforms.Compose([\n",
    "    transforms.Resize((inception_img_size, inception_img_size)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB if needed\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load Full Dataset for ResNet and MobileNetV2\n",
    "full_dataset_resnet_mobilenet = datasets.ImageFolder(root=dataset_dir, transform=transform_resnet_mobilenet)\n",
    "\n",
    "# Load Full Dataset for Inception-v3\n",
    "full_dataset_inception = datasets.ImageFolder(root=dataset_dir, transform=transform_inception)\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Total number of samples: {len(full_dataset_resnet_mobilenet)}\")\n",
    "class_to_idx = full_dataset_resnet_mobilenet.class_to_idx\n",
    "print(\"Class to index mapping:\", class_to_idx)\n",
    "for class_name, idx in class_to_idx.items():\n",
    "    class_samples = len([x for x, y in full_dataset_resnet_mobilenet.samples if y == idx])\n",
    "    print(f\"Class {class_name}: {class_samples} samples\")\n",
    "\n",
    "# Function to create data loaders with proper dataset splits and consistent indices\n",
    "def create_data_loaders(full_dataset):\n",
    "    # Get a generator with fixed seed for consistent splits\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    \n",
    "    # Split into Train, Validation, and Test\n",
    "    train_size = int(split_ratio[0] * len(full_dataset))\n",
    "    val_size = int(split_ratio[1] * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        full_dataset, \n",
    "        [train_size, val_size, test_size],\n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Create data loaders for ResNet and MobileNetV2\n",
    "train_loader_resnet_mobilenet, val_loader_resnet_mobilenet, test_loader_resnet_mobilenet, \\\n",
    "    train_dataset_resnet_mobilenet, val_dataset_resnet_mobilenet, test_dataset_resnet_mobilenet = \\\n",
    "    create_data_loaders(full_dataset_resnet_mobilenet)\n",
    "\n",
    "# Create data loaders for Inception-v3\n",
    "train_loader_inception, val_loader_inception, test_loader_inception, \\\n",
    "    train_dataset_inception, val_dataset_inception, test_dataset_inception = \\\n",
    "    create_data_loaders(full_dataset_inception)\n",
    "\n",
    "# Number of Classes\n",
    "num_classes = len(full_dataset_resnet_mobilenet.classes)\n",
    "class_names = full_dataset_resnet_mobilenet.classes\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Class Names: {class_names}\")\n",
    "\n",
    "# Function to extract SNR from filename\n",
    "def extract_snr(filename):\n",
    "    try:\n",
    "        # Assuming filename format like \"sample_0_snr_-14.png\"\n",
    "        parts = os.path.basename(filename).split('_')\n",
    "        snr_idx = parts.index('snr') + 1\n",
    "        return int(parts[snr_idx])\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "# Define a function to train and evaluate a model\n",
    "def train_and_evaluate_model(model_name, model, train_loader, val_loader, test_loader, test_dataset, full_dataset, img_size):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training and Evaluating {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model summary\n",
    "    print(f\"\\n{model_name} Summary:\")\n",
    "    input_shape = (3, img_size, img_size)\n",
    "    try:\n",
    "        summary(model, input_shape)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate detailed summary due to: {str(e)}\")\n",
    "        print(\"Continuing with training...\")\n",
    "    \n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "    \n",
    "    # Training Loop\n",
    "    best_val_acc = 0.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    early_stop_counter = 0\n",
    "    early_stop_patience = 5\n",
    "    best_model_weights = None\n",
    "    \n",
    "    print(f\"\\nStarting training {model_name}...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - handle Inception v3 specially\n",
    "            if model_name == \"Inception-v3\":\n",
    "                # Inception v3 returns tuple during training\n",
    "                outputs, aux_outputs = model(images)\n",
    "                loss1 = criterion(outputs, labels)\n",
    "                loss2 = criterion(aux_outputs, labels)\n",
    "                loss = loss1 + 0.4*loss2  # Auxiliary loss as per paper\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Print statistics (optional)\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # For Inception v3, in eval mode, only returns single output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Handle tuple return in case model still returns tuple during eval\n",
    "                if isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), f\"{model_name.lower().replace('-', '_')}_drone_rf.pth\")\n",
    "            print(f\"Model saved with validation accuracy: {val_accuracy:.4f}\")\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'{model_name} - Loss Over Epochs')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(f'{model_name} - Accuracy Over Epochs')\n",
    "    plt.savefig(f\"{model_name.lower().replace('-', '_')}_drone_rf_training_history.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Load the best model for evaluation\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    model.eval()\n",
    "    \n",
    "    # Testing the Model\n",
    "    print(f\"\\nEvaluating {model_name} on test set...\")\n",
    "    y_true, y_pred, y_scores = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Handle tuple return in case model still returns tuple during eval\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "                \n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_scores.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
    "    \n",
    "    # Classification Report\n",
    "    cls_report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    plt.savefig(f\"{model_name.lower().replace('-', '_')}_drone_rf_confusion_matrix.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # ROC Curve (for multi-class classification)\n",
    "    roc_auc = None\n",
    "    if num_classes > 2:\n",
    "        y_true_bin = label_binarize(y_true, classes=np.arange(num_classes))\n",
    "        y_scores_array = np.array(y_scores)\n",
    "        \n",
    "        roc_auc = roc_auc_score(y_true_bin, y_scores_array, multi_class=\"ovr\")\n",
    "        print(f\"\\n{model_name} Multi-class ROC AUC Score: {roc_auc:.4f}\")\n",
    "    \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for i in range(num_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_scores_array[:, i])\n",
    "            auc_score = roc_auc_score(y_true_bin[:, i], y_scores_array[:, i])\n",
    "            plt.plot(fpr, tpr, label=f\"Class {class_names[i]} (AUC = {auc_score:.2f})\")\n",
    "    \n",
    "        plt.plot([0, 1], [0, 1], \"k--\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"{model_name} ROC Curve\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{model_name.lower().replace('-', '_')}_drone_rf_roc_curve.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # SNR-based performance analysis\n",
    "    print(f\"\\nAnalyzing {model_name} performance by SNR levels...\")\n",
    "    \n",
    "    # Create a dictionary to store predictions by SNR\n",
    "    snr_results = {}\n",
    "    \n",
    "    # Re-run through test dataset to get filenames and predictions\n",
    "    test_dataset_files = [full_dataset.samples[i][0] for i in test_dataset.indices]\n",
    "    test_dataset_labels = [full_dataset.samples[i][1] for i in test_dataset.indices]\n",
    "    \n",
    "    # Match predictions with SNR values\n",
    "    for i, (file_path, true_label) in enumerate(zip(test_dataset_files, test_dataset_labels)):\n",
    "        snr = extract_snr(file_path)\n",
    "        if snr is not None:\n",
    "            if snr not in snr_results:\n",
    "                snr_results[snr] = {'correct': 0, 'total': 0}\n",
    "            snr_results[snr]['total'] += 1\n",
    "            if y_pred[i] == y_true[i]:\n",
    "                snr_results[snr]['correct'] += 1\n",
    "    \n",
    "    # Calculate accuracy by SNR\n",
    "    snr_accuracy = {snr: results['correct'] / results['total'] \n",
    "                    for snr, results in snr_results.items() if results['total'] > 0}\n",
    "    \n",
    "    # Plot SNR vs. Accuracy\n",
    "    sorted_snrs = sorted(snr_accuracy.keys())\n",
    "    accuracies = [snr_accuracy[snr] for snr in sorted_snrs]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sorted_snrs, accuracies, 'o-')\n",
    "    plt.xlabel('Signal-to-Noise Ratio (dB)')\n",
    "    plt.ylabel('Classification Accuracy')\n",
    "    plt.title(f'{model_name} Performance vs. Signal-to-Noise Ratio')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{model_name.lower().replace('-', '_')}_drone_rf_snr_performance.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Print SNR performance table\n",
    "    print(f\"\\n{model_name} Performance by SNR level:\")\n",
    "    print(\"SNR (dB) | Accuracy | Samples\")\n",
    "    print(\"-\" * 30)\n",
    "    snr_table = []\n",
    "    for snr in sorted_snrs:\n",
    "        acc = snr_accuracy[snr]\n",
    "        samples = snr_results[snr]['total']\n",
    "        print(f\"{snr:7d} | {acc:.4f} | {samples}\")\n",
    "        snr_table.append({\"snr\": snr, \"accuracy\": acc, \"samples\": samples})\n",
    "    \n",
    "    # Inference Time Calculation\n",
    "    sample_input = torch.randn(1, 3, img_size, img_size).to(device)\n",
    "    num_samples = 100\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            if model_name == \"Inception-v3\":\n",
    "                _ = model(sample_input)\n",
    "            else:\n",
    "                _ = model(sample_input)\n",
    "    inference_time = (time.time() - start_time) / num_samples\n",
    "    \n",
    "    # Number of Parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # FLOPs & MACs Calculation\n",
    "    try:\n",
    "        flops, macs = profile(model, inputs=(sample_input,), verbose=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating FLOPs and MACs: {str(e)}\")\n",
    "        flops, macs = 0, 0\n",
    "        \n",
    "    # Convert inference time to milliseconds\n",
    "    inference_time_ms = inference_time * 1000\n",
    "    \n",
    "    print(f\"\\n{model_name} Total Number of Parameters: {num_params:,}\")\n",
    "    print(f\"{model_name} Average Inference Time per Sample: {inference_time_ms:.3f} ms\")\n",
    "    print(f\"{model_name} FLOPs: {flops:,} ({format_units(flops)})\")\n",
    "    print(f\"{model_name} MACs: {macs:,} ({format_units(macs)})\\n\")\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "    class_acc_dict = {}\n",
    "    for i, acc in enumerate(class_accuracy):\n",
    "        print(f\"✅ {model_name} Accuracy for class '{class_names[i]}': {acc:.2%}\")\n",
    "        class_acc_dict[class_names[i]] = float(acc)\n",
    "    \n",
    "    # Calculate and display test accuracy with 3 decimal places\n",
    "    test_correct = sum([1 for i, j in zip(y_true, y_pred) if i == j])\n",
    "    test_total = len(y_true)\n",
    "    test_accuracy = test_correct / test_total\n",
    "    print(f\"\\n✅ {model_name} Test Set Accuracy: {test_accuracy:.3f}\")\n",
    "    \n",
    "    # Calculate and display model size in MB\n",
    "    # Each parameter is typically stored as a 32-bit float (4 bytes)\n",
    "    model_size_bytes = num_params * 4\n",
    "    model_size_mb = model_size_bytes / (1024 * 1024)\n",
    "    print(f\"📊 {model_name} Model Size: {model_size_mb:.2f} MB\")\n",
    "    \n",
    "    # Model characteristics based on model name\n",
    "    characteristics = []\n",
    "    if model_name == \"ResNet-50\":\n",
    "        characteristics = [\n",
    "            \"Deep residual network with skip connections to solve the vanishing gradient problem\",\n",
    "            \"50-layer architecture with bottleneck blocks\",\n",
    "            \"Well-established architecture with strong performance\",\n",
    "            \"Uses batch normalization after each convolutional layer\",\n",
    "            \"Popular backbone for many computer vision tasks\"\n",
    "        ]\n",
    "    elif model_name == \"MobileNetV2\":\n",
    "        characteristics = [\n",
    "            \"Designed specifically for mobile and edge devices\",\n",
    "            \"Uses inverted residual blocks with linear bottlenecks\",\n",
    "            \"Employs depthwise separable convolutions for efficiency\",\n",
    "            \"Excellent trade-off between accuracy and computational cost\",\n",
    "            \"Well-suited for real-time applications with limited resources\"\n",
    "        ]\n",
    "    elif model_name == \"Inception-v3\":\n",
    "        characteristics = [\n",
    "            \"Uses multiple parallel convolutional filters of different sizes\",\n",
    "            \"Efficient 'Inception modules' capture features at multiple scales\",\n",
    "            \"Employs factorized convolutions to reduce parameters\",\n",
    "            \"Auxiliary classifiers during training to improve convergence\",\n",
    "            \"Sophisticated architecture with strong performance on complex tasks\"\n",
    "        ]\n",
    "    \n",
    "    # Print characteristics\n",
    "    print(f\"\\nKey characteristics of {model_name}:\")\n",
    "    for char in characteristics:\n",
    "        print(f\"- {char}\")\n",
    "    \n",
    "    # Save all metrics to a JSON file\n",
    "    metrics = {\n",
    "        \"model_name\": model_name,\n",
    "        \"test_accuracy\": float(test_accuracy),\n",
    "        \"inference_time_ms\": float(inference_time_ms),\n",
    "        \"model_size_mb\": float(model_size_mb),\n",
    "        \"parameters\": int(num_params),\n",
    "        \"flops\": int(flops),\n",
    "        \"macs\": int(macs),\n",
    "        \"roc_auc_score\": float(roc_auc) if roc_auc is not None else None,\n",
    "        \"per_class_accuracy\": class_acc_dict,\n",
    "        \"snr_performance\": snr_table,\n",
    "        \"classification_report\": cls_report,\n",
    "        \"characteristics\": characteristics\n",
    "    }\n",
    "    \n",
    "    with open(f\"{model_name.lower().replace('-', '_')}_drone_rf_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    print(f\"\\nMetrics saved to {model_name.lower().replace('-', '_')}_drone_rf_metrics.json\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize models\n",
    "\n",
    "# 1. ResNet-50\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"TRAINING RESNET-50\")\n",
    "print(\"=\"*80)\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "train_and_evaluate_model('ResNet-50', resnet, \n",
    "                         train_loader_resnet_mobilenet, \n",
    "                         val_loader_resnet_mobilenet, \n",
    "                         test_loader_resnet_mobilenet,\n",
    "                         test_dataset_resnet_mobilenet,\n",
    "                         full_dataset_resnet_mobilenet,\n",
    "                         resnet_mobilenet_img_size)\n",
    "\n",
    "# 2. MobileNetV2\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"TRAINING MOBILENETV2\")\n",
    "print(\"=\"*80)\n",
    "mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "mobilenet.classifier[1] = nn.Linear(mobilenet.classifier[1].in_features, num_classes)\n",
    "train_and_evaluate_model('MobileNetV2', mobilenet, \n",
    "                         train_loader_resnet_mobilenet, \n",
    "                         val_loader_resnet_mobilenet, \n",
    "                         test_loader_resnet_mobilenet,\n",
    "                         test_dataset_resnet_mobilenet,\n",
    "                         full_dataset_resnet_mobilenet,\n",
    "                         resnet_mobilenet_img_size)\n",
    "\n",
    "# 3. Inception-v3\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"TRAINING INCEPTION-V3\")\n",
    "print(\"=\"*80)\n",
    "inception = models.inception_v3(pretrained=True, aux_logits=True)\n",
    "inception.AuxLogits.fc = nn.Linear(inception.AuxLogits.fc.in_features, num_classes)\n",
    "inception.fc = nn.Linear(inception.fc.in_features, num_classes)\n",
    "train_and_evaluate_model('Inception-v3', inception, \n",
    "                        train_loader_inception, \n",
    "                        val_loader_inception, \n",
    "                        test_loader_inception,\n",
    "                        test_dataset_inception,\n",
    "                        full_dataset_inception,\n",
    "                        inception_img_size)\n",
    "\n",
    "print(\"\\n\\nAll models have been trained and evaluated!\")\n",
    "print(\"Results have been saved to individual files for each model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3a500",
   "metadata": {
    "papermill": {
     "duration": 0.12361,
     "end_time": "2025-04-13T14:00:30.675084",
     "exception": false,
     "start_time": "2025-04-13T14:00:30.551474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 233589092,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9394.483109,
   "end_time": "2025-04-13T14:00:32.327977",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-13T11:23:57.844868",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
