{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "553d8395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:40:01.104287Z",
     "iopub.status.busy": "2025-04-13T11:40:01.104029Z",
     "iopub.status.idle": "2025-04-13T11:41:11.337972Z",
     "shell.execute_reply": "2025-04-13T11:41:11.337179Z"
    },
    "papermill": {
     "duration": 70.238502,
     "end_time": "2025-04-13T11:41:11.339387",
     "exception": false,
     "start_time": "2025-04-13T11:40:01.100885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thop\r\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from thop) (2.5.1+cu124)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (4.13.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->thop)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->thop)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->thop)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->thop)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->thop)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->thop)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->thop)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->thop) (3.0.2)\r\n",
      "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 thop-0.1.1.post2209072238\r\n"
     ]
    }
   ],
   "source": [
    "!pip install thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b87e27fe",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-13T11:41:11.376260Z",
     "iopub.status.busy": "2025-04-13T11:41:11.376021Z",
     "iopub.status.idle": "2025-04-13T14:00:51.166884Z",
     "shell.execute_reply": "2025-04-13T14:00:51.166010Z"
    },
    "papermill": {
     "duration": 8379.810675,
     "end_time": "2025-04-13T14:00:51.168193",
     "exception": false,
     "start_time": "2025-04-13T11:41:11.357518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_1-b8a52dc0.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 17744\n",
      "Class to index mapping: {'DJI': 0, 'FutabaT14': 1, 'FutabaT7': 2, 'Graupner': 3, 'Noise': 4, 'Taranis': 5, 'Turnigy': 6}\n",
      "Class DJI: 1280 samples\n",
      "Class FutabaT14: 3472 samples\n",
      "Class FutabaT7: 801 samples\n",
      "Class Graupner: 801 samples\n",
      "Class Noise: 8872 samples\n",
      "Class Taranis: 1663 samples\n",
      "Class Turnigy: 855 samples\n",
      "Number of classes: 7\n",
      "Class Names: ['DJI', 'FutabaT14', 'FutabaT7', 'Graupner', 'Noise', 'Taranis', 'Turnigy']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING SQUEEZENET\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.73M/4.73M [00:00<00:00, 100MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training and Evaluating SqueezeNet\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SqueezeNet Summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 111, 111]           1,792\n",
      "              ReLU-2         [-1, 64, 111, 111]               0\n",
      "         MaxPool2d-3           [-1, 64, 55, 55]               0\n",
      "            Conv2d-4           [-1, 16, 55, 55]           1,040\n",
      "              ReLU-5           [-1, 16, 55, 55]               0\n",
      "            Conv2d-6           [-1, 64, 55, 55]           1,088\n",
      "              ReLU-7           [-1, 64, 55, 55]               0\n",
      "            Conv2d-8           [-1, 64, 55, 55]           9,280\n",
      "              ReLU-9           [-1, 64, 55, 55]               0\n",
      "             Fire-10          [-1, 128, 55, 55]               0\n",
      "           Conv2d-11           [-1, 16, 55, 55]           2,064\n",
      "             ReLU-12           [-1, 16, 55, 55]               0\n",
      "           Conv2d-13           [-1, 64, 55, 55]           1,088\n",
      "             ReLU-14           [-1, 64, 55, 55]               0\n",
      "           Conv2d-15           [-1, 64, 55, 55]           9,280\n",
      "             ReLU-16           [-1, 64, 55, 55]               0\n",
      "             Fire-17          [-1, 128, 55, 55]               0\n",
      "        MaxPool2d-18          [-1, 128, 27, 27]               0\n",
      "           Conv2d-19           [-1, 32, 27, 27]           4,128\n",
      "             ReLU-20           [-1, 32, 27, 27]               0\n",
      "           Conv2d-21          [-1, 128, 27, 27]           4,224\n",
      "             ReLU-22          [-1, 128, 27, 27]               0\n",
      "           Conv2d-23          [-1, 128, 27, 27]          36,992\n",
      "             ReLU-24          [-1, 128, 27, 27]               0\n",
      "             Fire-25          [-1, 256, 27, 27]               0\n",
      "           Conv2d-26           [-1, 32, 27, 27]           8,224\n",
      "             ReLU-27           [-1, 32, 27, 27]               0\n",
      "           Conv2d-28          [-1, 128, 27, 27]           4,224\n",
      "             ReLU-29          [-1, 128, 27, 27]               0\n",
      "           Conv2d-30          [-1, 128, 27, 27]          36,992\n",
      "             ReLU-31          [-1, 128, 27, 27]               0\n",
      "             Fire-32          [-1, 256, 27, 27]               0\n",
      "        MaxPool2d-33          [-1, 256, 13, 13]               0\n",
      "           Conv2d-34           [-1, 48, 13, 13]          12,336\n",
      "             ReLU-35           [-1, 48, 13, 13]               0\n",
      "           Conv2d-36          [-1, 192, 13, 13]           9,408\n",
      "             ReLU-37          [-1, 192, 13, 13]               0\n",
      "           Conv2d-38          [-1, 192, 13, 13]          83,136\n",
      "             ReLU-39          [-1, 192, 13, 13]               0\n",
      "             Fire-40          [-1, 384, 13, 13]               0\n",
      "           Conv2d-41           [-1, 48, 13, 13]          18,480\n",
      "             ReLU-42           [-1, 48, 13, 13]               0\n",
      "           Conv2d-43          [-1, 192, 13, 13]           9,408\n",
      "             ReLU-44          [-1, 192, 13, 13]               0\n",
      "           Conv2d-45          [-1, 192, 13, 13]          83,136\n",
      "             ReLU-46          [-1, 192, 13, 13]               0\n",
      "             Fire-47          [-1, 384, 13, 13]               0\n",
      "           Conv2d-48           [-1, 64, 13, 13]          24,640\n",
      "             ReLU-49           [-1, 64, 13, 13]               0\n",
      "           Conv2d-50          [-1, 256, 13, 13]          16,640\n",
      "             ReLU-51          [-1, 256, 13, 13]               0\n",
      "           Conv2d-52          [-1, 256, 13, 13]         147,712\n",
      "             ReLU-53          [-1, 256, 13, 13]               0\n",
      "             Fire-54          [-1, 512, 13, 13]               0\n",
      "           Conv2d-55           [-1, 64, 13, 13]          32,832\n",
      "             ReLU-56           [-1, 64, 13, 13]               0\n",
      "           Conv2d-57          [-1, 256, 13, 13]          16,640\n",
      "             ReLU-58          [-1, 256, 13, 13]               0\n",
      "           Conv2d-59          [-1, 256, 13, 13]         147,712\n",
      "             ReLU-60          [-1, 256, 13, 13]               0\n",
      "             Fire-61          [-1, 512, 13, 13]               0\n",
      "          Dropout-62          [-1, 512, 13, 13]               0\n",
      "           Conv2d-63            [-1, 7, 13, 13]           3,591\n",
      "             ReLU-64            [-1, 7, 13, 13]               0\n",
      "AdaptiveAvgPool2d-65              [-1, 7, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 726,087\n",
      "Trainable params: 726,087\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 51.20\n",
      "Params size (MB): 2.77\n",
      "Estimated Total Size (MB): 54.54\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Starting training SqueezeNet...\n",
      "Epoch [1/50], Step [10/777], Loss: 1.4520\n",
      "Epoch [1/50], Step [20/777], Loss: 1.7505\n",
      "Epoch [1/50], Step [30/777], Loss: 1.2670\n",
      "Epoch [1/50], Step [40/777], Loss: 1.5484\n",
      "Epoch [1/50], Step [50/777], Loss: 1.1532\n",
      "Epoch [1/50], Step [60/777], Loss: 1.7337\n",
      "Epoch [1/50], Step [70/777], Loss: 1.7182\n",
      "Epoch [1/50], Step [80/777], Loss: 1.0626\n",
      "Epoch [1/50], Step [90/777], Loss: 1.7261\n",
      "Epoch [1/50], Step [100/777], Loss: 1.3773\n",
      "Epoch [1/50], Step [110/777], Loss: 1.2873\n",
      "Epoch [1/50], Step [120/777], Loss: 1.4554\n",
      "Epoch [1/50], Step [130/777], Loss: 0.8443\n",
      "Epoch [1/50], Step [140/777], Loss: 1.1579\n",
      "Epoch [1/50], Step [150/777], Loss: 1.1634\n",
      "Epoch [1/50], Step [160/777], Loss: 1.0625\n",
      "Epoch [1/50], Step [170/777], Loss: 1.2890\n",
      "Epoch [1/50], Step [180/777], Loss: 0.6929\n",
      "Epoch [1/50], Step [190/777], Loss: 1.2648\n",
      "Epoch [1/50], Step [200/777], Loss: 1.1347\n",
      "Epoch [1/50], Step [210/777], Loss: 1.1022\n",
      "Epoch [1/50], Step [220/777], Loss: 0.7225\n",
      "Epoch [1/50], Step [230/777], Loss: 1.0418\n",
      "Epoch [1/50], Step [240/777], Loss: 0.9948\n",
      "Epoch [1/50], Step [250/777], Loss: 1.2799\n",
      "Epoch [1/50], Step [260/777], Loss: 0.9423\n",
      "Epoch [1/50], Step [270/777], Loss: 0.6705\n",
      "Epoch [1/50], Step [280/777], Loss: 0.8616\n",
      "Epoch [1/50], Step [290/777], Loss: 0.4060\n",
      "Epoch [1/50], Step [300/777], Loss: 0.5472\n",
      "Epoch [1/50], Step [310/777], Loss: 0.6257\n",
      "Epoch [1/50], Step [320/777], Loss: 0.4488\n",
      "Epoch [1/50], Step [330/777], Loss: 1.2204\n",
      "Epoch [1/50], Step [340/777], Loss: 0.8295\n",
      "Epoch [1/50], Step [350/777], Loss: 0.8337\n",
      "Epoch [1/50], Step [360/777], Loss: 0.3445\n",
      "Epoch [1/50], Step [370/777], Loss: 0.5745\n",
      "Epoch [1/50], Step [380/777], Loss: 0.6305\n",
      "Epoch [1/50], Step [390/777], Loss: 0.6535\n",
      "Epoch [1/50], Step [400/777], Loss: 0.8308\n",
      "Epoch [1/50], Step [410/777], Loss: 0.3705\n",
      "Epoch [1/50], Step [420/777], Loss: 0.5115\n",
      "Epoch [1/50], Step [430/777], Loss: 0.6157\n",
      "Epoch [1/50], Step [440/777], Loss: 0.1300\n",
      "Epoch [1/50], Step [450/777], Loss: 0.8573\n",
      "Epoch [1/50], Step [460/777], Loss: 0.7853\n",
      "Epoch [1/50], Step [470/777], Loss: 0.2767\n",
      "Epoch [1/50], Step [480/777], Loss: 0.7228\n",
      "Epoch [1/50], Step [490/777], Loss: 0.3209\n",
      "Epoch [1/50], Step [500/777], Loss: 1.1339\n",
      "Epoch [1/50], Step [510/777], Loss: 0.3843\n",
      "Epoch [1/50], Step [520/777], Loss: 0.5412\n",
      "Epoch [1/50], Step [530/777], Loss: 1.1400\n",
      "Epoch [1/50], Step [540/777], Loss: 0.2181\n",
      "Epoch [1/50], Step [550/777], Loss: 0.3366\n",
      "Epoch [1/50], Step [560/777], Loss: 0.6598\n",
      "Epoch [1/50], Step [570/777], Loss: 0.1723\n",
      "Epoch [1/50], Step [580/777], Loss: 0.1062\n",
      "Epoch [1/50], Step [590/777], Loss: 0.1866\n",
      "Epoch [1/50], Step [600/777], Loss: 0.8267\n",
      "Epoch [1/50], Step [610/777], Loss: 0.4516\n",
      "Epoch [1/50], Step [620/777], Loss: 0.3780\n",
      "Epoch [1/50], Step [630/777], Loss: 0.0841\n",
      "Epoch [1/50], Step [640/777], Loss: 0.5809\n",
      "Epoch [1/50], Step [650/777], Loss: 0.7941\n",
      "Epoch [1/50], Step [660/777], Loss: 0.3826\n",
      "Epoch [1/50], Step [670/777], Loss: 0.2265\n",
      "Epoch [1/50], Step [680/777], Loss: 0.5334\n",
      "Epoch [1/50], Step [690/777], Loss: 0.5771\n",
      "Epoch [1/50], Step [700/777], Loss: 1.0210\n",
      "Epoch [1/50], Step [710/777], Loss: 0.6065\n",
      "Epoch [1/50], Step [720/777], Loss: 0.2385\n",
      "Epoch [1/50], Step [730/777], Loss: 0.2559\n",
      "Epoch [1/50], Step [740/777], Loss: 0.8836\n",
      "Epoch [1/50], Step [750/777], Loss: 0.4802\n",
      "Epoch [1/50], Step [760/777], Loss: 0.5385\n",
      "Epoch [1/50], Step [770/777], Loss: 0.4661\n",
      "Epoch [1/50], Train Loss: 0.7574, Val Loss: 0.4282, Val Accuracy: 0.8715\n",
      "Model saved with validation accuracy: 0.8715\n",
      "Epoch [2/50], Step [10/777], Loss: 0.2712\n",
      "Epoch [2/50], Step [20/777], Loss: 0.4678\n",
      "Epoch [2/50], Step [30/777], Loss: 0.4212\n",
      "Epoch [2/50], Step [40/777], Loss: 1.0154\n",
      "Epoch [2/50], Step [50/777], Loss: 0.4808\n",
      "Epoch [2/50], Step [60/777], Loss: 0.3271\n",
      "Epoch [2/50], Step [70/777], Loss: 0.6579\n",
      "Epoch [2/50], Step [80/777], Loss: 0.8791\n",
      "Epoch [2/50], Step [90/777], Loss: 0.6975\n",
      "Epoch [2/50], Step [100/777], Loss: 0.6561\n",
      "Epoch [2/50], Step [110/777], Loss: 0.2802\n",
      "Epoch [2/50], Step [120/777], Loss: 0.0895\n",
      "Epoch [2/50], Step [130/777], Loss: 0.7323\n",
      "Epoch [2/50], Step [140/777], Loss: 0.3958\n",
      "Epoch [2/50], Step [150/777], Loss: 0.3391\n",
      "Epoch [2/50], Step [160/777], Loss: 0.6973\n",
      "Epoch [2/50], Step [170/777], Loss: 0.2456\n",
      "Epoch [2/50], Step [180/777], Loss: 0.2508\n",
      "Epoch [2/50], Step [190/777], Loss: 0.4821\n",
      "Epoch [2/50], Step [200/777], Loss: 0.1666\n",
      "Epoch [2/50], Step [210/777], Loss: 0.3442\n",
      "Epoch [2/50], Step [220/777], Loss: 0.3293\n",
      "Epoch [2/50], Step [230/777], Loss: 0.1935\n",
      "Epoch [2/50], Step [240/777], Loss: 0.2477\n",
      "Epoch [2/50], Step [250/777], Loss: 0.4519\n",
      "Epoch [2/50], Step [260/777], Loss: 0.3534\n",
      "Epoch [2/50], Step [270/777], Loss: 0.1900\n",
      "Epoch [2/50], Step [280/777], Loss: 0.4654\n",
      "Epoch [2/50], Step [290/777], Loss: 1.0730\n",
      "Epoch [2/50], Step [300/777], Loss: 0.7805\n",
      "Epoch [2/50], Step [310/777], Loss: 0.2720\n",
      "Epoch [2/50], Step [320/777], Loss: 0.5834\n",
      "Epoch [2/50], Step [330/777], Loss: 0.4276\n",
      "Epoch [2/50], Step [340/777], Loss: 0.2170\n",
      "Epoch [2/50], Step [350/777], Loss: 0.3785\n",
      "Epoch [2/50], Step [360/777], Loss: 0.3916\n",
      "Epoch [2/50], Step [370/777], Loss: 0.2688\n",
      "Epoch [2/50], Step [380/777], Loss: 0.4720\n",
      "Epoch [2/50], Step [390/777], Loss: 0.2872\n",
      "Epoch [2/50], Step [400/777], Loss: 0.3890\n",
      "Epoch [2/50], Step [410/777], Loss: 0.1752\n",
      "Epoch [2/50], Step [420/777], Loss: 0.8254\n",
      "Epoch [2/50], Step [430/777], Loss: 0.5087\n",
      "Epoch [2/50], Step [440/777], Loss: 0.8667\n",
      "Epoch [2/50], Step [450/777], Loss: 0.0537\n",
      "Epoch [2/50], Step [460/777], Loss: 0.4671\n",
      "Epoch [2/50], Step [470/777], Loss: 0.3242\n",
      "Epoch [2/50], Step [480/777], Loss: 0.7588\n",
      "Epoch [2/50], Step [490/777], Loss: 0.4215\n",
      "Epoch [2/50], Step [500/777], Loss: 0.3051\n",
      "Epoch [2/50], Step [510/777], Loss: 0.5416\n",
      "Epoch [2/50], Step [520/777], Loss: 0.4050\n",
      "Epoch [2/50], Step [530/777], Loss: 0.4981\n",
      "Epoch [2/50], Step [540/777], Loss: 0.4399\n",
      "Epoch [2/50], Step [550/777], Loss: 0.2694\n",
      "Epoch [2/50], Step [560/777], Loss: 0.3066\n",
      "Epoch [2/50], Step [570/777], Loss: 0.4711\n",
      "Epoch [2/50], Step [580/777], Loss: 0.1164\n",
      "Epoch [2/50], Step [590/777], Loss: 0.6794\n",
      "Epoch [2/50], Step [600/777], Loss: 0.6902\n",
      "Epoch [2/50], Step [610/777], Loss: 0.3150\n",
      "Epoch [2/50], Step [620/777], Loss: 0.5777\n",
      "Epoch [2/50], Step [630/777], Loss: 0.5015\n",
      "Epoch [2/50], Step [640/777], Loss: 0.5998\n",
      "Epoch [2/50], Step [650/777], Loss: 0.1661\n",
      "Epoch [2/50], Step [660/777], Loss: 0.2379\n",
      "Epoch [2/50], Step [670/777], Loss: 0.8170\n",
      "Epoch [2/50], Step [680/777], Loss: 0.7131\n",
      "Epoch [2/50], Step [690/777], Loss: 0.2786\n",
      "Epoch [2/50], Step [700/777], Loss: 0.2713\n",
      "Epoch [2/50], Step [710/777], Loss: 0.5162\n",
      "Epoch [2/50], Step [720/777], Loss: 0.9322\n",
      "Epoch [2/50], Step [730/777], Loss: 0.6783\n",
      "Epoch [2/50], Step [740/777], Loss: 0.2637\n",
      "Epoch [2/50], Step [750/777], Loss: 0.1053\n",
      "Epoch [2/50], Step [760/777], Loss: 2.0231\n",
      "Epoch [2/50], Step [770/777], Loss: 0.4236\n",
      "Epoch [2/50], Train Loss: 0.4232, Val Loss: 0.3965, Val Accuracy: 0.8846\n",
      "Model saved with validation accuracy: 0.8846\n",
      "Epoch [3/50], Step [10/777], Loss: 0.3623\n",
      "Epoch [3/50], Step [20/777], Loss: 0.2855\n",
      "Epoch [3/50], Step [30/777], Loss: 0.3953\n",
      "Epoch [3/50], Step [40/777], Loss: 0.5109\n",
      "Epoch [3/50], Step [50/777], Loss: 0.1672\n",
      "Epoch [3/50], Step [60/777], Loss: 0.3470\n",
      "Epoch [3/50], Step [70/777], Loss: 0.1716\n",
      "Epoch [3/50], Step [80/777], Loss: 0.4423\n",
      "Epoch [3/50], Step [90/777], Loss: 0.2358\n",
      "Epoch [3/50], Step [100/777], Loss: 0.2639\n",
      "Epoch [3/50], Step [110/777], Loss: 0.0918\n",
      "Epoch [3/50], Step [120/777], Loss: 0.0934\n",
      "Epoch [3/50], Step [130/777], Loss: 0.2920\n",
      "Epoch [3/50], Step [140/777], Loss: 0.6996\n",
      "Epoch [3/50], Step [150/777], Loss: 0.7630\n",
      "Epoch [3/50], Step [160/777], Loss: 0.9462\n",
      "Epoch [3/50], Step [170/777], Loss: 0.7728\n",
      "Epoch [3/50], Step [180/777], Loss: 0.9011\n",
      "Epoch [3/50], Step [190/777], Loss: 0.5126\n",
      "Epoch [3/50], Step [200/777], Loss: 0.2116\n",
      "Epoch [3/50], Step [210/777], Loss: 0.3366\n",
      "Epoch [3/50], Step [220/777], Loss: 0.1742\n",
      "Epoch [3/50], Step [230/777], Loss: 0.4581\n",
      "Epoch [3/50], Step [240/777], Loss: 0.1366\n",
      "Epoch [3/50], Step [250/777], Loss: 0.5732\n",
      "Epoch [3/50], Step [260/777], Loss: 0.1284\n",
      "Epoch [3/50], Step [270/777], Loss: 0.1534\n",
      "Epoch [3/50], Step [280/777], Loss: 0.3079\n",
      "Epoch [3/50], Step [290/777], Loss: 0.3127\n",
      "Epoch [3/50], Step [300/777], Loss: 0.1596\n",
      "Epoch [3/50], Step [310/777], Loss: 0.2222\n",
      "Epoch [3/50], Step [320/777], Loss: 0.3419\n",
      "Epoch [3/50], Step [330/777], Loss: 0.4356\n",
      "Epoch [3/50], Step [340/777], Loss: 0.2406\n",
      "Epoch [3/50], Step [350/777], Loss: 0.6080\n",
      "Epoch [3/50], Step [360/777], Loss: 0.2667\n",
      "Epoch [3/50], Step [370/777], Loss: 0.5324\n",
      "Epoch [3/50], Step [380/777], Loss: 0.1970\n",
      "Epoch [3/50], Step [390/777], Loss: 0.4552\n",
      "Epoch [3/50], Step [400/777], Loss: 0.1103\n",
      "Epoch [3/50], Step [410/777], Loss: 0.5407\n",
      "Epoch [3/50], Step [420/777], Loss: 0.1231\n",
      "Epoch [3/50], Step [430/777], Loss: 0.0749\n",
      "Epoch [3/50], Step [440/777], Loss: 0.2578\n",
      "Epoch [3/50], Step [450/777], Loss: 0.3886\n",
      "Epoch [3/50], Step [460/777], Loss: 0.1224\n",
      "Epoch [3/50], Step [470/777], Loss: 0.0915\n",
      "Epoch [3/50], Step [480/777], Loss: 0.4554\n",
      "Epoch [3/50], Step [490/777], Loss: 0.4485\n",
      "Epoch [3/50], Step [500/777], Loss: 0.3113\n",
      "Epoch [3/50], Step [510/777], Loss: 0.8081\n",
      "Epoch [3/50], Step [520/777], Loss: 0.2828\n",
      "Epoch [3/50], Step [530/777], Loss: 0.2311\n",
      "Epoch [3/50], Step [540/777], Loss: 0.6015\n",
      "Epoch [3/50], Step [550/777], Loss: 0.2536\n",
      "Epoch [3/50], Step [560/777], Loss: 1.2096\n",
      "Epoch [3/50], Step [570/777], Loss: 0.1357\n",
      "Epoch [3/50], Step [580/777], Loss: 0.1380\n",
      "Epoch [3/50], Step [590/777], Loss: 0.5246\n",
      "Epoch [3/50], Step [600/777], Loss: 0.3154\n",
      "Epoch [3/50], Step [610/777], Loss: 0.0755\n",
      "Epoch [3/50], Step [620/777], Loss: 0.5956\n",
      "Epoch [3/50], Step [630/777], Loss: 0.1288\n",
      "Epoch [3/50], Step [640/777], Loss: 0.0816\n",
      "Epoch [3/50], Step [650/777], Loss: 0.2850\n",
      "Epoch [3/50], Step [660/777], Loss: 0.7626\n",
      "Epoch [3/50], Step [670/777], Loss: 0.2645\n",
      "Epoch [3/50], Step [680/777], Loss: 0.3327\n",
      "Epoch [3/50], Step [690/777], Loss: 0.6263\n",
      "Epoch [3/50], Step [700/777], Loss: 0.6730\n",
      "Epoch [3/50], Step [710/777], Loss: 0.5626\n",
      "Epoch [3/50], Step [720/777], Loss: 0.2779\n",
      "Epoch [3/50], Step [730/777], Loss: 0.3841\n",
      "Epoch [3/50], Step [740/777], Loss: 0.3876\n",
      "Epoch [3/50], Step [750/777], Loss: 0.1428\n",
      "Epoch [3/50], Step [760/777], Loss: 0.0638\n",
      "Epoch [3/50], Step [770/777], Loss: 0.0897\n",
      "Epoch [3/50], Train Loss: 0.3670, Val Loss: 0.3114, Val Accuracy: 0.9177\n",
      "Model saved with validation accuracy: 0.9177\n",
      "Epoch [4/50], Step [10/777], Loss: 0.4161\n",
      "Epoch [4/50], Step [20/777], Loss: 0.4203\n",
      "Epoch [4/50], Step [30/777], Loss: 0.5131\n",
      "Epoch [4/50], Step [40/777], Loss: 0.1020\n",
      "Epoch [4/50], Step [50/777], Loss: 0.5074\n",
      "Epoch [4/50], Step [60/777], Loss: 0.1485\n",
      "Epoch [4/50], Step [70/777], Loss: 0.0620\n",
      "Epoch [4/50], Step [80/777], Loss: 0.0825\n",
      "Epoch [4/50], Step [90/777], Loss: 0.4549\n",
      "Epoch [4/50], Step [100/777], Loss: 0.5972\n",
      "Epoch [4/50], Step [110/777], Loss: 0.2497\n",
      "Epoch [4/50], Step [120/777], Loss: 0.1804\n",
      "Epoch [4/50], Step [130/777], Loss: 0.0702\n",
      "Epoch [4/50], Step [140/777], Loss: 0.5753\n",
      "Epoch [4/50], Step [150/777], Loss: 0.5318\n",
      "Epoch [4/50], Step [160/777], Loss: 0.7309\n",
      "Epoch [4/50], Step [170/777], Loss: 0.3986\n",
      "Epoch [4/50], Step [180/777], Loss: 0.4626\n",
      "Epoch [4/50], Step [190/777], Loss: 0.7721\n",
      "Epoch [4/50], Step [200/777], Loss: 0.2159\n",
      "Epoch [4/50], Step [210/777], Loss: 0.0687\n",
      "Epoch [4/50], Step [220/777], Loss: 0.0758\n",
      "Epoch [4/50], Step [230/777], Loss: 0.4731\n",
      "Epoch [4/50], Step [240/777], Loss: 0.0397\n",
      "Epoch [4/50], Step [250/777], Loss: 0.8158\n",
      "Epoch [4/50], Step [260/777], Loss: 0.2055\n",
      "Epoch [4/50], Step [270/777], Loss: 0.2718\n",
      "Epoch [4/50], Step [280/777], Loss: 0.6882\n",
      "Epoch [4/50], Step [290/777], Loss: 0.0342\n",
      "Epoch [4/50], Step [300/777], Loss: 0.2777\n",
      "Epoch [4/50], Step [310/777], Loss: 0.4192\n",
      "Epoch [4/50], Step [320/777], Loss: 0.4456\n",
      "Epoch [4/50], Step [330/777], Loss: 0.5920\n",
      "Epoch [4/50], Step [340/777], Loss: 0.5146\n",
      "Epoch [4/50], Step [350/777], Loss: 0.2152\n",
      "Epoch [4/50], Step [360/777], Loss: 0.1397\n",
      "Epoch [4/50], Step [370/777], Loss: 0.3959\n",
      "Epoch [4/50], Step [380/777], Loss: 0.3664\n",
      "Epoch [4/50], Step [390/777], Loss: 0.1302\n",
      "Epoch [4/50], Step [400/777], Loss: 0.6194\n",
      "Epoch [4/50], Step [410/777], Loss: 0.0317\n",
      "Epoch [4/50], Step [420/777], Loss: 0.5790\n",
      "Epoch [4/50], Step [430/777], Loss: 0.3567\n",
      "Epoch [4/50], Step [440/777], Loss: 0.4538\n",
      "Epoch [4/50], Step [450/777], Loss: 0.2388\n",
      "Epoch [4/50], Step [460/777], Loss: 0.0655\n",
      "Epoch [4/50], Step [470/777], Loss: 0.4098\n",
      "Epoch [4/50], Step [480/777], Loss: 0.2139\n",
      "Epoch [4/50], Step [490/777], Loss: 0.0628\n",
      "Epoch [4/50], Step [500/777], Loss: 0.1718\n",
      "Epoch [4/50], Step [510/777], Loss: 0.1348\n",
      "Epoch [4/50], Step [520/777], Loss: 0.3518\n",
      "Epoch [4/50], Step [530/777], Loss: 0.1200\n",
      "Epoch [4/50], Step [540/777], Loss: 0.2251\n",
      "Epoch [4/50], Step [550/777], Loss: 0.5855\n",
      "Epoch [4/50], Step [560/777], Loss: 0.4138\n",
      "Epoch [4/50], Step [570/777], Loss: 0.1200\n",
      "Epoch [4/50], Step [580/777], Loss: 0.3661\n",
      "Epoch [4/50], Step [590/777], Loss: 0.2868\n",
      "Epoch [4/50], Step [600/777], Loss: 0.2448\n",
      "Epoch [4/50], Step [610/777], Loss: 0.3125\n",
      "Epoch [4/50], Step [620/777], Loss: 0.2124\n",
      "Epoch [4/50], Step [630/777], Loss: 0.3744\n",
      "Epoch [4/50], Step [640/777], Loss: 0.1038\n",
      "Epoch [4/50], Step [650/777], Loss: 0.3053\n",
      "Epoch [4/50], Step [660/777], Loss: 0.0326\n",
      "Epoch [4/50], Step [670/777], Loss: 0.2025\n",
      "Epoch [4/50], Step [680/777], Loss: 0.5523\n",
      "Epoch [4/50], Step [690/777], Loss: 0.1149\n",
      "Epoch [4/50], Step [700/777], Loss: 0.2325\n",
      "Epoch [4/50], Step [710/777], Loss: 0.2378\n",
      "Epoch [4/50], Step [720/777], Loss: 0.2167\n",
      "Epoch [4/50], Step [730/777], Loss: 0.0479\n",
      "Epoch [4/50], Step [740/777], Loss: 0.3578\n",
      "Epoch [4/50], Step [750/777], Loss: 0.1774\n",
      "Epoch [4/50], Step [760/777], Loss: 0.2795\n",
      "Epoch [4/50], Step [770/777], Loss: 0.3103\n",
      "Epoch [4/50], Train Loss: 0.3243, Val Loss: 0.2633, Val Accuracy: 0.9263\n",
      "Model saved with validation accuracy: 0.9263\n",
      "Epoch [5/50], Step [10/777], Loss: 0.1699\n",
      "Epoch [5/50], Step [20/777], Loss: 0.2311\n",
      "Epoch [5/50], Step [30/777], Loss: 0.4736\n",
      "Epoch [5/50], Step [40/777], Loss: 0.2961\n",
      "Epoch [5/50], Step [50/777], Loss: 0.1217\n",
      "Epoch [5/50], Step [60/777], Loss: 0.7061\n",
      "Epoch [5/50], Step [70/777], Loss: 0.5944\n",
      "Epoch [5/50], Step [80/777], Loss: 0.2398\n",
      "Epoch [5/50], Step [90/777], Loss: 0.2264\n",
      "Epoch [5/50], Step [100/777], Loss: 0.0662\n",
      "Epoch [5/50], Step [110/777], Loss: 0.3387\n",
      "Epoch [5/50], Step [120/777], Loss: 0.0917\n",
      "Epoch [5/50], Step [130/777], Loss: 0.4965\n",
      "Epoch [5/50], Step [140/777], Loss: 0.1178\n",
      "Epoch [5/50], Step [150/777], Loss: 0.3468\n",
      "Epoch [5/50], Step [160/777], Loss: 0.2577\n",
      "Epoch [5/50], Step [170/777], Loss: 0.0241\n",
      "Epoch [5/50], Step [180/777], Loss: 0.0699\n",
      "Epoch [5/50], Step [190/777], Loss: 0.0845\n",
      "Epoch [5/50], Step [200/777], Loss: 0.1187\n",
      "Epoch [5/50], Step [210/777], Loss: 0.3076\n",
      "Epoch [5/50], Step [220/777], Loss: 0.4082\n",
      "Epoch [5/50], Step [230/777], Loss: 0.4950\n",
      "Epoch [5/50], Step [240/777], Loss: 0.2220\n",
      "Epoch [5/50], Step [250/777], Loss: 0.0261\n",
      "Epoch [5/50], Step [260/777], Loss: 0.0421\n",
      "Epoch [5/50], Step [270/777], Loss: 0.3441\n",
      "Epoch [5/50], Step [280/777], Loss: 0.4657\n",
      "Epoch [5/50], Step [290/777], Loss: 0.1045\n",
      "Epoch [5/50], Step [300/777], Loss: 0.5144\n",
      "Epoch [5/50], Step [310/777], Loss: 0.0957\n",
      "Epoch [5/50], Step [320/777], Loss: 0.1963\n",
      "Epoch [5/50], Step [330/777], Loss: 0.2996\n",
      "Epoch [5/50], Step [340/777], Loss: 0.3212\n",
      "Epoch [5/50], Step [350/777], Loss: 0.0273\n",
      "Epoch [5/50], Step [360/777], Loss: 0.1044\n",
      "Epoch [5/50], Step [370/777], Loss: 0.7994\n",
      "Epoch [5/50], Step [380/777], Loss: 0.2802\n",
      "Epoch [5/50], Step [390/777], Loss: 0.0703\n",
      "Epoch [5/50], Step [400/777], Loss: 0.2349\n",
      "Epoch [5/50], Step [410/777], Loss: 0.2374\n",
      "Epoch [5/50], Step [420/777], Loss: 0.0340\n",
      "Epoch [5/50], Step [430/777], Loss: 0.1280\n",
      "Epoch [5/50], Step [440/777], Loss: 0.0149\n",
      "Epoch [5/50], Step [450/777], Loss: 0.5829\n",
      "Epoch [5/50], Step [460/777], Loss: 0.2631\n",
      "Epoch [5/50], Step [470/777], Loss: 0.1004\n",
      "Epoch [5/50], Step [480/777], Loss: 0.0610\n",
      "Epoch [5/50], Step [490/777], Loss: 0.5369\n",
      "Epoch [5/50], Step [500/777], Loss: 0.0715\n",
      "Epoch [5/50], Step [510/777], Loss: 0.6686\n",
      "Epoch [5/50], Step [520/777], Loss: 0.5018\n",
      "Epoch [5/50], Step [530/777], Loss: 0.2814\n",
      "Epoch [5/50], Step [540/777], Loss: 0.0699\n",
      "Epoch [5/50], Step [550/777], Loss: 0.2676\n",
      "Epoch [5/50], Step [560/777], Loss: 0.5976\n",
      "Epoch [5/50], Step [570/777], Loss: 0.5505\n",
      "Epoch [5/50], Step [580/777], Loss: 0.0560\n",
      "Epoch [5/50], Step [590/777], Loss: 0.0729\n",
      "Epoch [5/50], Step [600/777], Loss: 0.4493\n",
      "Epoch [5/50], Step [610/777], Loss: 0.2680\n",
      "Epoch [5/50], Step [620/777], Loss: 0.0429\n",
      "Epoch [5/50], Step [630/777], Loss: 0.5108\n",
      "Epoch [5/50], Step [640/777], Loss: 0.0514\n",
      "Epoch [5/50], Step [650/777], Loss: 0.0947\n",
      "Epoch [5/50], Step [660/777], Loss: 0.3318\n",
      "Epoch [5/50], Step [670/777], Loss: 0.2298\n",
      "Epoch [5/50], Step [680/777], Loss: 0.0301\n",
      "Epoch [5/50], Step [690/777], Loss: 0.4693\n",
      "Epoch [5/50], Step [700/777], Loss: 0.4353\n",
      "Epoch [5/50], Step [710/777], Loss: 0.0855\n",
      "Epoch [5/50], Step [720/777], Loss: 0.2643\n",
      "Epoch [5/50], Step [730/777], Loss: 0.2440\n",
      "Epoch [5/50], Step [740/777], Loss: 0.4708\n",
      "Epoch [5/50], Step [750/777], Loss: 0.2870\n",
      "Epoch [5/50], Step [760/777], Loss: 0.2320\n",
      "Epoch [5/50], Step [770/777], Loss: 0.1371\n",
      "Epoch [5/50], Train Loss: 0.2902, Val Loss: 0.2636, Val Accuracy: 0.9252\n",
      "Epoch [6/50], Step [10/777], Loss: 0.5886\n",
      "Epoch [6/50], Step [20/777], Loss: 0.3549\n",
      "Epoch [6/50], Step [30/777], Loss: 0.0400\n",
      "Epoch [6/50], Step [40/777], Loss: 0.1229\n",
      "Epoch [6/50], Step [50/777], Loss: 0.2582\n",
      "Epoch [6/50], Step [60/777], Loss: 0.0936\n",
      "Epoch [6/50], Step [70/777], Loss: 0.3449\n",
      "Epoch [6/50], Step [80/777], Loss: 0.1162\n",
      "Epoch [6/50], Step [90/777], Loss: 0.0328\n",
      "Epoch [6/50], Step [100/777], Loss: 0.0386\n",
      "Epoch [6/50], Step [110/777], Loss: 0.0062\n",
      "Epoch [6/50], Step [120/777], Loss: 0.2148\n",
      "Epoch [6/50], Step [130/777], Loss: 0.1460\n",
      "Epoch [6/50], Step [140/777], Loss: 0.5322\n",
      "Epoch [6/50], Step [150/777], Loss: 0.3343\n",
      "Epoch [6/50], Step [160/777], Loss: 0.1989\n",
      "Epoch [6/50], Step [170/777], Loss: 0.1969\n",
      "Epoch [6/50], Step [180/777], Loss: 0.0451\n",
      "Epoch [6/50], Step [190/777], Loss: 0.8003\n",
      "Epoch [6/50], Step [200/777], Loss: 0.3236\n",
      "Epoch [6/50], Step [210/777], Loss: 0.4400\n",
      "Epoch [6/50], Step [220/777], Loss: 0.2017\n",
      "Epoch [6/50], Step [230/777], Loss: 0.4240\n",
      "Epoch [6/50], Step [240/777], Loss: 0.8384\n",
      "Epoch [6/50], Step [250/777], Loss: 0.1880\n",
      "Epoch [6/50], Step [260/777], Loss: 0.1906\n",
      "Epoch [6/50], Step [270/777], Loss: 0.3854\n",
      "Epoch [6/50], Step [280/777], Loss: 0.2065\n",
      "Epoch [6/50], Step [290/777], Loss: 0.6183\n",
      "Epoch [6/50], Step [300/777], Loss: 0.1525\n",
      "Epoch [6/50], Step [310/777], Loss: 0.2665\n",
      "Epoch [6/50], Step [320/777], Loss: 0.0660\n",
      "Epoch [6/50], Step [330/777], Loss: 0.2784\n",
      "Epoch [6/50], Step [340/777], Loss: 0.4200\n",
      "Epoch [6/50], Step [350/777], Loss: 0.3672\n",
      "Epoch [6/50], Step [360/777], Loss: 0.3536\n",
      "Epoch [6/50], Step [370/777], Loss: 0.9054\n",
      "Epoch [6/50], Step [380/777], Loss: 0.2215\n",
      "Epoch [6/50], Step [390/777], Loss: 0.2355\n",
      "Epoch [6/50], Step [400/777], Loss: 0.4128\n",
      "Epoch [6/50], Step [410/777], Loss: 0.1915\n",
      "Epoch [6/50], Step [420/777], Loss: 0.2241\n",
      "Epoch [6/50], Step [430/777], Loss: 0.6047\n",
      "Epoch [6/50], Step [440/777], Loss: 0.3850\n",
      "Epoch [6/50], Step [450/777], Loss: 0.0927\n",
      "Epoch [6/50], Step [460/777], Loss: 0.0883\n",
      "Epoch [6/50], Step [470/777], Loss: 0.2314\n",
      "Epoch [6/50], Step [480/777], Loss: 0.1190\n",
      "Epoch [6/50], Step [490/777], Loss: 0.2418\n",
      "Epoch [6/50], Step [500/777], Loss: 0.2067\n",
      "Epoch [6/50], Step [510/777], Loss: 0.3007\n",
      "Epoch [6/50], Step [520/777], Loss: 0.3936\n",
      "Epoch [6/50], Step [530/777], Loss: 0.0647\n",
      "Epoch [6/50], Step [540/777], Loss: 0.2504\n",
      "Epoch [6/50], Step [550/777], Loss: 0.1399\n",
      "Epoch [6/50], Step [560/777], Loss: 0.0583\n",
      "Epoch [6/50], Step [570/777], Loss: 0.2406\n",
      "Epoch [6/50], Step [580/777], Loss: 0.6013\n",
      "Epoch [6/50], Step [590/777], Loss: 0.3448\n",
      "Epoch [6/50], Step [600/777], Loss: 0.0397\n",
      "Epoch [6/50], Step [610/777], Loss: 0.0757\n",
      "Epoch [6/50], Step [620/777], Loss: 0.2963\n",
      "Epoch [6/50], Step [630/777], Loss: 0.2958\n",
      "Epoch [6/50], Step [640/777], Loss: 0.2447\n",
      "Epoch [6/50], Step [650/777], Loss: 0.3502\n",
      "Epoch [6/50], Step [660/777], Loss: 0.2564\n",
      "Epoch [6/50], Step [670/777], Loss: 0.4408\n",
      "Epoch [6/50], Step [680/777], Loss: 0.0918\n",
      "Epoch [6/50], Step [690/777], Loss: 0.5147\n",
      "Epoch [6/50], Step [700/777], Loss: 0.2054\n",
      "Epoch [6/50], Step [710/777], Loss: 0.1399\n",
      "Epoch [6/50], Step [720/777], Loss: 0.0695\n",
      "Epoch [6/50], Step [730/777], Loss: 0.1349\n",
      "Epoch [6/50], Step [740/777], Loss: 0.1698\n",
      "Epoch [6/50], Step [750/777], Loss: 0.2430\n",
      "Epoch [6/50], Step [760/777], Loss: 0.2508\n",
      "Epoch [6/50], Step [770/777], Loss: 0.1210\n",
      "Epoch [6/50], Train Loss: 0.2823, Val Loss: 0.3453, Val Accuracy: 0.9166\n",
      "Epoch [7/50], Step [10/777], Loss: 0.3653\n",
      "Epoch [7/50], Step [20/777], Loss: 0.4420\n",
      "Epoch [7/50], Step [30/777], Loss: 0.2816\n",
      "Epoch [7/50], Step [40/777], Loss: 0.0604\n",
      "Epoch [7/50], Step [50/777], Loss: 0.2980\n",
      "Epoch [7/50], Step [60/777], Loss: 0.3129\n",
      "Epoch [7/50], Step [70/777], Loss: 0.0794\n",
      "Epoch [7/50], Step [80/777], Loss: 0.2655\n",
      "Epoch [7/50], Step [90/777], Loss: 0.2669\n",
      "Epoch [7/50], Step [100/777], Loss: 0.2929\n",
      "Epoch [7/50], Step [110/777], Loss: 0.0482\n",
      "Epoch [7/50], Step [120/777], Loss: 0.4774\n",
      "Epoch [7/50], Step [130/777], Loss: 0.5707\n",
      "Epoch [7/50], Step [140/777], Loss: 0.2087\n",
      "Epoch [7/50], Step [150/777], Loss: 0.1161\n",
      "Epoch [7/50], Step [160/777], Loss: 0.2369\n",
      "Epoch [7/50], Step [170/777], Loss: 0.0239\n",
      "Epoch [7/50], Step [180/777], Loss: 0.0864\n",
      "Epoch [7/50], Step [190/777], Loss: 0.0659\n",
      "Epoch [7/50], Step [200/777], Loss: 0.1694\n",
      "Epoch [7/50], Step [210/777], Loss: 0.1140\n",
      "Epoch [7/50], Step [220/777], Loss: 0.2465\n",
      "Epoch [7/50], Step [230/777], Loss: 0.3027\n",
      "Epoch [7/50], Step [240/777], Loss: 0.2500\n",
      "Epoch [7/50], Step [250/777], Loss: 0.0388\n",
      "Epoch [7/50], Step [260/777], Loss: 0.1475\n",
      "Epoch [7/50], Step [270/777], Loss: 0.2477\n",
      "Epoch [7/50], Step [280/777], Loss: 0.1254\n",
      "Epoch [7/50], Step [290/777], Loss: 0.1083\n",
      "Epoch [7/50], Step [300/777], Loss: 0.2908\n",
      "Epoch [7/50], Step [310/777], Loss: 0.8987\n",
      "Epoch [7/50], Step [320/777], Loss: 0.2261\n",
      "Epoch [7/50], Step [330/777], Loss: 0.5698\n",
      "Epoch [7/50], Step [340/777], Loss: 0.1309\n",
      "Epoch [7/50], Step [350/777], Loss: 0.5560\n",
      "Epoch [7/50], Step [360/777], Loss: 0.1209\n",
      "Epoch [7/50], Step [370/777], Loss: 0.8404\n",
      "Epoch [7/50], Step [380/777], Loss: 0.0880\n",
      "Epoch [7/50], Step [390/777], Loss: 0.4278\n",
      "Epoch [7/50], Step [400/777], Loss: 0.0872\n",
      "Epoch [7/50], Step [410/777], Loss: 0.5586\n",
      "Epoch [7/50], Step [420/777], Loss: 0.3526\n",
      "Epoch [7/50], Step [430/777], Loss: 0.3027\n",
      "Epoch [7/50], Step [440/777], Loss: 0.0606\n",
      "Epoch [7/50], Step [450/777], Loss: 0.0387\n",
      "Epoch [7/50], Step [460/777], Loss: 0.0325\n",
      "Epoch [7/50], Step [470/777], Loss: 0.3743\n",
      "Epoch [7/50], Step [480/777], Loss: 0.4156\n",
      "Epoch [7/50], Step [490/777], Loss: 0.2795\n",
      "Epoch [7/50], Step [500/777], Loss: 0.3389\n",
      "Epoch [7/50], Step [510/777], Loss: 0.5144\n",
      "Epoch [7/50], Step [520/777], Loss: 0.2684\n",
      "Epoch [7/50], Step [530/777], Loss: 0.2985\n",
      "Epoch [7/50], Step [540/777], Loss: 0.2425\n",
      "Epoch [7/50], Step [550/777], Loss: 0.0553\n",
      "Epoch [7/50], Step [560/777], Loss: 0.1171\n",
      "Epoch [7/50], Step [570/777], Loss: 0.4954\n",
      "Epoch [7/50], Step [580/777], Loss: 0.4779\n",
      "Epoch [7/50], Step [590/777], Loss: 0.4623\n",
      "Epoch [7/50], Step [600/777], Loss: 0.1739\n",
      "Epoch [7/50], Step [610/777], Loss: 0.3143\n",
      "Epoch [7/50], Step [620/777], Loss: 0.0406\n",
      "Epoch [7/50], Step [630/777], Loss: 0.4369\n",
      "Epoch [7/50], Step [640/777], Loss: 0.1883\n",
      "Epoch [7/50], Step [650/777], Loss: 0.1810\n",
      "Epoch [7/50], Step [660/777], Loss: 0.0411\n",
      "Epoch [7/50], Step [670/777], Loss: 0.4205\n",
      "Epoch [7/50], Step [680/777], Loss: 0.0593\n",
      "Epoch [7/50], Step [690/777], Loss: 0.0559\n",
      "Epoch [7/50], Step [700/777], Loss: 0.3397\n",
      "Epoch [7/50], Step [710/777], Loss: 0.0997\n",
      "Epoch [7/50], Step [720/777], Loss: 0.0901\n",
      "Epoch [7/50], Step [730/777], Loss: 0.0089\n",
      "Epoch [7/50], Step [740/777], Loss: 0.6528\n",
      "Epoch [7/50], Step [750/777], Loss: 0.1063\n",
      "Epoch [7/50], Step [760/777], Loss: 0.1870\n",
      "Epoch [7/50], Step [770/777], Loss: 0.8859\n",
      "Epoch [7/50], Train Loss: 0.2563, Val Loss: 0.2290, Val Accuracy: 0.9399\n",
      "Model saved with validation accuracy: 0.9399\n",
      "Epoch [8/50], Step [10/777], Loss: 0.0429\n",
      "Epoch [8/50], Step [20/777], Loss: 0.2216\n",
      "Epoch [8/50], Step [30/777], Loss: 0.2404\n",
      "Epoch [8/50], Step [40/777], Loss: 0.6318\n",
      "Epoch [8/50], Step [50/777], Loss: 0.1331\n",
      "Epoch [8/50], Step [60/777], Loss: 0.2637\n",
      "Epoch [8/50], Step [70/777], Loss: 0.1327\n",
      "Epoch [8/50], Step [80/777], Loss: 0.2105\n",
      "Epoch [8/50], Step [90/777], Loss: 0.0611\n",
      "Epoch [8/50], Step [100/777], Loss: 0.0416\n",
      "Epoch [8/50], Step [110/777], Loss: 0.1209\n",
      "Epoch [8/50], Step [120/777], Loss: 0.3625\n",
      "Epoch [8/50], Step [130/777], Loss: 0.1946\n",
      "Epoch [8/50], Step [140/777], Loss: 0.2184\n",
      "Epoch [8/50], Step [150/777], Loss: 0.1406\n",
      "Epoch [8/50], Step [160/777], Loss: 0.2353\n",
      "Epoch [8/50], Step [170/777], Loss: 0.2905\n",
      "Epoch [8/50], Step [180/777], Loss: 0.2419\n",
      "Epoch [8/50], Step [190/777], Loss: 0.0751\n",
      "Epoch [8/50], Step [200/777], Loss: 0.3187\n",
      "Epoch [8/50], Step [210/777], Loss: 0.2941\n",
      "Epoch [8/50], Step [220/777], Loss: 0.3423\n",
      "Epoch [8/50], Step [230/777], Loss: 0.3035\n",
      "Epoch [8/50], Step [240/777], Loss: 0.3083\n",
      "Epoch [8/50], Step [250/777], Loss: 1.0908\n",
      "Epoch [8/50], Step [260/777], Loss: 0.2185\n",
      "Epoch [8/50], Step [270/777], Loss: 0.2876\n",
      "Epoch [8/50], Step [280/777], Loss: 0.2366\n",
      "Epoch [8/50], Step [290/777], Loss: 0.0363\n",
      "Epoch [8/50], Step [300/777], Loss: 0.2215\n",
      "Epoch [8/50], Step [310/777], Loss: 0.0770\n",
      "Epoch [8/50], Step [320/777], Loss: 0.2405\n",
      "Epoch [8/50], Step [330/777], Loss: 0.1080\n",
      "Epoch [8/50], Step [340/777], Loss: 0.2199\n",
      "Epoch [8/50], Step [350/777], Loss: 0.2104\n",
      "Epoch [8/50], Step [360/777], Loss: 0.4805\n",
      "Epoch [8/50], Step [370/777], Loss: 0.4682\n",
      "Epoch [8/50], Step [380/777], Loss: 0.0388\n",
      "Epoch [8/50], Step [390/777], Loss: 0.6145\n",
      "Epoch [8/50], Step [400/777], Loss: 0.1923\n",
      "Epoch [8/50], Step [410/777], Loss: 0.3668\n",
      "Epoch [8/50], Step [420/777], Loss: 0.0829\n",
      "Epoch [8/50], Step [430/777], Loss: 0.2351\n",
      "Epoch [8/50], Step [440/777], Loss: 0.6715\n",
      "Epoch [8/50], Step [450/777], Loss: 0.2524\n",
      "Epoch [8/50], Step [460/777], Loss: 0.3071\n",
      "Epoch [8/50], Step [470/777], Loss: 0.0237\n",
      "Epoch [8/50], Step [480/777], Loss: 0.1918\n",
      "Epoch [8/50], Step [490/777], Loss: 0.2070\n",
      "Epoch [8/50], Step [500/777], Loss: 0.0798\n",
      "Epoch [8/50], Step [510/777], Loss: 0.3591\n",
      "Epoch [8/50], Step [520/777], Loss: 0.2654\n",
      "Epoch [8/50], Step [530/777], Loss: 0.0542\n",
      "Epoch [8/50], Step [540/777], Loss: 0.3235\n",
      "Epoch [8/50], Step [550/777], Loss: 0.7252\n",
      "Epoch [8/50], Step [560/777], Loss: 0.0653\n",
      "Epoch [8/50], Step [570/777], Loss: 0.0751\n",
      "Epoch [8/50], Step [580/777], Loss: 0.2149\n",
      "Epoch [8/50], Step [590/777], Loss: 0.2448\n",
      "Epoch [8/50], Step [600/777], Loss: 0.0601\n",
      "Epoch [8/50], Step [610/777], Loss: 0.1911\n",
      "Epoch [8/50], Step [620/777], Loss: 0.1239\n",
      "Epoch [8/50], Step [630/777], Loss: 0.2127\n",
      "Epoch [8/50], Step [640/777], Loss: 0.2560\n",
      "Epoch [8/50], Step [650/777], Loss: 0.1624\n",
      "Epoch [8/50], Step [660/777], Loss: 0.3504\n",
      "Epoch [8/50], Step [670/777], Loss: 0.0879\n",
      "Epoch [8/50], Step [680/777], Loss: 0.7259\n",
      "Epoch [8/50], Step [690/777], Loss: 0.3280\n",
      "Epoch [8/50], Step [700/777], Loss: 0.0450\n",
      "Epoch [8/50], Step [710/777], Loss: 0.5643\n",
      "Epoch [8/50], Step [720/777], Loss: 0.2105\n",
      "Epoch [8/50], Step [730/777], Loss: 0.0458\n",
      "Epoch [8/50], Step [740/777], Loss: 0.2613\n",
      "Epoch [8/50], Step [750/777], Loss: 0.2997\n",
      "Epoch [8/50], Step [760/777], Loss: 0.0349\n",
      "Epoch [8/50], Step [770/777], Loss: 0.0854\n",
      "Epoch [8/50], Train Loss: 0.2448, Val Loss: 0.2980, Val Accuracy: 0.9049\n",
      "Epoch [9/50], Step [10/777], Loss: 0.0678\n",
      "Epoch [9/50], Step [20/777], Loss: 0.0630\n",
      "Epoch [9/50], Step [30/777], Loss: 0.0538\n",
      "Epoch [9/50], Step [40/777], Loss: 0.1499\n",
      "Epoch [9/50], Step [50/777], Loss: 0.1355\n",
      "Epoch [9/50], Step [60/777], Loss: 0.0503\n",
      "Epoch [9/50], Step [70/777], Loss: 0.4615\n",
      "Epoch [9/50], Step [80/777], Loss: 0.0786\n",
      "Epoch [9/50], Step [90/777], Loss: 0.0945\n",
      "Epoch [9/50], Step [100/777], Loss: 0.2649\n",
      "Epoch [9/50], Step [110/777], Loss: 0.0489\n",
      "Epoch [9/50], Step [120/777], Loss: 0.6460\n",
      "Epoch [9/50], Step [130/777], Loss: 0.2010\n",
      "Epoch [9/50], Step [140/777], Loss: 0.0798\n",
      "Epoch [9/50], Step [150/777], Loss: 0.2002\n",
      "Epoch [9/50], Step [160/777], Loss: 0.3904\n",
      "Epoch [9/50], Step [170/777], Loss: 0.0791\n",
      "Epoch [9/50], Step [180/777], Loss: 0.4822\n",
      "Epoch [9/50], Step [190/777], Loss: 0.0245\n",
      "Epoch [9/50], Step [200/777], Loss: 0.0414\n",
      "Epoch [9/50], Step [210/777], Loss: 0.3018\n",
      "Epoch [9/50], Step [220/777], Loss: 0.0266\n",
      "Epoch [9/50], Step [230/777], Loss: 0.0291\n",
      "Epoch [9/50], Step [240/777], Loss: 0.0247\n",
      "Epoch [9/50], Step [250/777], Loss: 0.6628\n",
      "Epoch [9/50], Step [260/777], Loss: 0.5462\n",
      "Epoch [9/50], Step [270/777], Loss: 0.0281\n",
      "Epoch [9/50], Step [280/777], Loss: 0.1876\n",
      "Epoch [9/50], Step [290/777], Loss: 0.2547\n",
      "Epoch [9/50], Step [300/777], Loss: 0.1353\n",
      "Epoch [9/50], Step [310/777], Loss: 0.4332\n",
      "Epoch [9/50], Step [320/777], Loss: 0.0134\n",
      "Epoch [9/50], Step [330/777], Loss: 0.0285\n",
      "Epoch [9/50], Step [340/777], Loss: 0.4795\n",
      "Epoch [9/50], Step [350/777], Loss: 0.1411\n",
      "Epoch [9/50], Step [360/777], Loss: 0.0362\n",
      "Epoch [9/50], Step [370/777], Loss: 0.0221\n",
      "Epoch [9/50], Step [380/777], Loss: 0.1356\n",
      "Epoch [9/50], Step [390/777], Loss: 0.1425\n",
      "Epoch [9/50], Step [400/777], Loss: 0.6237\n",
      "Epoch [9/50], Step [410/777], Loss: 1.2209\n",
      "Epoch [9/50], Step [420/777], Loss: 0.1484\n",
      "Epoch [9/50], Step [430/777], Loss: 0.1980\n",
      "Epoch [9/50], Step [440/777], Loss: 0.5113\n",
      "Epoch [9/50], Step [450/777], Loss: 0.1159\n",
      "Epoch [9/50], Step [460/777], Loss: 0.2930\n",
      "Epoch [9/50], Step [470/777], Loss: 0.0452\n",
      "Epoch [9/50], Step [480/777], Loss: 0.9812\n",
      "Epoch [9/50], Step [490/777], Loss: 0.2367\n",
      "Epoch [9/50], Step [500/777], Loss: 0.4767\n",
      "Epoch [9/50], Step [510/777], Loss: 0.0437\n",
      "Epoch [9/50], Step [520/777], Loss: 0.0685\n",
      "Epoch [9/50], Step [530/777], Loss: 0.1220\n",
      "Epoch [9/50], Step [540/777], Loss: 0.0363\n",
      "Epoch [9/50], Step [550/777], Loss: 0.1648\n",
      "Epoch [9/50], Step [560/777], Loss: 0.2252\n",
      "Epoch [9/50], Step [570/777], Loss: 0.2617\n",
      "Epoch [9/50], Step [580/777], Loss: 0.4251\n",
      "Epoch [9/50], Step [590/777], Loss: 0.2641\n",
      "Epoch [9/50], Step [600/777], Loss: 0.2297\n",
      "Epoch [9/50], Step [610/777], Loss: 0.3768\n",
      "Epoch [9/50], Step [620/777], Loss: 0.3566\n",
      "Epoch [9/50], Step [630/777], Loss: 0.4943\n",
      "Epoch [9/50], Step [640/777], Loss: 0.3154\n",
      "Epoch [9/50], Step [650/777], Loss: 0.2955\n",
      "Epoch [9/50], Step [660/777], Loss: 0.0494\n",
      "Epoch [9/50], Step [670/777], Loss: 0.1130\n",
      "Epoch [9/50], Step [680/777], Loss: 0.6322\n",
      "Epoch [9/50], Step [690/777], Loss: 0.0283\n",
      "Epoch [9/50], Step [700/777], Loss: 0.1665\n",
      "Epoch [9/50], Step [710/777], Loss: 0.0340\n",
      "Epoch [9/50], Step [720/777], Loss: 0.0883\n",
      "Epoch [9/50], Step [730/777], Loss: 0.2068\n",
      "Epoch [9/50], Step [740/777], Loss: 0.0815\n",
      "Epoch [9/50], Step [750/777], Loss: 0.3390\n",
      "Epoch [9/50], Step [760/777], Loss: 0.0226\n",
      "Epoch [9/50], Step [770/777], Loss: 0.0437\n",
      "Epoch [9/50], Train Loss: 0.2368, Val Loss: 0.2490, Val Accuracy: 0.9384\n",
      "Epoch [10/50], Step [10/777], Loss: 0.2160\n",
      "Epoch [10/50], Step [20/777], Loss: 0.4924\n",
      "Epoch [10/50], Step [30/777], Loss: 0.0626\n",
      "Epoch [10/50], Step [40/777], Loss: 0.4214\n",
      "Epoch [10/50], Step [50/777], Loss: 0.1210\n",
      "Epoch [10/50], Step [60/777], Loss: 0.1829\n",
      "Epoch [10/50], Step [70/777], Loss: 0.0822\n",
      "Epoch [10/50], Step [80/777], Loss: 0.0173\n",
      "Epoch [10/50], Step [90/777], Loss: 0.0159\n",
      "Epoch [10/50], Step [100/777], Loss: 0.3747\n",
      "Epoch [10/50], Step [110/777], Loss: 0.1966\n",
      "Epoch [10/50], Step [120/777], Loss: 0.7475\n",
      "Epoch [10/50], Step [130/777], Loss: 0.0601\n",
      "Epoch [10/50], Step [140/777], Loss: 0.7636\n",
      "Epoch [10/50], Step [150/777], Loss: 0.0886\n",
      "Epoch [10/50], Step [160/777], Loss: 0.4669\n",
      "Epoch [10/50], Step [170/777], Loss: 0.4137\n",
      "Epoch [10/50], Step [180/777], Loss: 0.0322\n",
      "Epoch [10/50], Step [190/777], Loss: 0.3658\n",
      "Epoch [10/50], Step [200/777], Loss: 0.2998\n",
      "Epoch [10/50], Step [210/777], Loss: 0.3248\n",
      "Epoch [10/50], Step [220/777], Loss: 0.1886\n",
      "Epoch [10/50], Step [230/777], Loss: 0.6899\n",
      "Epoch [10/50], Step [240/777], Loss: 0.0243\n",
      "Epoch [10/50], Step [250/777], Loss: 0.0473\n",
      "Epoch [10/50], Step [260/777], Loss: 0.1253\n",
      "Epoch [10/50], Step [270/777], Loss: 0.0704\n",
      "Epoch [10/50], Step [280/777], Loss: 0.5281\n",
      "Epoch [10/50], Step [290/777], Loss: 0.4641\n",
      "Epoch [10/50], Step [300/777], Loss: 0.4552\n",
      "Epoch [10/50], Step [310/777], Loss: 0.4984\n",
      "Epoch [10/50], Step [320/777], Loss: 0.9315\n",
      "Epoch [10/50], Step [330/777], Loss: 0.8198\n",
      "Epoch [10/50], Step [340/777], Loss: 0.2090\n",
      "Epoch [10/50], Step [350/777], Loss: 0.4206\n",
      "Epoch [10/50], Step [360/777], Loss: 0.3497\n",
      "Epoch [10/50], Step [370/777], Loss: 0.7718\n",
      "Epoch [10/50], Step [380/777], Loss: 0.3654\n",
      "Epoch [10/50], Step [390/777], Loss: 0.4044\n",
      "Epoch [10/50], Step [400/777], Loss: 0.1799\n",
      "Epoch [10/50], Step [410/777], Loss: 0.1825\n",
      "Epoch [10/50], Step [420/777], Loss: 0.3552\n",
      "Epoch [10/50], Step [430/777], Loss: 0.4364\n",
      "Epoch [10/50], Step [440/777], Loss: 0.0582\n",
      "Epoch [10/50], Step [450/777], Loss: 0.0298\n",
      "Epoch [10/50], Step [460/777], Loss: 0.2792\n",
      "Epoch [10/50], Step [470/777], Loss: 0.0366\n",
      "Epoch [10/50], Step [480/777], Loss: 0.1626\n",
      "Epoch [10/50], Step [490/777], Loss: 0.6788\n",
      "Epoch [10/50], Step [500/777], Loss: 0.0852\n",
      "Epoch [10/50], Step [510/777], Loss: 0.0209\n",
      "Epoch [10/50], Step [520/777], Loss: 0.2405\n",
      "Epoch [10/50], Step [530/777], Loss: 0.1116\n",
      "Epoch [10/50], Step [540/777], Loss: 0.3686\n",
      "Epoch [10/50], Step [550/777], Loss: 0.0310\n",
      "Epoch [10/50], Step [560/777], Loss: 0.3208\n",
      "Epoch [10/50], Step [570/777], Loss: 0.1867\n",
      "Epoch [10/50], Step [580/777], Loss: 0.5434\n",
      "Epoch [10/50], Step [590/777], Loss: 0.0961\n",
      "Epoch [10/50], Step [600/777], Loss: 0.2977\n",
      "Epoch [10/50], Step [610/777], Loss: 0.2844\n",
      "Epoch [10/50], Step [620/777], Loss: 0.1968\n",
      "Epoch [10/50], Step [630/777], Loss: 0.1956\n",
      "Epoch [10/50], Step [640/777], Loss: 0.0217\n",
      "Epoch [10/50], Step [650/777], Loss: 0.0180\n",
      "Epoch [10/50], Step [660/777], Loss: 0.2450\n",
      "Epoch [10/50], Step [670/777], Loss: 0.2534\n",
      "Epoch [10/50], Step [680/777], Loss: 0.0726\n",
      "Epoch [10/50], Step [690/777], Loss: 0.7769\n",
      "Epoch [10/50], Step [700/777], Loss: 0.0778\n",
      "Epoch [10/50], Step [710/777], Loss: 0.1931\n",
      "Epoch [10/50], Step [720/777], Loss: 0.5070\n",
      "Epoch [10/50], Step [730/777], Loss: 0.3546\n",
      "Epoch [10/50], Step [740/777], Loss: 0.0402\n",
      "Epoch [10/50], Step [750/777], Loss: 0.0460\n",
      "Epoch [10/50], Step [760/777], Loss: 0.0483\n",
      "Epoch [10/50], Step [770/777], Loss: 0.0069\n",
      "Epoch [10/50], Train Loss: 0.2216, Val Loss: 0.2522, Val Accuracy: 0.9369\n",
      "Epoch [11/50], Step [10/777], Loss: 0.3333\n",
      "Epoch [11/50], Step [20/777], Loss: 0.0312\n",
      "Epoch [11/50], Step [30/777], Loss: 0.0616\n",
      "Epoch [11/50], Step [40/777], Loss: 0.0579\n",
      "Epoch [11/50], Step [50/777], Loss: 0.1025\n",
      "Epoch [11/50], Step [60/777], Loss: 0.1161\n",
      "Epoch [11/50], Step [70/777], Loss: 0.0395\n",
      "Epoch [11/50], Step [80/777], Loss: 0.0303\n",
      "Epoch [11/50], Step [90/777], Loss: 0.4931\n",
      "Epoch [11/50], Step [100/777], Loss: 0.1121\n",
      "Epoch [11/50], Step [110/777], Loss: 0.1070\n",
      "Epoch [11/50], Step [120/777], Loss: 0.2235\n",
      "Epoch [11/50], Step [130/777], Loss: 0.0625\n",
      "Epoch [11/50], Step [140/777], Loss: 0.2378\n",
      "Epoch [11/50], Step [150/777], Loss: 0.3180\n",
      "Epoch [11/50], Step [160/777], Loss: 0.1856\n",
      "Epoch [11/50], Step [170/777], Loss: 0.2638\n",
      "Epoch [11/50], Step [180/777], Loss: 0.1810\n",
      "Epoch [11/50], Step [190/777], Loss: 0.0112\n",
      "Epoch [11/50], Step [200/777], Loss: 0.0435\n",
      "Epoch [11/50], Step [210/777], Loss: 0.3916\n",
      "Epoch [11/50], Step [220/777], Loss: 0.0403\n",
      "Epoch [11/50], Step [230/777], Loss: 0.6093\n",
      "Epoch [11/50], Step [240/777], Loss: 0.4652\n",
      "Epoch [11/50], Step [250/777], Loss: 0.0387\n",
      "Epoch [11/50], Step [260/777], Loss: 0.0668\n",
      "Epoch [11/50], Step [270/777], Loss: 0.0380\n",
      "Epoch [11/50], Step [280/777], Loss: 0.1945\n",
      "Epoch [11/50], Step [290/777], Loss: 0.3239\n",
      "Epoch [11/50], Step [300/777], Loss: 0.0592\n",
      "Epoch [11/50], Step [310/777], Loss: 0.3290\n",
      "Epoch [11/50], Step [320/777], Loss: 0.2176\n",
      "Epoch [11/50], Step [330/777], Loss: 0.0268\n",
      "Epoch [11/50], Step [340/777], Loss: 0.1938\n",
      "Epoch [11/50], Step [350/777], Loss: 0.0315\n",
      "Epoch [11/50], Step [360/777], Loss: 0.0786\n",
      "Epoch [11/50], Step [370/777], Loss: 0.1516\n",
      "Epoch [11/50], Step [380/777], Loss: 0.2059\n",
      "Epoch [11/50], Step [390/777], Loss: 0.2444\n",
      "Epoch [11/50], Step [400/777], Loss: 0.0474\n",
      "Epoch [11/50], Step [410/777], Loss: 0.3636\n",
      "Epoch [11/50], Step [420/777], Loss: 0.1407\n",
      "Epoch [11/50], Step [430/777], Loss: 0.2228\n",
      "Epoch [11/50], Step [440/777], Loss: 0.1634\n",
      "Epoch [11/50], Step [450/777], Loss: 0.0841\n",
      "Epoch [11/50], Step [460/777], Loss: 0.0891\n",
      "Epoch [11/50], Step [470/777], Loss: 0.4517\n",
      "Epoch [11/50], Step [480/777], Loss: 0.4666\n",
      "Epoch [11/50], Step [490/777], Loss: 0.1085\n",
      "Epoch [11/50], Step [500/777], Loss: 0.0357\n",
      "Epoch [11/50], Step [510/777], Loss: 0.2353\n",
      "Epoch [11/50], Step [520/777], Loss: 0.3735\n",
      "Epoch [11/50], Step [530/777], Loss: 0.0673\n",
      "Epoch [11/50], Step [540/777], Loss: 0.5299\n",
      "Epoch [11/50], Step [550/777], Loss: 0.0898\n",
      "Epoch [11/50], Step [560/777], Loss: 0.2148\n",
      "Epoch [11/50], Step [570/777], Loss: 0.4634\n",
      "Epoch [11/50], Step [580/777], Loss: 0.0406\n",
      "Epoch [11/50], Step [590/777], Loss: 0.3667\n",
      "Epoch [11/50], Step [600/777], Loss: 0.2192\n",
      "Epoch [11/50], Step [610/777], Loss: 0.1245\n",
      "Epoch [11/50], Step [620/777], Loss: 0.2543\n",
      "Epoch [11/50], Step [630/777], Loss: 0.0732\n",
      "Epoch [11/50], Step [640/777], Loss: 0.0751\n",
      "Epoch [11/50], Step [650/777], Loss: 0.0556\n",
      "Epoch [11/50], Step [660/777], Loss: 0.0898\n",
      "Epoch [11/50], Step [670/777], Loss: 0.0412\n",
      "Epoch [11/50], Step [680/777], Loss: 0.1401\n",
      "Epoch [11/50], Step [690/777], Loss: 0.3152\n",
      "Epoch [11/50], Step [700/777], Loss: 0.0325\n",
      "Epoch [11/50], Step [710/777], Loss: 0.0850\n",
      "Epoch [11/50], Step [720/777], Loss: 0.3374\n",
      "Epoch [11/50], Step [730/777], Loss: 0.2314\n",
      "Epoch [11/50], Step [740/777], Loss: 0.2826\n",
      "Epoch [11/50], Step [750/777], Loss: 0.0308\n",
      "Epoch [11/50], Step [760/777], Loss: 0.0399\n",
      "Epoch [11/50], Step [770/777], Loss: 0.3098\n",
      "Epoch [11/50], Train Loss: 0.2184, Val Loss: 0.2250, Val Accuracy: 0.9414\n",
      "Model saved with validation accuracy: 0.9414\n",
      "Epoch [12/50], Step [10/777], Loss: 0.2704\n",
      "Epoch [12/50], Step [20/777], Loss: 0.0480\n",
      "Epoch [12/50], Step [30/777], Loss: 0.2320\n",
      "Epoch [12/50], Step [40/777], Loss: 0.2519\n",
      "Epoch [12/50], Step [50/777], Loss: 0.0586\n",
      "Epoch [12/50], Step [60/777], Loss: 0.0550\n",
      "Epoch [12/50], Step [70/777], Loss: 0.5903\n",
      "Epoch [12/50], Step [80/777], Loss: 0.0440\n",
      "Epoch [12/50], Step [90/777], Loss: 0.0244\n",
      "Epoch [12/50], Step [100/777], Loss: 0.1257\n",
      "Epoch [12/50], Step [110/777], Loss: 0.2545\n",
      "Epoch [12/50], Step [120/777], Loss: 0.2077\n",
      "Epoch [12/50], Step [130/777], Loss: 0.4196\n",
      "Epoch [12/50], Step [140/777], Loss: 0.0789\n",
      "Epoch [12/50], Step [150/777], Loss: 0.0709\n",
      "Epoch [12/50], Step [160/777], Loss: 0.1545\n",
      "Epoch [12/50], Step [170/777], Loss: 0.0215\n",
      "Epoch [12/50], Step [180/777], Loss: 0.2380\n",
      "Epoch [12/50], Step [190/777], Loss: 0.2045\n",
      "Epoch [12/50], Step [200/777], Loss: 0.0414\n",
      "Epoch [12/50], Step [210/777], Loss: 0.5092\n",
      "Epoch [12/50], Step [220/777], Loss: 0.3142\n",
      "Epoch [12/50], Step [230/777], Loss: 0.2926\n",
      "Epoch [12/50], Step [240/777], Loss: 0.0972\n",
      "Epoch [12/50], Step [250/777], Loss: 0.1171\n",
      "Epoch [12/50], Step [260/777], Loss: 0.1337\n",
      "Epoch [12/50], Step [270/777], Loss: 0.5549\n",
      "Epoch [12/50], Step [280/777], Loss: 0.6332\n",
      "Epoch [12/50], Step [290/777], Loss: 0.0315\n",
      "Epoch [12/50], Step [300/777], Loss: 0.0555\n",
      "Epoch [12/50], Step [310/777], Loss: 0.0149\n",
      "Epoch [12/50], Step [320/777], Loss: 0.3015\n",
      "Epoch [12/50], Step [330/777], Loss: 0.0170\n",
      "Epoch [12/50], Step [340/777], Loss: 0.0243\n",
      "Epoch [12/50], Step [350/777], Loss: 0.0324\n",
      "Epoch [12/50], Step [360/777], Loss: 0.4754\n",
      "Epoch [12/50], Step [370/777], Loss: 0.2735\n",
      "Epoch [12/50], Step [380/777], Loss: 0.1861\n",
      "Epoch [12/50], Step [390/777], Loss: 0.0970\n",
      "Epoch [12/50], Step [400/777], Loss: 0.2594\n",
      "Epoch [12/50], Step [410/777], Loss: 0.1780\n",
      "Epoch [12/50], Step [420/777], Loss: 0.6066\n",
      "Epoch [12/50], Step [430/777], Loss: 0.6836\n",
      "Epoch [12/50], Step [440/777], Loss: 0.1191\n",
      "Epoch [12/50], Step [450/777], Loss: 0.4569\n",
      "Epoch [12/50], Step [460/777], Loss: 0.2578\n",
      "Epoch [12/50], Step [470/777], Loss: 0.3359\n",
      "Epoch [12/50], Step [480/777], Loss: 0.0342\n",
      "Epoch [12/50], Step [490/777], Loss: 0.0867\n",
      "Epoch [12/50], Step [500/777], Loss: 0.3780\n",
      "Epoch [12/50], Step [510/777], Loss: 0.3700\n",
      "Epoch [12/50], Step [520/777], Loss: 0.2396\n",
      "Epoch [12/50], Step [530/777], Loss: 0.2861\n",
      "Epoch [12/50], Step [540/777], Loss: 0.0948\n",
      "Epoch [12/50], Step [550/777], Loss: 0.3479\n",
      "Epoch [12/50], Step [560/777], Loss: 0.0367\n",
      "Epoch [12/50], Step [570/777], Loss: 0.0534\n",
      "Epoch [12/50], Step [580/777], Loss: 0.0818\n",
      "Epoch [12/50], Step [590/777], Loss: 0.3279\n",
      "Epoch [12/50], Step [600/777], Loss: 0.0501\n",
      "Epoch [12/50], Step [610/777], Loss: 0.2284\n",
      "Epoch [12/50], Step [620/777], Loss: 0.0259\n",
      "Epoch [12/50], Step [630/777], Loss: 0.0765\n",
      "Epoch [12/50], Step [640/777], Loss: 0.0754\n",
      "Epoch [12/50], Step [650/777], Loss: 0.0426\n",
      "Epoch [12/50], Step [660/777], Loss: 0.4156\n",
      "Epoch [12/50], Step [670/777], Loss: 0.1823\n",
      "Epoch [12/50], Step [680/777], Loss: 0.0523\n",
      "Epoch [12/50], Step [690/777], Loss: 0.0473\n",
      "Epoch [12/50], Step [700/777], Loss: 0.0121\n",
      "Epoch [12/50], Step [710/777], Loss: 0.0677\n",
      "Epoch [12/50], Step [720/777], Loss: 0.2606\n",
      "Epoch [12/50], Step [730/777], Loss: 0.3374\n",
      "Epoch [12/50], Step [740/777], Loss: 0.0761\n",
      "Epoch [12/50], Step [750/777], Loss: 0.0385\n",
      "Epoch [12/50], Step [760/777], Loss: 0.0625\n",
      "Epoch [12/50], Step [770/777], Loss: 0.1551\n",
      "Epoch [12/50], Train Loss: 0.2109, Val Loss: 0.2348, Val Accuracy: 0.9406\n",
      "Epoch [13/50], Step [10/777], Loss: 0.3603\n",
      "Epoch [13/50], Step [20/777], Loss: 0.0668\n",
      "Epoch [13/50], Step [30/777], Loss: 0.0114\n",
      "Epoch [13/50], Step [40/777], Loss: 0.0208\n",
      "Epoch [13/50], Step [50/777], Loss: 0.0523\n",
      "Epoch [13/50], Step [60/777], Loss: 0.2216\n",
      "Epoch [13/50], Step [70/777], Loss: 0.1416\n",
      "Epoch [13/50], Step [80/777], Loss: 0.1411\n",
      "Epoch [13/50], Step [90/777], Loss: 0.0291\n",
      "Epoch [13/50], Step [100/777], Loss: 1.1516\n",
      "Epoch [13/50], Step [110/777], Loss: 0.0726\n",
      "Epoch [13/50], Step [120/777], Loss: 0.3293\n",
      "Epoch [13/50], Step [130/777], Loss: 0.0412\n",
      "Epoch [13/50], Step [140/777], Loss: 0.0620\n",
      "Epoch [13/50], Step [150/777], Loss: 0.4345\n",
      "Epoch [13/50], Step [160/777], Loss: 0.2733\n",
      "Epoch [13/50], Step [170/777], Loss: 0.0210\n",
      "Epoch [13/50], Step [180/777], Loss: 0.1074\n",
      "Epoch [13/50], Step [190/777], Loss: 0.1550\n",
      "Epoch [13/50], Step [200/777], Loss: 0.2967\n",
      "Epoch [13/50], Step [210/777], Loss: 0.2041\n",
      "Epoch [13/50], Step [220/777], Loss: 0.0233\n",
      "Epoch [13/50], Step [230/777], Loss: 0.0449\n",
      "Epoch [13/50], Step [240/777], Loss: 0.0721\n",
      "Epoch [13/50], Step [250/777], Loss: 0.1166\n",
      "Epoch [13/50], Step [260/777], Loss: 0.2900\n",
      "Epoch [13/50], Step [270/777], Loss: 0.0317\n",
      "Epoch [13/50], Step [280/777], Loss: 0.0422\n",
      "Epoch [13/50], Step [290/777], Loss: 0.0955\n",
      "Epoch [13/50], Step [300/777], Loss: 0.0458\n",
      "Epoch [13/50], Step [310/777], Loss: 0.0677\n",
      "Epoch [13/50], Step [320/777], Loss: 0.3078\n",
      "Epoch [13/50], Step [330/777], Loss: 0.0918\n",
      "Epoch [13/50], Step [340/777], Loss: 0.0674\n",
      "Epoch [13/50], Step [350/777], Loss: 0.4229\n",
      "Epoch [13/50], Step [360/777], Loss: 0.0633\n",
      "Epoch [13/50], Step [370/777], Loss: 0.3968\n",
      "Epoch [13/50], Step [380/777], Loss: 0.1621\n",
      "Epoch [13/50], Step [390/777], Loss: 0.4253\n",
      "Epoch [13/50], Step [400/777], Loss: 0.7190\n",
      "Epoch [13/50], Step [410/777], Loss: 0.7636\n",
      "Epoch [13/50], Step [420/777], Loss: 0.0443\n",
      "Epoch [13/50], Step [430/777], Loss: 0.3138\n",
      "Epoch [13/50], Step [440/777], Loss: 0.0265\n",
      "Epoch [13/50], Step [450/777], Loss: 0.2519\n",
      "Epoch [13/50], Step [460/777], Loss: 0.0411\n",
      "Epoch [13/50], Step [470/777], Loss: 0.0740\n",
      "Epoch [13/50], Step [480/777], Loss: 0.0461\n",
      "Epoch [13/50], Step [490/777], Loss: 0.2767\n",
      "Epoch [13/50], Step [500/777], Loss: 0.0094\n",
      "Epoch [13/50], Step [510/777], Loss: 0.1346\n",
      "Epoch [13/50], Step [520/777], Loss: 0.0623\n",
      "Epoch [13/50], Step [530/777], Loss: 0.1769\n",
      "Epoch [13/50], Step [540/777], Loss: 0.5901\n",
      "Epoch [13/50], Step [550/777], Loss: 0.0884\n",
      "Epoch [13/50], Step [560/777], Loss: 0.0088\n",
      "Epoch [13/50], Step [570/777], Loss: 0.6398\n",
      "Epoch [13/50], Step [580/777], Loss: 0.0705\n",
      "Epoch [13/50], Step [590/777], Loss: 0.3030\n",
      "Epoch [13/50], Step [600/777], Loss: 0.1501\n",
      "Epoch [13/50], Step [610/777], Loss: 0.2522\n",
      "Epoch [13/50], Step [620/777], Loss: 0.5995\n",
      "Epoch [13/50], Step [630/777], Loss: 0.1382\n",
      "Epoch [13/50], Step [640/777], Loss: 0.0474\n",
      "Epoch [13/50], Step [650/777], Loss: 0.0302\n",
      "Epoch [13/50], Step [660/777], Loss: 0.3177\n",
      "Epoch [13/50], Step [670/777], Loss: 0.2784\n",
      "Epoch [13/50], Step [680/777], Loss: 0.2453\n",
      "Epoch [13/50], Step [690/777], Loss: 0.0521\n",
      "Epoch [13/50], Step [700/777], Loss: 0.0301\n",
      "Epoch [13/50], Step [710/777], Loss: 0.1701\n",
      "Epoch [13/50], Step [720/777], Loss: 0.1184\n",
      "Epoch [13/50], Step [730/777], Loss: 0.2193\n",
      "Epoch [13/50], Step [740/777], Loss: 0.2363\n",
      "Epoch [13/50], Step [750/777], Loss: 0.0311\n",
      "Epoch [13/50], Step [760/777], Loss: 0.4468\n",
      "Epoch [13/50], Step [770/777], Loss: 0.3240\n",
      "Epoch [13/50], Train Loss: 0.2023, Val Loss: 0.2263, Val Accuracy: 0.9421\n",
      "Model saved with validation accuracy: 0.9421\n",
      "Epoch [14/50], Step [10/777], Loss: 0.0331\n",
      "Epoch [14/50], Step [20/777], Loss: 0.0752\n",
      "Epoch [14/50], Step [30/777], Loss: 0.3919\n",
      "Epoch [14/50], Step [40/777], Loss: 0.4224\n",
      "Epoch [14/50], Step [50/777], Loss: 0.0503\n",
      "Epoch [14/50], Step [60/777], Loss: 0.5073\n",
      "Epoch [14/50], Step [70/777], Loss: 0.2638\n",
      "Epoch [14/50], Step [80/777], Loss: 0.1902\n",
      "Epoch [14/50], Step [90/777], Loss: 0.1152\n",
      "Epoch [14/50], Step [100/777], Loss: 0.0268\n",
      "Epoch [14/50], Step [110/777], Loss: 0.0276\n",
      "Epoch [14/50], Step [120/777], Loss: 0.0398\n",
      "Epoch [14/50], Step [130/777], Loss: 0.1416\n",
      "Epoch [14/50], Step [140/777], Loss: 0.0236\n",
      "Epoch [14/50], Step [150/777], Loss: 0.0361\n",
      "Epoch [14/50], Step [160/777], Loss: 0.2527\n",
      "Epoch [14/50], Step [170/777], Loss: 0.0216\n",
      "Epoch [14/50], Step [180/777], Loss: 0.1578\n",
      "Epoch [14/50], Step [190/777], Loss: 0.0371\n",
      "Epoch [14/50], Step [200/777], Loss: 0.0542\n",
      "Epoch [14/50], Step [210/777], Loss: 0.2056\n",
      "Epoch [14/50], Step [220/777], Loss: 1.0513\n",
      "Epoch [14/50], Step [230/777], Loss: 0.0299\n",
      "Epoch [14/50], Step [240/777], Loss: 0.1921\n",
      "Epoch [14/50], Step [250/777], Loss: 0.0562\n",
      "Epoch [14/50], Step [260/777], Loss: 0.0448\n",
      "Epoch [14/50], Step [270/777], Loss: 0.0862\n",
      "Epoch [14/50], Step [280/777], Loss: 0.2570\n",
      "Epoch [14/50], Step [290/777], Loss: 0.3551\n",
      "Epoch [14/50], Step [300/777], Loss: 0.2123\n",
      "Epoch [14/50], Step [310/777], Loss: 0.0358\n",
      "Epoch [14/50], Step [320/777], Loss: 0.2119\n",
      "Epoch [14/50], Step [330/777], Loss: 0.0124\n",
      "Epoch [14/50], Step [340/777], Loss: 0.0698\n",
      "Epoch [14/50], Step [350/777], Loss: 0.0312\n",
      "Epoch [14/50], Step [360/777], Loss: 0.1781\n",
      "Epoch [14/50], Step [370/777], Loss: 0.6216\n",
      "Epoch [14/50], Step [380/777], Loss: 0.2953\n",
      "Epoch [14/50], Step [390/777], Loss: 0.2346\n",
      "Epoch [14/50], Step [400/777], Loss: 0.3238\n",
      "Epoch [14/50], Step [410/777], Loss: 0.1592\n",
      "Epoch [14/50], Step [420/777], Loss: 0.1121\n",
      "Epoch [14/50], Step [430/777], Loss: 0.0292\n",
      "Epoch [14/50], Step [440/777], Loss: 0.6230\n",
      "Epoch [14/50], Step [450/777], Loss: 0.4317\n",
      "Epoch [14/50], Step [460/777], Loss: 0.2689\n",
      "Epoch [14/50], Step [470/777], Loss: 0.0153\n",
      "Epoch [14/50], Step [480/777], Loss: 0.1876\n",
      "Epoch [14/50], Step [490/777], Loss: 0.0411\n",
      "Epoch [14/50], Step [500/777], Loss: 0.4117\n",
      "Epoch [14/50], Step [510/777], Loss: 0.0417\n",
      "Epoch [14/50], Step [520/777], Loss: 0.6002\n",
      "Epoch [14/50], Step [530/777], Loss: 0.1667\n",
      "Epoch [14/50], Step [540/777], Loss: 0.2733\n",
      "Epoch [14/50], Step [550/777], Loss: 0.1117\n",
      "Epoch [14/50], Step [560/777], Loss: 0.4452\n",
      "Epoch [14/50], Step [570/777], Loss: 0.0362\n",
      "Epoch [14/50], Step [580/777], Loss: 0.3514\n",
      "Epoch [14/50], Step [590/777], Loss: 0.1351\n",
      "Epoch [14/50], Step [600/777], Loss: 0.1544\n",
      "Epoch [14/50], Step [610/777], Loss: 0.0930\n",
      "Epoch [14/50], Step [620/777], Loss: 0.0615\n",
      "Epoch [14/50], Step [630/777], Loss: 0.3873\n",
      "Epoch [14/50], Step [640/777], Loss: 0.0200\n",
      "Epoch [14/50], Step [650/777], Loss: 0.0764\n",
      "Epoch [14/50], Step [660/777], Loss: 0.0225\n",
      "Epoch [14/50], Step [670/777], Loss: 0.2451\n",
      "Epoch [14/50], Step [680/777], Loss: 0.3239\n",
      "Epoch [14/50], Step [690/777], Loss: 0.0738\n",
      "Epoch [14/50], Step [700/777], Loss: 0.5323\n",
      "Epoch [14/50], Step [710/777], Loss: 0.0624\n",
      "Epoch [14/50], Step [720/777], Loss: 0.4784\n",
      "Epoch [14/50], Step [730/777], Loss: 0.1861\n",
      "Epoch [14/50], Step [740/777], Loss: 0.0570\n",
      "Epoch [14/50], Step [750/777], Loss: 0.4298\n",
      "Epoch [14/50], Step [760/777], Loss: 0.6214\n",
      "Epoch [14/50], Step [770/777], Loss: 0.5418\n",
      "Epoch [14/50], Train Loss: 0.1946, Val Loss: 0.2441, Val Accuracy: 0.9365\n",
      "Epoch [15/50], Step [10/777], Loss: 0.2739\n",
      "Epoch [15/50], Step [20/777], Loss: 0.1821\n",
      "Epoch [15/50], Step [30/777], Loss: 0.1719\n",
      "Epoch [15/50], Step [40/777], Loss: 0.0324\n",
      "Epoch [15/50], Step [50/777], Loss: 0.3386\n",
      "Epoch [15/50], Step [60/777], Loss: 0.2395\n",
      "Epoch [15/50], Step [70/777], Loss: 0.0334\n",
      "Epoch [15/50], Step [80/777], Loss: 0.0203\n",
      "Epoch [15/50], Step [90/777], Loss: 0.2143\n",
      "Epoch [15/50], Step [100/777], Loss: 0.4197\n",
      "Epoch [15/50], Step [110/777], Loss: 0.0983\n",
      "Epoch [15/50], Step [120/777], Loss: 0.0844\n",
      "Epoch [15/50], Step [130/777], Loss: 0.1855\n",
      "Epoch [15/50], Step [140/777], Loss: 0.4823\n",
      "Epoch [15/50], Step [150/777], Loss: 0.0470\n",
      "Epoch [15/50], Step [160/777], Loss: 0.0237\n",
      "Epoch [15/50], Step [170/777], Loss: 0.0194\n",
      "Epoch [15/50], Step [180/777], Loss: 0.1784\n",
      "Epoch [15/50], Step [190/777], Loss: 0.0633\n",
      "Epoch [15/50], Step [200/777], Loss: 0.1950\n",
      "Epoch [15/50], Step [210/777], Loss: 0.0103\n",
      "Epoch [15/50], Step [220/777], Loss: 0.0094\n",
      "Epoch [15/50], Step [230/777], Loss: 0.0389\n",
      "Epoch [15/50], Step [240/777], Loss: 0.3334\n",
      "Epoch [15/50], Step [250/777], Loss: 0.1476\n",
      "Epoch [15/50], Step [260/777], Loss: 0.0235\n",
      "Epoch [15/50], Step [270/777], Loss: 0.0234\n",
      "Epoch [15/50], Step [280/777], Loss: 0.2487\n",
      "Epoch [15/50], Step [290/777], Loss: 0.5296\n",
      "Epoch [15/50], Step [300/777], Loss: 0.0320\n",
      "Epoch [15/50], Step [310/777], Loss: 0.0776\n",
      "Epoch [15/50], Step [320/777], Loss: 0.6898\n",
      "Epoch [15/50], Step [330/777], Loss: 0.0311\n",
      "Epoch [15/50], Step [340/777], Loss: 0.2447\n",
      "Epoch [15/50], Step [350/777], Loss: 0.3137\n",
      "Epoch [15/50], Step [360/777], Loss: 0.0542\n",
      "Epoch [15/50], Step [370/777], Loss: 0.2833\n",
      "Epoch [15/50], Step [380/777], Loss: 0.1684\n",
      "Epoch [15/50], Step [390/777], Loss: 0.0414\n",
      "Epoch [15/50], Step [400/777], Loss: 0.0431\n",
      "Epoch [15/50], Step [410/777], Loss: 0.2476\n",
      "Epoch [15/50], Step [420/777], Loss: 0.4240\n",
      "Epoch [15/50], Step [430/777], Loss: 0.0317\n",
      "Epoch [15/50], Step [440/777], Loss: 0.0827\n",
      "Epoch [15/50], Step [450/777], Loss: 0.0182\n",
      "Epoch [15/50], Step [460/777], Loss: 0.3740\n",
      "Epoch [15/50], Step [470/777], Loss: 0.0891\n",
      "Epoch [15/50], Step [480/777], Loss: 0.0088\n",
      "Epoch [15/50], Step [490/777], Loss: 0.2611\n",
      "Epoch [15/50], Step [500/777], Loss: 0.1083\n",
      "Epoch [15/50], Step [510/777], Loss: 0.2928\n",
      "Epoch [15/50], Step [520/777], Loss: 0.4872\n",
      "Epoch [15/50], Step [530/777], Loss: 0.0422\n",
      "Epoch [15/50], Step [540/777], Loss: 0.4814\n",
      "Epoch [15/50], Step [550/777], Loss: 0.1148\n",
      "Epoch [15/50], Step [560/777], Loss: 0.2976\n",
      "Epoch [15/50], Step [570/777], Loss: 0.1517\n",
      "Epoch [15/50], Step [580/777], Loss: 0.0484\n",
      "Epoch [15/50], Step [590/777], Loss: 0.2280\n",
      "Epoch [15/50], Step [600/777], Loss: 0.2267\n",
      "Epoch [15/50], Step [610/777], Loss: 0.2719\n",
      "Epoch [15/50], Step [620/777], Loss: 0.1035\n",
      "Epoch [15/50], Step [630/777], Loss: 0.6023\n",
      "Epoch [15/50], Step [640/777], Loss: 0.0753\n",
      "Epoch [15/50], Step [650/777], Loss: 0.2670\n",
      "Epoch [15/50], Step [660/777], Loss: 0.3500\n",
      "Epoch [15/50], Step [670/777], Loss: 0.4820\n",
      "Epoch [15/50], Step [680/777], Loss: 0.1126\n",
      "Epoch [15/50], Step [690/777], Loss: 0.1723\n",
      "Epoch [15/50], Step [700/777], Loss: 0.0283\n",
      "Epoch [15/50], Step [710/777], Loss: 0.0281\n",
      "Epoch [15/50], Step [720/777], Loss: 0.2702\n",
      "Epoch [15/50], Step [730/777], Loss: 0.0422\n",
      "Epoch [15/50], Step [740/777], Loss: 0.0090\n",
      "Epoch [15/50], Step [750/777], Loss: 0.3959\n",
      "Epoch [15/50], Step [760/777], Loss: 0.1096\n",
      "Epoch [15/50], Step [770/777], Loss: 0.2451\n",
      "Epoch [15/50], Train Loss: 0.1885, Val Loss: 0.2493, Val Accuracy: 0.9320\n",
      "Epoch [16/50], Step [10/777], Loss: 0.1802\n",
      "Epoch [16/50], Step [20/777], Loss: 0.1214\n",
      "Epoch [16/50], Step [30/777], Loss: 0.2136\n",
      "Epoch [16/50], Step [40/777], Loss: 0.2481\n",
      "Epoch [16/50], Step [50/777], Loss: 0.2284\n",
      "Epoch [16/50], Step [60/777], Loss: 0.0233\n",
      "Epoch [16/50], Step [70/777], Loss: 0.0521\n",
      "Epoch [16/50], Step [80/777], Loss: 0.0689\n",
      "Epoch [16/50], Step [90/777], Loss: 0.1782\n",
      "Epoch [16/50], Step [100/777], Loss: 0.0340\n",
      "Epoch [16/50], Step [110/777], Loss: 0.3213\n",
      "Epoch [16/50], Step [120/777], Loss: 0.0565\n",
      "Epoch [16/50], Step [130/777], Loss: 0.0452\n",
      "Epoch [16/50], Step [140/777], Loss: 0.0398\n",
      "Epoch [16/50], Step [150/777], Loss: 0.0265\n",
      "Epoch [16/50], Step [160/777], Loss: 0.2148\n",
      "Epoch [16/50], Step [170/777], Loss: 0.0294\n",
      "Epoch [16/50], Step [180/777], Loss: 0.3788\n",
      "Epoch [16/50], Step [190/777], Loss: 0.0133\n",
      "Epoch [16/50], Step [200/777], Loss: 0.0474\n",
      "Epoch [16/50], Step [210/777], Loss: 0.4042\n",
      "Epoch [16/50], Step [220/777], Loss: 0.0376\n",
      "Epoch [16/50], Step [230/777], Loss: 0.1773\n",
      "Epoch [16/50], Step [240/777], Loss: 0.4654\n",
      "Epoch [16/50], Step [250/777], Loss: 0.0753\n",
      "Epoch [16/50], Step [260/777], Loss: 0.0185\n",
      "Epoch [16/50], Step [270/777], Loss: 0.4487\n",
      "Epoch [16/50], Step [280/777], Loss: 0.0717\n",
      "Epoch [16/50], Step [290/777], Loss: 0.5605\n",
      "Epoch [16/50], Step [300/777], Loss: 0.1782\n",
      "Epoch [16/50], Step [310/777], Loss: 0.0285\n",
      "Epoch [16/50], Step [320/777], Loss: 0.0332\n",
      "Epoch [16/50], Step [330/777], Loss: 0.2702\n",
      "Epoch [16/50], Step [340/777], Loss: 0.2246\n",
      "Epoch [16/50], Step [350/777], Loss: 0.5072\n",
      "Epoch [16/50], Step [360/777], Loss: 0.1872\n",
      "Epoch [16/50], Step [370/777], Loss: 0.2917\n",
      "Epoch [16/50], Step [380/777], Loss: 0.0501\n",
      "Epoch [16/50], Step [390/777], Loss: 0.0977\n",
      "Epoch [16/50], Step [400/777], Loss: 0.0194\n",
      "Epoch [16/50], Step [410/777], Loss: 0.0402\n",
      "Epoch [16/50], Step [420/777], Loss: 0.5285\n",
      "Epoch [16/50], Step [430/777], Loss: 0.0295\n",
      "Epoch [16/50], Step [440/777], Loss: 0.0092\n",
      "Epoch [16/50], Step [450/777], Loss: 0.4433\n",
      "Epoch [16/50], Step [460/777], Loss: 0.2393\n",
      "Epoch [16/50], Step [470/777], Loss: 0.0709\n",
      "Epoch [16/50], Step [480/777], Loss: 0.2493\n",
      "Epoch [16/50], Step [490/777], Loss: 0.0294\n",
      "Epoch [16/50], Step [500/777], Loss: 0.0769\n",
      "Epoch [16/50], Step [510/777], Loss: 0.3051\n",
      "Epoch [16/50], Step [520/777], Loss: 0.0334\n",
      "Epoch [16/50], Step [530/777], Loss: 0.1838\n",
      "Epoch [16/50], Step [540/777], Loss: 0.1876\n",
      "Epoch [16/50], Step [550/777], Loss: 0.0237\n",
      "Epoch [16/50], Step [560/777], Loss: 0.0261\n",
      "Epoch [16/50], Step [570/777], Loss: 0.5087\n",
      "Epoch [16/50], Step [580/777], Loss: 0.1844\n",
      "Epoch [16/50], Step [590/777], Loss: 0.4510\n",
      "Epoch [16/50], Step [600/777], Loss: 0.0377\n",
      "Epoch [16/50], Step [610/777], Loss: 0.0209\n",
      "Epoch [16/50], Step [620/777], Loss: 0.3124\n",
      "Epoch [16/50], Step [630/777], Loss: 0.0292\n",
      "Epoch [16/50], Step [640/777], Loss: 0.2091\n",
      "Epoch [16/50], Step [650/777], Loss: 0.2265\n",
      "Epoch [16/50], Step [660/777], Loss: 0.0439\n",
      "Epoch [16/50], Step [670/777], Loss: 0.2555\n",
      "Epoch [16/50], Step [680/777], Loss: 0.2180\n",
      "Epoch [16/50], Step [690/777], Loss: 0.6249\n",
      "Epoch [16/50], Step [700/777], Loss: 0.0415\n",
      "Epoch [16/50], Step [710/777], Loss: 0.3943\n",
      "Epoch [16/50], Step [720/777], Loss: 0.0804\n",
      "Epoch [16/50], Step [730/777], Loss: 0.2189\n",
      "Epoch [16/50], Step [740/777], Loss: 0.2613\n",
      "Epoch [16/50], Step [750/777], Loss: 0.0163\n",
      "Epoch [16/50], Step [760/777], Loss: 0.1585\n",
      "Epoch [16/50], Step [770/777], Loss: 0.0326\n",
      "Epoch [16/50], Train Loss: 0.1539, Val Loss: 0.2142, Val Accuracy: 0.9459\n",
      "Model saved with validation accuracy: 0.9459\n",
      "Epoch [17/50], Step [10/777], Loss: 0.2779\n",
      "Epoch [17/50], Step [20/777], Loss: 0.0105\n",
      "Epoch [17/50], Step [30/777], Loss: 0.1586\n",
      "Epoch [17/50], Step [40/777], Loss: 0.0940\n",
      "Epoch [17/50], Step [50/777], Loss: 0.0768\n",
      "Epoch [17/50], Step [60/777], Loss: 0.0286\n",
      "Epoch [17/50], Step [70/777], Loss: 0.0158\n",
      "Epoch [17/50], Step [80/777], Loss: 0.0338\n",
      "Epoch [17/50], Step [90/777], Loss: 0.1280\n",
      "Epoch [17/50], Step [100/777], Loss: 0.3723\n",
      "Epoch [17/50], Step [110/777], Loss: 0.4138\n",
      "Epoch [17/50], Step [120/777], Loss: 0.0142\n",
      "Epoch [17/50], Step [130/777], Loss: 0.2021\n",
      "Epoch [17/50], Step [140/777], Loss: 0.2455\n",
      "Epoch [17/50], Step [150/777], Loss: 0.0393\n",
      "Epoch [17/50], Step [160/777], Loss: 0.3837\n",
      "Epoch [17/50], Step [170/777], Loss: 0.0137\n",
      "Epoch [17/50], Step [180/777], Loss: 0.0034\n",
      "Epoch [17/50], Step [190/777], Loss: 0.6642\n",
      "Epoch [17/50], Step [200/777], Loss: 0.0214\n",
      "Epoch [17/50], Step [210/777], Loss: 0.0364\n",
      "Epoch [17/50], Step [220/777], Loss: 0.3344\n",
      "Epoch [17/50], Step [230/777], Loss: 0.0516\n",
      "Epoch [17/50], Step [240/777], Loss: 0.0106\n",
      "Epoch [17/50], Step [250/777], Loss: 0.1032\n",
      "Epoch [17/50], Step [260/777], Loss: 0.0225\n",
      "Epoch [17/50], Step [270/777], Loss: 0.0390\n",
      "Epoch [17/50], Step [280/777], Loss: 0.6774\n",
      "Epoch [17/50], Step [290/777], Loss: 0.0437\n",
      "Epoch [17/50], Step [300/777], Loss: 0.0263\n",
      "Epoch [17/50], Step [310/777], Loss: 0.0501\n",
      "Epoch [17/50], Step [320/777], Loss: 0.1801\n",
      "Epoch [17/50], Step [330/777], Loss: 0.0368\n",
      "Epoch [17/50], Step [340/777], Loss: 0.2328\n",
      "Epoch [17/50], Step [350/777], Loss: 0.0233\n",
      "Epoch [17/50], Step [360/777], Loss: 0.0287\n",
      "Epoch [17/50], Step [370/777], Loss: 0.1613\n",
      "Epoch [17/50], Step [380/777], Loss: 0.0242\n",
      "Epoch [17/50], Step [390/777], Loss: 0.1608\n",
      "Epoch [17/50], Step [400/777], Loss: 0.2498\n",
      "Epoch [17/50], Step [410/777], Loss: 0.0213\n",
      "Epoch [17/50], Step [420/777], Loss: 0.5652\n",
      "Epoch [17/50], Step [430/777], Loss: 0.1194\n",
      "Epoch [17/50], Step [440/777], Loss: 0.1023\n",
      "Epoch [17/50], Step [450/777], Loss: 0.0070\n",
      "Epoch [17/50], Step [460/777], Loss: 0.0110\n",
      "Epoch [17/50], Step [470/777], Loss: 0.1626\n",
      "Epoch [17/50], Step [480/777], Loss: 0.4788\n",
      "Epoch [17/50], Step [490/777], Loss: 0.0088\n",
      "Epoch [17/50], Step [500/777], Loss: 0.5285\n",
      "Epoch [17/50], Step [510/777], Loss: 0.0324\n",
      "Epoch [17/50], Step [520/777], Loss: 0.1975\n",
      "Epoch [17/50], Step [530/777], Loss: 0.0536\n",
      "Epoch [17/50], Step [540/777], Loss: 0.0602\n",
      "Epoch [17/50], Step [550/777], Loss: 0.2114\n",
      "Epoch [17/50], Step [560/777], Loss: 0.2243\n",
      "Epoch [17/50], Step [570/777], Loss: 0.2748\n",
      "Epoch [17/50], Step [580/777], Loss: 0.1174\n",
      "Epoch [17/50], Step [590/777], Loss: 0.0139\n",
      "Epoch [17/50], Step [600/777], Loss: 0.0111\n",
      "Epoch [17/50], Step [610/777], Loss: 0.0252\n",
      "Epoch [17/50], Step [620/777], Loss: 0.0179\n",
      "Epoch [17/50], Step [630/777], Loss: 0.0758\n",
      "Epoch [17/50], Step [640/777], Loss: 0.3731\n",
      "Epoch [17/50], Step [650/777], Loss: 0.0516\n",
      "Epoch [17/50], Step [660/777], Loss: 0.4016\n",
      "Epoch [17/50], Step [670/777], Loss: 0.6433\n",
      "Epoch [17/50], Step [680/777], Loss: 0.0418\n",
      "Epoch [17/50], Step [690/777], Loss: 0.0110\n",
      "Epoch [17/50], Step [700/777], Loss: 0.0301\n",
      "Epoch [17/50], Step [710/777], Loss: 0.3168\n",
      "Epoch [17/50], Step [720/777], Loss: 0.1663\n",
      "Epoch [17/50], Step [730/777], Loss: 0.0082\n",
      "Epoch [17/50], Step [740/777], Loss: 0.1051\n",
      "Epoch [17/50], Step [750/777], Loss: 0.5615\n",
      "Epoch [17/50], Step [760/777], Loss: 0.1440\n",
      "Epoch [17/50], Step [770/777], Loss: 0.2962\n",
      "Epoch [17/50], Train Loss: 0.1428, Val Loss: 0.2254, Val Accuracy: 0.9399\n",
      "Epoch [18/50], Step [10/777], Loss: 0.2174\n",
      "Epoch [18/50], Step [20/777], Loss: 0.0810\n",
      "Epoch [18/50], Step [30/777], Loss: 0.0140\n",
      "Epoch [18/50], Step [40/777], Loss: 0.2275\n",
      "Epoch [18/50], Step [50/777], Loss: 0.0428\n",
      "Epoch [18/50], Step [60/777], Loss: 0.3348\n",
      "Epoch [18/50], Step [70/777], Loss: 0.0780\n",
      "Epoch [18/50], Step [80/777], Loss: 0.5161\n",
      "Epoch [18/50], Step [90/777], Loss: 0.0121\n",
      "Epoch [18/50], Step [100/777], Loss: 0.2796\n",
      "Epoch [18/50], Step [110/777], Loss: 0.0286\n",
      "Epoch [18/50], Step [120/777], Loss: 0.1356\n",
      "Epoch [18/50], Step [130/777], Loss: 0.1160\n",
      "Epoch [18/50], Step [140/777], Loss: 0.1926\n",
      "Epoch [18/50], Step [150/777], Loss: 0.0432\n",
      "Epoch [18/50], Step [160/777], Loss: 0.0275\n",
      "Epoch [18/50], Step [170/777], Loss: 0.0074\n",
      "Epoch [18/50], Step [180/777], Loss: 0.2216\n",
      "Epoch [18/50], Step [190/777], Loss: 0.0686\n",
      "Epoch [18/50], Step [200/777], Loss: 0.2064\n",
      "Epoch [18/50], Step [210/777], Loss: 0.1672\n",
      "Epoch [18/50], Step [220/777], Loss: 0.1285\n",
      "Epoch [18/50], Step [230/777], Loss: 0.0947\n",
      "Epoch [18/50], Step [240/777], Loss: 0.2217\n",
      "Epoch [18/50], Step [250/777], Loss: 0.0292\n",
      "Epoch [18/50], Step [260/777], Loss: 0.0318\n",
      "Epoch [18/50], Step [270/777], Loss: 0.2329\n",
      "Epoch [18/50], Step [280/777], Loss: 0.2523\n",
      "Epoch [18/50], Step [290/777], Loss: 0.0183\n",
      "Epoch [18/50], Step [300/777], Loss: 0.2000\n",
      "Epoch [18/50], Step [310/777], Loss: 0.1148\n",
      "Epoch [18/50], Step [320/777], Loss: 0.0436\n",
      "Epoch [18/50], Step [330/777], Loss: 0.0262\n",
      "Epoch [18/50], Step [340/777], Loss: 0.0286\n",
      "Epoch [18/50], Step [350/777], Loss: 0.0873\n",
      "Epoch [18/50], Step [360/777], Loss: 0.5670\n",
      "Epoch [18/50], Step [370/777], Loss: 0.0297\n",
      "Epoch [18/50], Step [380/777], Loss: 0.2974\n",
      "Epoch [18/50], Step [390/777], Loss: 0.3983\n",
      "Epoch [18/50], Step [400/777], Loss: 0.4487\n",
      "Epoch [18/50], Step [410/777], Loss: 0.2157\n",
      "Epoch [18/50], Step [420/777], Loss: 0.0224\n",
      "Epoch [18/50], Step [430/777], Loss: 0.2432\n",
      "Epoch [18/50], Step [440/777], Loss: 0.0643\n",
      "Epoch [18/50], Step [450/777], Loss: 0.0511\n",
      "Epoch [18/50], Step [460/777], Loss: 0.1313\n",
      "Epoch [18/50], Step [470/777], Loss: 0.3072\n",
      "Epoch [18/50], Step [480/777], Loss: 0.1310\n",
      "Epoch [18/50], Step [490/777], Loss: 0.4495\n",
      "Epoch [18/50], Step [500/777], Loss: 0.0308\n",
      "Epoch [18/50], Step [510/777], Loss: 0.0470\n",
      "Epoch [18/50], Step [520/777], Loss: 0.2955\n",
      "Epoch [18/50], Step [530/777], Loss: 0.1269\n",
      "Epoch [18/50], Step [540/777], Loss: 0.0381\n",
      "Epoch [18/50], Step [550/777], Loss: 0.2560\n",
      "Epoch [18/50], Step [560/777], Loss: 0.0240\n",
      "Epoch [18/50], Step [570/777], Loss: 0.1956\n",
      "Epoch [18/50], Step [580/777], Loss: 0.1166\n",
      "Epoch [18/50], Step [590/777], Loss: 0.0205\n",
      "Epoch [18/50], Step [600/777], Loss: 0.0228\n",
      "Epoch [18/50], Step [610/777], Loss: 0.0187\n",
      "Epoch [18/50], Step [620/777], Loss: 0.0680\n",
      "Epoch [18/50], Step [630/777], Loss: 0.5017\n",
      "Epoch [18/50], Step [640/777], Loss: 0.0278\n",
      "Epoch [18/50], Step [650/777], Loss: 0.1966\n",
      "Epoch [18/50], Step [660/777], Loss: 0.0353\n",
      "Epoch [18/50], Step [670/777], Loss: 0.2383\n",
      "Epoch [18/50], Step [680/777], Loss: 0.2795\n",
      "Epoch [18/50], Step [690/777], Loss: 0.0185\n",
      "Epoch [18/50], Step [700/777], Loss: 0.1759\n",
      "Epoch [18/50], Step [710/777], Loss: 0.0256\n",
      "Epoch [18/50], Step [720/777], Loss: 0.2303\n",
      "Epoch [18/50], Step [730/777], Loss: 0.1768\n",
      "Epoch [18/50], Step [740/777], Loss: 0.2434\n",
      "Epoch [18/50], Step [750/777], Loss: 0.0367\n",
      "Epoch [18/50], Step [760/777], Loss: 0.0695\n",
      "Epoch [18/50], Step [770/777], Loss: 0.0160\n",
      "Epoch [18/50], Train Loss: 0.1402, Val Loss: 0.2183, Val Accuracy: 0.9440\n",
      "Epoch [19/50], Step [10/777], Loss: 0.0178\n",
      "Epoch [19/50], Step [20/777], Loss: 0.0496\n",
      "Epoch [19/50], Step [30/777], Loss: 0.2376\n",
      "Epoch [19/50], Step [40/777], Loss: 0.0313\n",
      "Epoch [19/50], Step [50/777], Loss: 0.0199\n",
      "Epoch [19/50], Step [60/777], Loss: 0.3164\n",
      "Epoch [19/50], Step [70/777], Loss: 0.0485\n",
      "Epoch [19/50], Step [80/777], Loss: 0.2351\n",
      "Epoch [19/50], Step [90/777], Loss: 0.0973\n",
      "Epoch [19/50], Step [100/777], Loss: 0.1617\n",
      "Epoch [19/50], Step [110/777], Loss: 0.2805\n",
      "Epoch [19/50], Step [120/777], Loss: 0.1583\n",
      "Epoch [19/50], Step [130/777], Loss: 0.1591\n",
      "Epoch [19/50], Step [140/777], Loss: 0.2555\n",
      "Epoch [19/50], Step [150/777], Loss: 0.1799\n",
      "Epoch [19/50], Step [160/777], Loss: 0.0548\n",
      "Epoch [19/50], Step [170/777], Loss: 0.0354\n",
      "Epoch [19/50], Step [180/777], Loss: 0.0358\n",
      "Epoch [19/50], Step [190/777], Loss: 0.1458\n",
      "Epoch [19/50], Step [200/777], Loss: 0.1859\n",
      "Epoch [19/50], Step [210/777], Loss: 0.0123\n",
      "Epoch [19/50], Step [220/777], Loss: 0.6053\n",
      "Epoch [19/50], Step [230/777], Loss: 0.5557\n",
      "Epoch [19/50], Step [240/777], Loss: 0.0916\n",
      "Epoch [19/50], Step [250/777], Loss: 0.1082\n",
      "Epoch [19/50], Step [260/777], Loss: 0.1064\n",
      "Epoch [19/50], Step [270/777], Loss: 0.0226\n",
      "Epoch [19/50], Step [280/777], Loss: 0.1941\n",
      "Epoch [19/50], Step [290/777], Loss: 0.0210\n",
      "Epoch [19/50], Step [300/777], Loss: 0.0290\n",
      "Epoch [19/50], Step [310/777], Loss: 0.0252\n",
      "Epoch [19/50], Step [320/777], Loss: 0.4870\n",
      "Epoch [19/50], Step [330/777], Loss: 0.0296\n",
      "Epoch [19/50], Step [340/777], Loss: 0.3270\n",
      "Epoch [19/50], Step [350/777], Loss: 0.2031\n",
      "Epoch [19/50], Step [360/777], Loss: 0.0783\n",
      "Epoch [19/50], Step [370/777], Loss: 0.0155\n",
      "Epoch [19/50], Step [380/777], Loss: 0.5180\n",
      "Epoch [19/50], Step [390/777], Loss: 0.1750\n",
      "Epoch [19/50], Step [400/777], Loss: 0.0321\n",
      "Epoch [19/50], Step [410/777], Loss: 0.5130\n",
      "Epoch [19/50], Step [420/777], Loss: 0.2968\n",
      "Epoch [19/50], Step [430/777], Loss: 0.0457\n",
      "Epoch [19/50], Step [440/777], Loss: 0.0340\n",
      "Epoch [19/50], Step [450/777], Loss: 0.0293\n",
      "Epoch [19/50], Step [460/777], Loss: 0.0203\n",
      "Epoch [19/50], Step [470/777], Loss: 0.0739\n",
      "Epoch [19/50], Step [480/777], Loss: 0.0274\n",
      "Epoch [19/50], Step [490/777], Loss: 0.0856\n",
      "Epoch [19/50], Step [500/777], Loss: 0.0330\n",
      "Epoch [19/50], Step [510/777], Loss: 0.0620\n",
      "Epoch [19/50], Step [520/777], Loss: 0.2951\n",
      "Epoch [19/50], Step [530/777], Loss: 0.0580\n",
      "Epoch [19/50], Step [540/777], Loss: 0.0369\n",
      "Epoch [19/50], Step [550/777], Loss: 0.1785\n",
      "Epoch [19/50], Step [560/777], Loss: 0.0574\n",
      "Epoch [19/50], Step [570/777], Loss: 0.0473\n",
      "Epoch [19/50], Step [580/777], Loss: 0.0332\n",
      "Epoch [19/50], Step [590/777], Loss: 0.1649\n",
      "Epoch [19/50], Step [600/777], Loss: 0.2963\n",
      "Epoch [19/50], Step [610/777], Loss: 0.0173\n",
      "Epoch [19/50], Step [620/777], Loss: 0.0104\n",
      "Epoch [19/50], Step [630/777], Loss: 0.0287\n",
      "Epoch [19/50], Step [640/777], Loss: 0.0123\n",
      "Epoch [19/50], Step [650/777], Loss: 0.1258\n",
      "Epoch [19/50], Step [660/777], Loss: 0.0288\n",
      "Epoch [19/50], Step [670/777], Loss: 0.0398\n",
      "Epoch [19/50], Step [680/777], Loss: 0.2919\n",
      "Epoch [19/50], Step [690/777], Loss: 0.0414\n",
      "Epoch [19/50], Step [700/777], Loss: 0.0226\n",
      "Epoch [19/50], Step [710/777], Loss: 0.0220\n",
      "Epoch [19/50], Step [720/777], Loss: 0.0206\n",
      "Epoch [19/50], Step [730/777], Loss: 0.0328\n",
      "Epoch [19/50], Step [740/777], Loss: 0.1787\n",
      "Epoch [19/50], Step [750/777], Loss: 0.0217\n",
      "Epoch [19/50], Step [760/777], Loss: 0.1230\n",
      "Epoch [19/50], Step [770/777], Loss: 0.3110\n",
      "Epoch [19/50], Train Loss: 0.1361, Val Loss: 0.2248, Val Accuracy: 0.9440\n",
      "Epoch [20/50], Step [10/777], Loss: 0.0737\n",
      "Epoch [20/50], Step [20/777], Loss: 0.0360\n",
      "Epoch [20/50], Step [30/777], Loss: 0.1506\n",
      "Epoch [20/50], Step [40/777], Loss: 0.0074\n",
      "Epoch [20/50], Step [50/777], Loss: 0.2070\n",
      "Epoch [20/50], Step [60/777], Loss: 0.4480\n",
      "Epoch [20/50], Step [70/777], Loss: 0.0306\n",
      "Epoch [20/50], Step [80/777], Loss: 0.0474\n",
      "Epoch [20/50], Step [90/777], Loss: 0.0441\n",
      "Epoch [20/50], Step [100/777], Loss: 0.0381\n",
      "Epoch [20/50], Step [110/777], Loss: 0.0390\n",
      "Epoch [20/50], Step [120/777], Loss: 0.0058\n",
      "Epoch [20/50], Step [130/777], Loss: 0.2185\n",
      "Epoch [20/50], Step [140/777], Loss: 0.0331\n",
      "Epoch [20/50], Step [150/777], Loss: 0.1317\n",
      "Epoch [20/50], Step [160/777], Loss: 0.0440\n",
      "Epoch [20/50], Step [170/777], Loss: 0.0253\n",
      "Epoch [20/50], Step [180/777], Loss: 0.1499\n",
      "Epoch [20/50], Step [190/777], Loss: 0.0354\n",
      "Epoch [20/50], Step [200/777], Loss: 0.0472\n",
      "Epoch [20/50], Step [210/777], Loss: 0.4523\n",
      "Epoch [20/50], Step [220/777], Loss: 0.2067\n",
      "Epoch [20/50], Step [230/777], Loss: 0.1035\n",
      "Epoch [20/50], Step [240/777], Loss: 0.0731\n",
      "Epoch [20/50], Step [250/777], Loss: 0.2406\n",
      "Epoch [20/50], Step [260/777], Loss: 0.3520\n",
      "Epoch [20/50], Step [270/777], Loss: 0.2681\n",
      "Epoch [20/50], Step [280/777], Loss: 0.3693\n",
      "Epoch [20/50], Step [290/777], Loss: 0.2731\n",
      "Epoch [20/50], Step [300/777], Loss: 0.0492\n",
      "Epoch [20/50], Step [310/777], Loss: 0.0779\n",
      "Epoch [20/50], Step [320/777], Loss: 0.5190\n",
      "Epoch [20/50], Step [330/777], Loss: 0.1594\n",
      "Epoch [20/50], Step [340/777], Loss: 0.1586\n",
      "Epoch [20/50], Step [350/777], Loss: 0.0140\n",
      "Epoch [20/50], Step [360/777], Loss: 0.0433\n",
      "Epoch [20/50], Step [370/777], Loss: 0.0384\n",
      "Epoch [20/50], Step [380/777], Loss: 0.0168\n",
      "Epoch [20/50], Step [390/777], Loss: 0.0589\n",
      "Epoch [20/50], Step [400/777], Loss: 0.3063\n",
      "Epoch [20/50], Step [410/777], Loss: 0.0402\n",
      "Epoch [20/50], Step [420/777], Loss: 0.0404\n",
      "Epoch [20/50], Step [430/777], Loss: 0.0851\n",
      "Epoch [20/50], Step [440/777], Loss: 0.0218\n",
      "Epoch [20/50], Step [450/777], Loss: 0.0559\n",
      "Epoch [20/50], Step [460/777], Loss: 0.1543\n",
      "Epoch [20/50], Step [470/777], Loss: 0.3451\n",
      "Epoch [20/50], Step [480/777], Loss: 0.0212\n",
      "Epoch [20/50], Step [490/777], Loss: 0.0457\n",
      "Epoch [20/50], Step [500/777], Loss: 0.0130\n",
      "Epoch [20/50], Step [510/777], Loss: 0.0904\n",
      "Epoch [20/50], Step [520/777], Loss: 0.0336\n",
      "Epoch [20/50], Step [530/777], Loss: 0.1909\n",
      "Epoch [20/50], Step [540/777], Loss: 0.1993\n",
      "Epoch [20/50], Step [550/777], Loss: 0.1978\n",
      "Epoch [20/50], Step [560/777], Loss: 0.0357\n",
      "Epoch [20/50], Step [570/777], Loss: 0.0734\n",
      "Epoch [20/50], Step [580/777], Loss: 0.2021\n",
      "Epoch [20/50], Step [590/777], Loss: 0.4396\n",
      "Epoch [20/50], Step [600/777], Loss: 0.0432\n",
      "Epoch [20/50], Step [610/777], Loss: 0.0364\n",
      "Epoch [20/50], Step [620/777], Loss: 0.3462\n",
      "Epoch [20/50], Step [630/777], Loss: 0.0318\n",
      "Epoch [20/50], Step [640/777], Loss: 0.0370\n",
      "Epoch [20/50], Step [650/777], Loss: 0.0717\n",
      "Epoch [20/50], Step [660/777], Loss: 0.0290\n",
      "Epoch [20/50], Step [670/777], Loss: 0.2012\n",
      "Epoch [20/50], Step [680/777], Loss: 0.0877\n",
      "Epoch [20/50], Step [690/777], Loss: 0.0125\n",
      "Epoch [20/50], Step [700/777], Loss: 0.0620\n",
      "Epoch [20/50], Step [710/777], Loss: 0.0303\n",
      "Epoch [20/50], Step [720/777], Loss: 0.1859\n",
      "Epoch [20/50], Step [730/777], Loss: 0.5362\n",
      "Epoch [20/50], Step [740/777], Loss: 0.0195\n",
      "Epoch [20/50], Step [750/777], Loss: 0.0336\n",
      "Epoch [20/50], Step [760/777], Loss: 0.0288\n",
      "Epoch [20/50], Step [770/777], Loss: 0.4350\n",
      "Epoch [20/50], Train Loss: 0.1335, Val Loss: 0.2232, Val Accuracy: 0.9444\n",
      "Epoch [21/50], Step [10/777], Loss: 0.0884\n",
      "Epoch [21/50], Step [20/777], Loss: 0.1988\n",
      "Epoch [21/50], Step [30/777], Loss: 0.0263\n",
      "Epoch [21/50], Step [40/777], Loss: 0.0161\n",
      "Epoch [21/50], Step [50/777], Loss: 0.0136\n",
      "Epoch [21/50], Step [60/777], Loss: 0.2217\n",
      "Epoch [21/50], Step [70/777], Loss: 0.2332\n",
      "Epoch [21/50], Step [80/777], Loss: 0.1806\n",
      "Epoch [21/50], Step [90/777], Loss: 0.0618\n",
      "Epoch [21/50], Step [100/777], Loss: 0.0293\n",
      "Epoch [21/50], Step [110/777], Loss: 0.2975\n",
      "Epoch [21/50], Step [120/777], Loss: 0.0072\n",
      "Epoch [21/50], Step [130/777], Loss: 0.0214\n",
      "Epoch [21/50], Step [140/777], Loss: 0.0380\n",
      "Epoch [21/50], Step [150/777], Loss: 0.0466\n",
      "Epoch [21/50], Step [160/777], Loss: 0.0218\n",
      "Epoch [21/50], Step [170/777], Loss: 0.1712\n",
      "Epoch [21/50], Step [180/777], Loss: 0.0213\n",
      "Epoch [21/50], Step [190/777], Loss: 0.2109\n",
      "Epoch [21/50], Step [200/777], Loss: 0.4573\n",
      "Epoch [21/50], Step [210/777], Loss: 0.2657\n",
      "Epoch [21/50], Step [220/777], Loss: 0.0646\n",
      "Epoch [21/50], Step [230/777], Loss: 0.2044\n",
      "Epoch [21/50], Step [240/777], Loss: 0.0346\n",
      "Epoch [21/50], Step [250/777], Loss: 0.0292\n",
      "Epoch [21/50], Step [260/777], Loss: 0.0645\n",
      "Epoch [21/50], Step [270/777], Loss: 0.1811\n",
      "Epoch [21/50], Step [280/777], Loss: 0.1128\n",
      "Epoch [21/50], Step [290/777], Loss: 0.2555\n",
      "Epoch [21/50], Step [300/777], Loss: 0.0442\n",
      "Epoch [21/50], Step [310/777], Loss: 0.0429\n",
      "Epoch [21/50], Step [320/777], Loss: 0.0155\n",
      "Epoch [21/50], Step [330/777], Loss: 0.1808\n",
      "Epoch [21/50], Step [340/777], Loss: 0.3866\n",
      "Epoch [21/50], Step [350/777], Loss: 0.0148\n",
      "Epoch [21/50], Step [360/777], Loss: 0.4681\n",
      "Epoch [21/50], Step [370/777], Loss: 0.2068\n",
      "Epoch [21/50], Step [380/777], Loss: 0.0190\n",
      "Epoch [21/50], Step [390/777], Loss: 0.0687\n",
      "Epoch [21/50], Step [400/777], Loss: 0.0456\n",
      "Epoch [21/50], Step [410/777], Loss: 0.0567\n",
      "Epoch [21/50], Step [420/777], Loss: 0.0343\n",
      "Epoch [21/50], Step [430/777], Loss: 0.0193\n",
      "Epoch [21/50], Step [440/777], Loss: 0.0305\n",
      "Epoch [21/50], Step [450/777], Loss: 0.1030\n",
      "Epoch [21/50], Step [460/777], Loss: 0.0362\n",
      "Epoch [21/50], Step [470/777], Loss: 0.3199\n",
      "Epoch [21/50], Step [480/777], Loss: 0.8037\n",
      "Epoch [21/50], Step [490/777], Loss: 0.0526\n",
      "Epoch [21/50], Step [500/777], Loss: 0.2817\n",
      "Epoch [21/50], Step [510/777], Loss: 0.0361\n",
      "Epoch [21/50], Step [520/777], Loss: 0.0441\n",
      "Epoch [21/50], Step [530/777], Loss: 0.0345\n",
      "Epoch [21/50], Step [540/777], Loss: 0.0215\n",
      "Epoch [21/50], Step [550/777], Loss: 0.1333\n",
      "Epoch [21/50], Step [560/777], Loss: 0.0514\n",
      "Epoch [21/50], Step [570/777], Loss: 0.0412\n",
      "Epoch [21/50], Step [580/777], Loss: 0.0472\n",
      "Epoch [21/50], Step [590/777], Loss: 0.2582\n",
      "Epoch [21/50], Step [600/777], Loss: 0.3375\n",
      "Epoch [21/50], Step [610/777], Loss: 0.0470\n",
      "Epoch [21/50], Step [620/777], Loss: 0.0343\n",
      "Epoch [21/50], Step [630/777], Loss: 0.0185\n",
      "Epoch [21/50], Step [640/777], Loss: 0.3381\n",
      "Epoch [21/50], Step [650/777], Loss: 0.0966\n",
      "Epoch [21/50], Step [660/777], Loss: 0.2567\n",
      "Epoch [21/50], Step [670/777], Loss: 0.2058\n",
      "Epoch [21/50], Step [680/777], Loss: 0.0556\n",
      "Epoch [21/50], Step [690/777], Loss: 0.0091\n",
      "Epoch [21/50], Step [700/777], Loss: 0.1870\n",
      "Epoch [21/50], Step [710/777], Loss: 0.1799\n",
      "Epoch [21/50], Step [720/777], Loss: 0.0770\n",
      "Epoch [21/50], Step [730/777], Loss: 0.0429\n",
      "Epoch [21/50], Step [740/777], Loss: 0.0001\n",
      "Epoch [21/50], Step [750/777], Loss: 0.3010\n",
      "Epoch [21/50], Step [760/777], Loss: 0.0365\n",
      "Epoch [21/50], Step [770/777], Loss: 0.0068\n",
      "Epoch [21/50], Train Loss: 0.1275, Val Loss: 0.2255, Val Accuracy: 0.9440\n",
      "Early stopping triggered after 21 epochs\n",
      "\n",
      "Evaluating SqueezeNet on test set...\n",
      "\n",
      "SqueezeNet Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DJI       0.95      0.84      0.89       200\n",
      "   FutabaT14       0.96      0.87      0.91       548\n",
      "    FutabaT7       0.94      0.89      0.92        93\n",
      "    Graupner       0.99      0.96      0.98       107\n",
      "       Noise       0.92      0.98      0.95      1314\n",
      "     Taranis       1.00      0.99      0.99       268\n",
      "     Turnigy       0.96      0.96      0.96       133\n",
      "\n",
      "    accuracy                           0.94      2663\n",
      "   macro avg       0.96      0.93      0.94      2663\n",
      "weighted avg       0.95      0.94      0.94      2663\n",
      "\n",
      "\n",
      "SqueezeNet Multi-class ROC AUC Score: 0.9891\n",
      "\n",
      "Analyzing SqueezeNet performance by SNR levels...\n",
      "\n",
      "SqueezeNet Performance by SNR level:\n",
      "SNR (dB) | Accuracy | Samples\n",
      "------------------------------\n",
      "\n",
      "SqueezeNet Total Number of Parameters: 726,087\n",
      "SqueezeNet Average Inference Time per Sample: 2.244 ms\n",
      "SqueezeNet FLOPs: 263,230,822.0 (263.23 M)\n",
      "SqueezeNet MACs: 726,087.0 (726.09 K)\n",
      "\n",
      "✅ SqueezeNet Accuracy for class 'DJI': 84.00%\n",
      "✅ SqueezeNet Accuracy for class 'FutabaT14': 86.68%\n",
      "✅ SqueezeNet Accuracy for class 'FutabaT7': 89.25%\n",
      "✅ SqueezeNet Accuracy for class 'Graupner': 96.26%\n",
      "✅ SqueezeNet Accuracy for class 'Noise': 98.33%\n",
      "✅ SqueezeNet Accuracy for class 'Taranis': 98.88%\n",
      "✅ SqueezeNet Accuracy for class 'Turnigy': 96.24%\n",
      "\n",
      "✅ SqueezeNet Test Set Accuracy: 0.944\n",
      "📊 SqueezeNet Model Size: 2.77 MB\n",
      "\n",
      "Key characteristics of SqueezeNet:\n",
      "- Extremely lightweight model (around 3MB)\n",
      "- Uses 'fire modules' with squeeze and expand layers\n",
      "- Achieves AlexNet-level accuracy with 50x fewer parameters\n",
      "- Ideal for resource-constrained environments like drones\n",
      "\n",
      "Metrics saved to squeezenet_drone_rf_metrics.json\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING SHUFFLENET\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1`. You can also use `weights=ShuffleNet_V2_X1_0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth\" to /root/.cache/torch/hub/checkpoints/shufflenetv2_x1-5666bf0f80.pth\n",
      "100%|██████████| 8.79M/8.79M [00:00<00:00, 142MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training and Evaluating ShuffleNet\n",
      "==================================================\n",
      "\n",
      "ShuffleNet Summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 112, 112]             648\n",
      "       BatchNorm2d-2         [-1, 24, 112, 112]              48\n",
      "              ReLU-3         [-1, 24, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 24, 56, 56]               0\n",
      "            Conv2d-5           [-1, 24, 28, 28]             216\n",
      "       BatchNorm2d-6           [-1, 24, 28, 28]              48\n",
      "            Conv2d-7           [-1, 58, 28, 28]           1,392\n",
      "       BatchNorm2d-8           [-1, 58, 28, 28]             116\n",
      "              ReLU-9           [-1, 58, 28, 28]               0\n",
      "           Conv2d-10           [-1, 58, 56, 56]           1,392\n",
      "      BatchNorm2d-11           [-1, 58, 56, 56]             116\n",
      "             ReLU-12           [-1, 58, 56, 56]               0\n",
      "           Conv2d-13           [-1, 58, 28, 28]             522\n",
      "      BatchNorm2d-14           [-1, 58, 28, 28]             116\n",
      "           Conv2d-15           [-1, 58, 28, 28]           3,364\n",
      "      BatchNorm2d-16           [-1, 58, 28, 28]             116\n",
      "             ReLU-17           [-1, 58, 28, 28]               0\n",
      " InvertedResidual-18          [-1, 116, 28, 28]               0\n",
      "           Conv2d-19           [-1, 58, 28, 28]           3,364\n",
      "      BatchNorm2d-20           [-1, 58, 28, 28]             116\n",
      "             ReLU-21           [-1, 58, 28, 28]               0\n",
      "           Conv2d-22           [-1, 58, 28, 28]             522\n",
      "      BatchNorm2d-23           [-1, 58, 28, 28]             116\n",
      "           Conv2d-24           [-1, 58, 28, 28]           3,364\n",
      "      BatchNorm2d-25           [-1, 58, 28, 28]             116\n",
      "             ReLU-26           [-1, 58, 28, 28]               0\n",
      " InvertedResidual-27          [-1, 116, 28, 28]               0\n",
      "           Conv2d-28           [-1, 58, 28, 28]           3,364\n",
      "      BatchNorm2d-29           [-1, 58, 28, 28]             116\n",
      "             ReLU-30           [-1, 58, 28, 28]               0\n",
      "           Conv2d-31           [-1, 58, 28, 28]             522\n",
      "      BatchNorm2d-32           [-1, 58, 28, 28]             116\n",
      "           Conv2d-33           [-1, 58, 28, 28]           3,364\n",
      "      BatchNorm2d-34           [-1, 58, 28, 28]             116\n",
      "             ReLU-35           [-1, 58, 28, 28]               0\n",
      " InvertedResidual-36          [-1, 116, 28, 28]               0\n",
      "           Conv2d-37           [-1, 58, 28, 28]           3,364\n",
      "      BatchNorm2d-38           [-1, 58, 28, 28]             116\n",
      "             ReLU-39           [-1, 58, 28, 28]               0\n",
      "           Conv2d-40           [-1, 58, 28, 28]             522\n",
      "      BatchNorm2d-41           [-1, 58, 28, 28]             116\n",
      "           Conv2d-42           [-1, 58, 28, 28]           3,364\n",
      "      BatchNorm2d-43           [-1, 58, 28, 28]             116\n",
      "             ReLU-44           [-1, 58, 28, 28]               0\n",
      " InvertedResidual-45          [-1, 116, 28, 28]               0\n",
      "           Conv2d-46          [-1, 116, 14, 14]           1,044\n",
      "      BatchNorm2d-47          [-1, 116, 14, 14]             232\n",
      "           Conv2d-48          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-49          [-1, 116, 14, 14]             232\n",
      "             ReLU-50          [-1, 116, 14, 14]               0\n",
      "           Conv2d-51          [-1, 116, 28, 28]          13,456\n",
      "      BatchNorm2d-52          [-1, 116, 28, 28]             232\n",
      "             ReLU-53          [-1, 116, 28, 28]               0\n",
      "           Conv2d-54          [-1, 116, 14, 14]           1,044\n",
      "      BatchNorm2d-55          [-1, 116, 14, 14]             232\n",
      "           Conv2d-56          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-57          [-1, 116, 14, 14]             232\n",
      "             ReLU-58          [-1, 116, 14, 14]               0\n",
      " InvertedResidual-59          [-1, 232, 14, 14]               0\n",
      "           Conv2d-60          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-61          [-1, 116, 14, 14]             232\n",
      "             ReLU-62          [-1, 116, 14, 14]               0\n",
      "           Conv2d-63          [-1, 116, 14, 14]           1,044\n",
      "      BatchNorm2d-64          [-1, 116, 14, 14]             232\n",
      "           Conv2d-65          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-66          [-1, 116, 14, 14]             232\n",
      "             ReLU-67          [-1, 116, 14, 14]               0\n",
      " InvertedResidual-68          [-1, 232, 14, 14]               0\n",
      "           Conv2d-69          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-70          [-1, 116, 14, 14]             232\n",
      "             ReLU-71          [-1, 116, 14, 14]               0\n",
      "           Conv2d-72          [-1, 116, 14, 14]           1,044\n",
      "      BatchNorm2d-73          [-1, 116, 14, 14]             232\n",
      "           Conv2d-74          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-75          [-1, 116, 14, 14]             232\n",
      "             ReLU-76          [-1, 116, 14, 14]               0\n",
      " InvertedResidual-77          [-1, 232, 14, 14]               0\n",
      "           Conv2d-78          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-79          [-1, 116, 14, 14]             232\n",
      "             ReLU-80          [-1, 116, 14, 14]               0\n",
      "           Conv2d-81          [-1, 116, 14, 14]           1,044\n",
      "      BatchNorm2d-82          [-1, 116, 14, 14]             232\n",
      "           Conv2d-83          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-84          [-1, 116, 14, 14]             232\n",
      "             ReLU-85          [-1, 116, 14, 14]               0\n",
      " InvertedResidual-86          [-1, 232, 14, 14]               0\n",
      "           Conv2d-87          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-88          [-1, 116, 14, 14]             232\n",
      "             ReLU-89          [-1, 116, 14, 14]               0\n",
      "           Conv2d-90          [-1, 116, 14, 14]           1,044\n",
      "      BatchNorm2d-91          [-1, 116, 14, 14]             232\n",
      "           Conv2d-92          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-93          [-1, 116, 14, 14]             232\n",
      "             ReLU-94          [-1, 116, 14, 14]               0\n",
      " InvertedResidual-95          [-1, 232, 14, 14]               0\n",
      "           Conv2d-96          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-97          [-1, 116, 14, 14]             232\n",
      "             ReLU-98          [-1, 116, 14, 14]               0\n",
      "           Conv2d-99          [-1, 116, 14, 14]           1,044\n",
      "     BatchNorm2d-100          [-1, 116, 14, 14]             232\n",
      "          Conv2d-101          [-1, 116, 14, 14]          13,456\n",
      "     BatchNorm2d-102          [-1, 116, 14, 14]             232\n",
      "            ReLU-103          [-1, 116, 14, 14]               0\n",
      "InvertedResidual-104          [-1, 232, 14, 14]               0\n",
      "          Conv2d-105          [-1, 116, 14, 14]          13,456\n",
      "     BatchNorm2d-106          [-1, 116, 14, 14]             232\n",
      "            ReLU-107          [-1, 116, 14, 14]               0\n",
      "          Conv2d-108          [-1, 116, 14, 14]           1,044\n",
      "     BatchNorm2d-109          [-1, 116, 14, 14]             232\n",
      "          Conv2d-110          [-1, 116, 14, 14]          13,456\n",
      "     BatchNorm2d-111          [-1, 116, 14, 14]             232\n",
      "            ReLU-112          [-1, 116, 14, 14]               0\n",
      "InvertedResidual-113          [-1, 232, 14, 14]               0\n",
      "          Conv2d-114          [-1, 116, 14, 14]          13,456\n",
      "     BatchNorm2d-115          [-1, 116, 14, 14]             232\n",
      "            ReLU-116          [-1, 116, 14, 14]               0\n",
      "          Conv2d-117          [-1, 116, 14, 14]           1,044\n",
      "     BatchNorm2d-118          [-1, 116, 14, 14]             232\n",
      "          Conv2d-119          [-1, 116, 14, 14]          13,456\n",
      "     BatchNorm2d-120          [-1, 116, 14, 14]             232\n",
      "            ReLU-121          [-1, 116, 14, 14]               0\n",
      "InvertedResidual-122          [-1, 232, 14, 14]               0\n",
      "          Conv2d-123            [-1, 232, 7, 7]           2,088\n",
      "     BatchNorm2d-124            [-1, 232, 7, 7]             464\n",
      "          Conv2d-125            [-1, 232, 7, 7]          53,824\n",
      "     BatchNorm2d-126            [-1, 232, 7, 7]             464\n",
      "            ReLU-127            [-1, 232, 7, 7]               0\n",
      "          Conv2d-128          [-1, 232, 14, 14]          53,824\n",
      "     BatchNorm2d-129          [-1, 232, 14, 14]             464\n",
      "            ReLU-130          [-1, 232, 14, 14]               0\n",
      "          Conv2d-131            [-1, 232, 7, 7]           2,088\n",
      "     BatchNorm2d-132            [-1, 232, 7, 7]             464\n",
      "          Conv2d-133            [-1, 232, 7, 7]          53,824\n",
      "     BatchNorm2d-134            [-1, 232, 7, 7]             464\n",
      "            ReLU-135            [-1, 232, 7, 7]               0\n",
      "InvertedResidual-136            [-1, 464, 7, 7]               0\n",
      "          Conv2d-137            [-1, 232, 7, 7]          53,824\n",
      "     BatchNorm2d-138            [-1, 232, 7, 7]             464\n",
      "            ReLU-139            [-1, 232, 7, 7]               0\n",
      "          Conv2d-140            [-1, 232, 7, 7]           2,088\n",
      "     BatchNorm2d-141            [-1, 232, 7, 7]             464\n",
      "          Conv2d-142            [-1, 232, 7, 7]          53,824\n",
      "     BatchNorm2d-143            [-1, 232, 7, 7]             464\n",
      "            ReLU-144            [-1, 232, 7, 7]               0\n",
      "InvertedResidual-145            [-1, 464, 7, 7]               0\n",
      "          Conv2d-146            [-1, 232, 7, 7]          53,824\n",
      "     BatchNorm2d-147            [-1, 232, 7, 7]             464\n",
      "            ReLU-148            [-1, 232, 7, 7]               0\n",
      "          Conv2d-149            [-1, 232, 7, 7]           2,088\n",
      "     BatchNorm2d-150            [-1, 232, 7, 7]             464\n",
      "          Conv2d-151            [-1, 232, 7, 7]          53,824\n",
      "     BatchNorm2d-152            [-1, 232, 7, 7]             464\n",
      "            ReLU-153            [-1, 232, 7, 7]               0\n",
      "InvertedResidual-154            [-1, 464, 7, 7]               0\n",
      "          Conv2d-155            [-1, 232, 7, 7]          53,824\n",
      "     BatchNorm2d-156            [-1, 232, 7, 7]             464\n",
      "            ReLU-157            [-1, 232, 7, 7]               0\n",
      "          Conv2d-158            [-1, 232, 7, 7]           2,088\n",
      "     BatchNorm2d-159            [-1, 232, 7, 7]             464\n",
      "          Conv2d-160            [-1, 232, 7, 7]          53,824\n",
      "     BatchNorm2d-161            [-1, 232, 7, 7]             464\n",
      "            ReLU-162            [-1, 232, 7, 7]               0\n",
      "InvertedResidual-163            [-1, 464, 7, 7]               0\n",
      "          Conv2d-164           [-1, 1024, 7, 7]         475,136\n",
      "     BatchNorm2d-165           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-166           [-1, 1024, 7, 7]               0\n",
      "          Linear-167                    [-1, 7]           7,175\n",
      "================================================================\n",
      "Total params: 1,260,779\n",
      "Trainable params: 1,260,779\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 47.93\n",
      "Params size (MB): 4.81\n",
      "Estimated Total Size (MB): 53.31\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Starting training ShuffleNet...\n",
      "Epoch [1/50], Step [10/777], Loss: 1.9403\n",
      "Epoch [1/50], Step [20/777], Loss: 1.9166\n",
      "Epoch [1/50], Step [30/777], Loss: 1.9000\n",
      "Epoch [1/50], Step [40/777], Loss: 1.8592\n",
      "Epoch [1/50], Step [50/777], Loss: 1.8171\n",
      "Epoch [1/50], Step [60/777], Loss: 1.7496\n",
      "Epoch [1/50], Step [70/777], Loss: 1.7226\n",
      "Epoch [1/50], Step [80/777], Loss: 1.7774\n",
      "Epoch [1/50], Step [90/777], Loss: 1.6434\n",
      "Epoch [1/50], Step [100/777], Loss: 1.6136\n",
      "Epoch [1/50], Step [110/777], Loss: 1.6669\n",
      "Epoch [1/50], Step [120/777], Loss: 1.6388\n",
      "Epoch [1/50], Step [130/777], Loss: 1.5846\n",
      "Epoch [1/50], Step [140/777], Loss: 1.4437\n",
      "Epoch [1/50], Step [150/777], Loss: 1.4658\n",
      "Epoch [1/50], Step [160/777], Loss: 1.6400\n",
      "Epoch [1/50], Step [170/777], Loss: 1.4910\n",
      "Epoch [1/50], Step [180/777], Loss: 1.3711\n",
      "Epoch [1/50], Step [190/777], Loss: 1.3121\n",
      "Epoch [1/50], Step [200/777], Loss: 1.5223\n",
      "Epoch [1/50], Step [210/777], Loss: 1.3568\n",
      "Epoch [1/50], Step [220/777], Loss: 1.5101\n",
      "Epoch [1/50], Step [230/777], Loss: 1.4833\n",
      "Epoch [1/50], Step [240/777], Loss: 1.3578\n",
      "Epoch [1/50], Step [250/777], Loss: 1.1738\n",
      "Epoch [1/50], Step [260/777], Loss: 1.5413\n",
      "Epoch [1/50], Step [270/777], Loss: 1.2561\n",
      "Epoch [1/50], Step [280/777], Loss: 1.0236\n",
      "Epoch [1/50], Step [290/777], Loss: 1.6056\n",
      "Epoch [1/50], Step [300/777], Loss: 1.1671\n",
      "Epoch [1/50], Step [310/777], Loss: 1.5687\n",
      "Epoch [1/50], Step [320/777], Loss: 1.0040\n",
      "Epoch [1/50], Step [330/777], Loss: 1.7228\n",
      "Epoch [1/50], Step [340/777], Loss: 1.1372\n",
      "Epoch [1/50], Step [350/777], Loss: 1.0215\n",
      "Epoch [1/50], Step [360/777], Loss: 0.9438\n",
      "Epoch [1/50], Step [370/777], Loss: 0.9222\n",
      "Epoch [1/50], Step [380/777], Loss: 1.3938\n",
      "Epoch [1/50], Step [390/777], Loss: 0.9963\n",
      "Epoch [1/50], Step [400/777], Loss: 0.7802\n",
      "Epoch [1/50], Step [410/777], Loss: 1.0779\n",
      "Epoch [1/50], Step [420/777], Loss: 1.1211\n",
      "Epoch [1/50], Step [430/777], Loss: 0.9000\n",
      "Epoch [1/50], Step [440/777], Loss: 1.0520\n",
      "Epoch [1/50], Step [450/777], Loss: 0.7805\n",
      "Epoch [1/50], Step [460/777], Loss: 1.5245\n",
      "Epoch [1/50], Step [470/777], Loss: 0.9491\n",
      "Epoch [1/50], Step [480/777], Loss: 1.1710\n",
      "Epoch [1/50], Step [490/777], Loss: 1.1722\n",
      "Epoch [1/50], Step [500/777], Loss: 0.7407\n",
      "Epoch [1/50], Step [510/777], Loss: 1.0876\n",
      "Epoch [1/50], Step [520/777], Loss: 0.9945\n",
      "Epoch [1/50], Step [530/777], Loss: 0.9519\n",
      "Epoch [1/50], Step [540/777], Loss: 0.8428\n",
      "Epoch [1/50], Step [550/777], Loss: 0.8121\n",
      "Epoch [1/50], Step [560/777], Loss: 0.8628\n",
      "Epoch [1/50], Step [570/777], Loss: 0.7764\n",
      "Epoch [1/50], Step [580/777], Loss: 1.2158\n",
      "Epoch [1/50], Step [590/777], Loss: 0.7167\n",
      "Epoch [1/50], Step [600/777], Loss: 1.1267\n",
      "Epoch [1/50], Step [610/777], Loss: 0.6425\n",
      "Epoch [1/50], Step [620/777], Loss: 0.8017\n",
      "Epoch [1/50], Step [630/777], Loss: 0.7149\n",
      "Epoch [1/50], Step [640/777], Loss: 0.7946\n",
      "Epoch [1/50], Step [650/777], Loss: 0.6547\n",
      "Epoch [1/50], Step [660/777], Loss: 0.7914\n",
      "Epoch [1/50], Step [670/777], Loss: 0.8175\n",
      "Epoch [1/50], Step [680/777], Loss: 0.9236\n",
      "Epoch [1/50], Step [690/777], Loss: 1.0416\n",
      "Epoch [1/50], Step [700/777], Loss: 0.5732\n",
      "Epoch [1/50], Step [710/777], Loss: 0.8251\n",
      "Epoch [1/50], Step [720/777], Loss: 0.6549\n",
      "Epoch [1/50], Step [730/777], Loss: 1.3576\n",
      "Epoch [1/50], Step [740/777], Loss: 0.9621\n",
      "Epoch [1/50], Step [750/777], Loss: 0.9196\n",
      "Epoch [1/50], Step [760/777], Loss: 0.5778\n",
      "Epoch [1/50], Step [770/777], Loss: 0.6257\n",
      "Epoch [1/50], Train Loss: 1.2312, Val Loss: 0.7351, Val Accuracy: 0.7689\n",
      "Model saved with validation accuracy: 0.7689\n",
      "Epoch [2/50], Step [10/777], Loss: 0.7904\n",
      "Epoch [2/50], Step [20/777], Loss: 0.6373\n",
      "Epoch [2/50], Step [30/777], Loss: 0.6016\n",
      "Epoch [2/50], Step [40/777], Loss: 0.7678\n",
      "Epoch [2/50], Step [50/777], Loss: 0.7736\n",
      "Epoch [2/50], Step [60/777], Loss: 1.2188\n",
      "Epoch [2/50], Step [70/777], Loss: 0.5700\n",
      "Epoch [2/50], Step [80/777], Loss: 0.8024\n",
      "Epoch [2/50], Step [90/777], Loss: 0.9969\n",
      "Epoch [2/50], Step [100/777], Loss: 0.6216\n",
      "Epoch [2/50], Step [110/777], Loss: 0.9235\n",
      "Epoch [2/50], Step [120/777], Loss: 0.4997\n",
      "Epoch [2/50], Step [130/777], Loss: 0.8511\n",
      "Epoch [2/50], Step [140/777], Loss: 0.6726\n",
      "Epoch [2/50], Step [150/777], Loss: 0.6281\n",
      "Epoch [2/50], Step [160/777], Loss: 0.7468\n",
      "Epoch [2/50], Step [170/777], Loss: 0.9066\n",
      "Epoch [2/50], Step [180/777], Loss: 0.5132\n",
      "Epoch [2/50], Step [190/777], Loss: 0.6808\n",
      "Epoch [2/50], Step [200/777], Loss: 0.5633\n",
      "Epoch [2/50], Step [210/777], Loss: 1.0028\n",
      "Epoch [2/50], Step [220/777], Loss: 0.7710\n",
      "Epoch [2/50], Step [230/777], Loss: 0.3754\n",
      "Epoch [2/50], Step [240/777], Loss: 0.5476\n",
      "Epoch [2/50], Step [250/777], Loss: 0.6985\n",
      "Epoch [2/50], Step [260/777], Loss: 0.5233\n",
      "Epoch [2/50], Step [270/777], Loss: 0.6622\n",
      "Epoch [2/50], Step [280/777], Loss: 0.8223\n",
      "Epoch [2/50], Step [290/777], Loss: 0.6882\n",
      "Epoch [2/50], Step [300/777], Loss: 0.8206\n",
      "Epoch [2/50], Step [310/777], Loss: 0.7875\n",
      "Epoch [2/50], Step [320/777], Loss: 0.3946\n",
      "Epoch [2/50], Step [330/777], Loss: 0.5231\n",
      "Epoch [2/50], Step [340/777], Loss: 0.6116\n",
      "Epoch [2/50], Step [350/777], Loss: 0.3955\n",
      "Epoch [2/50], Step [360/777], Loss: 0.8382\n",
      "Epoch [2/50], Step [370/777], Loss: 0.5029\n",
      "Epoch [2/50], Step [380/777], Loss: 0.9081\n",
      "Epoch [2/50], Step [390/777], Loss: 1.1342\n",
      "Epoch [2/50], Step [400/777], Loss: 0.4250\n",
      "Epoch [2/50], Step [410/777], Loss: 0.5076\n",
      "Epoch [2/50], Step [420/777], Loss: 1.3592\n",
      "Epoch [2/50], Step [430/777], Loss: 0.7311\n",
      "Epoch [2/50], Step [440/777], Loss: 0.5652\n",
      "Epoch [2/50], Step [450/777], Loss: 1.2257\n",
      "Epoch [2/50], Step [460/777], Loss: 0.4060\n",
      "Epoch [2/50], Step [470/777], Loss: 0.5207\n",
      "Epoch [2/50], Step [480/777], Loss: 0.6793\n",
      "Epoch [2/50], Step [490/777], Loss: 0.3737\n",
      "Epoch [2/50], Step [500/777], Loss: 0.8847\n",
      "Epoch [2/50], Step [510/777], Loss: 0.6911\n",
      "Epoch [2/50], Step [520/777], Loss: 0.9725\n",
      "Epoch [2/50], Step [530/777], Loss: 0.7841\n",
      "Epoch [2/50], Step [540/777], Loss: 0.9127\n",
      "Epoch [2/50], Step [550/777], Loss: 0.7029\n",
      "Epoch [2/50], Step [560/777], Loss: 0.3497\n",
      "Epoch [2/50], Step [570/777], Loss: 0.7025\n",
      "Epoch [2/50], Step [580/777], Loss: 0.6114\n",
      "Epoch [2/50], Step [590/777], Loss: 0.4577\n",
      "Epoch [2/50], Step [600/777], Loss: 0.6943\n",
      "Epoch [2/50], Step [610/777], Loss: 0.4356\n",
      "Epoch [2/50], Step [620/777], Loss: 0.5740\n",
      "Epoch [2/50], Step [630/777], Loss: 0.5692\n",
      "Epoch [2/50], Step [640/777], Loss: 0.4648\n",
      "Epoch [2/50], Step [650/777], Loss: 0.1752\n",
      "Epoch [2/50], Step [660/777], Loss: 0.4839\n",
      "Epoch [2/50], Step [670/777], Loss: 0.5149\n",
      "Epoch [2/50], Step [680/777], Loss: 1.1046\n",
      "Epoch [2/50], Step [690/777], Loss: 0.5860\n",
      "Epoch [2/50], Step [700/777], Loss: 0.3214\n",
      "Epoch [2/50], Step [710/777], Loss: 0.2965\n",
      "Epoch [2/50], Step [720/777], Loss: 0.3997\n",
      "Epoch [2/50], Step [730/777], Loss: 0.8783\n",
      "Epoch [2/50], Step [740/777], Loss: 0.5505\n",
      "Epoch [2/50], Step [750/777], Loss: 0.2723\n",
      "Epoch [2/50], Step [760/777], Loss: 0.4525\n",
      "Epoch [2/50], Step [770/777], Loss: 0.7863\n",
      "Epoch [2/50], Train Loss: 0.6813, Val Loss: 0.4391, Val Accuracy: 0.8921\n",
      "Model saved with validation accuracy: 0.8921\n",
      "Epoch [3/50], Step [10/777], Loss: 0.3581\n",
      "Epoch [3/50], Step [20/777], Loss: 0.3968\n",
      "Epoch [3/50], Step [30/777], Loss: 0.3607\n",
      "Epoch [3/50], Step [40/777], Loss: 0.6845\n",
      "Epoch [3/50], Step [50/777], Loss: 0.3422\n",
      "Epoch [3/50], Step [60/777], Loss: 0.2742\n",
      "Epoch [3/50], Step [70/777], Loss: 0.2511\n",
      "Epoch [3/50], Step [80/777], Loss: 0.2934\n",
      "Epoch [3/50], Step [90/777], Loss: 0.2253\n",
      "Epoch [3/50], Step [100/777], Loss: 0.7704\n",
      "Epoch [3/50], Step [110/777], Loss: 0.2862\n",
      "Epoch [3/50], Step [120/777], Loss: 0.6369\n",
      "Epoch [3/50], Step [130/777], Loss: 0.3112\n",
      "Epoch [3/50], Step [140/777], Loss: 0.6105\n",
      "Epoch [3/50], Step [150/777], Loss: 0.4930\n",
      "Epoch [3/50], Step [160/777], Loss: 0.3020\n",
      "Epoch [3/50], Step [170/777], Loss: 0.5857\n",
      "Epoch [3/50], Step [180/777], Loss: 0.6149\n",
      "Epoch [3/50], Step [190/777], Loss: 0.5356\n",
      "Epoch [3/50], Step [200/777], Loss: 0.5346\n",
      "Epoch [3/50], Step [210/777], Loss: 0.6405\n",
      "Epoch [3/50], Step [220/777], Loss: 0.2005\n",
      "Epoch [3/50], Step [230/777], Loss: 0.7087\n",
      "Epoch [3/50], Step [240/777], Loss: 0.1513\n",
      "Epoch [3/50], Step [250/777], Loss: 0.2406\n",
      "Epoch [3/50], Step [260/777], Loss: 0.4234\n",
      "Epoch [3/50], Step [270/777], Loss: 0.4203\n",
      "Epoch [3/50], Step [280/777], Loss: 0.3412\n",
      "Epoch [3/50], Step [290/777], Loss: 0.4418\n",
      "Epoch [3/50], Step [300/777], Loss: 0.3422\n",
      "Epoch [3/50], Step [310/777], Loss: 0.5484\n",
      "Epoch [3/50], Step [320/777], Loss: 0.1805\n",
      "Epoch [3/50], Step [330/777], Loss: 0.5032\n",
      "Epoch [3/50], Step [340/777], Loss: 0.5207\n",
      "Epoch [3/50], Step [350/777], Loss: 0.1415\n",
      "Epoch [3/50], Step [360/777], Loss: 0.9645\n",
      "Epoch [3/50], Step [370/777], Loss: 0.5302\n",
      "Epoch [3/50], Step [380/777], Loss: 0.3940\n",
      "Epoch [3/50], Step [390/777], Loss: 0.8249\n",
      "Epoch [3/50], Step [400/777], Loss: 0.1234\n",
      "Epoch [3/50], Step [410/777], Loss: 0.4861\n",
      "Epoch [3/50], Step [420/777], Loss: 0.3584\n",
      "Epoch [3/50], Step [430/777], Loss: 0.4002\n",
      "Epoch [3/50], Step [440/777], Loss: 0.1916\n",
      "Epoch [3/50], Step [450/777], Loss: 0.3075\n",
      "Epoch [3/50], Step [460/777], Loss: 0.3522\n",
      "Epoch [3/50], Step [470/777], Loss: 0.6703\n",
      "Epoch [3/50], Step [480/777], Loss: 0.6808\n",
      "Epoch [3/50], Step [490/777], Loss: 0.4721\n",
      "Epoch [3/50], Step [500/777], Loss: 0.8197\n",
      "Epoch [3/50], Step [510/777], Loss: 0.1908\n",
      "Epoch [3/50], Step [520/777], Loss: 0.4260\n",
      "Epoch [3/50], Step [530/777], Loss: 0.2782\n",
      "Epoch [3/50], Step [540/777], Loss: 0.5592\n",
      "Epoch [3/50], Step [550/777], Loss: 0.6120\n",
      "Epoch [3/50], Step [560/777], Loss: 0.4539\n",
      "Epoch [3/50], Step [570/777], Loss: 0.2003\n",
      "Epoch [3/50], Step [580/777], Loss: 0.1341\n",
      "Epoch [3/50], Step [590/777], Loss: 0.1336\n",
      "Epoch [3/50], Step [600/777], Loss: 0.2736\n",
      "Epoch [3/50], Step [610/777], Loss: 0.8863\n",
      "Epoch [3/50], Step [620/777], Loss: 0.2163\n",
      "Epoch [3/50], Step [630/777], Loss: 0.3139\n",
      "Epoch [3/50], Step [640/777], Loss: 0.5061\n",
      "Epoch [3/50], Step [650/777], Loss: 0.2380\n",
      "Epoch [3/50], Step [660/777], Loss: 0.4900\n",
      "Epoch [3/50], Step [670/777], Loss: 0.3448\n",
      "Epoch [3/50], Step [680/777], Loss: 0.1511\n",
      "Epoch [3/50], Step [690/777], Loss: 0.2797\n",
      "Epoch [3/50], Step [700/777], Loss: 0.5054\n",
      "Epoch [3/50], Step [710/777], Loss: 0.4803\n",
      "Epoch [3/50], Step [720/777], Loss: 0.3152\n",
      "Epoch [3/50], Step [730/777], Loss: 0.5256\n",
      "Epoch [3/50], Step [740/777], Loss: 0.8464\n",
      "Epoch [3/50], Step [750/777], Loss: 0.2422\n",
      "Epoch [3/50], Step [760/777], Loss: 0.2609\n",
      "Epoch [3/50], Step [770/777], Loss: 0.4142\n",
      "Epoch [3/50], Train Loss: 0.4229, Val Loss: 0.3130, Val Accuracy: 0.9173\n",
      "Model saved with validation accuracy: 0.9173\n",
      "Epoch [4/50], Step [10/777], Loss: 0.1965\n",
      "Epoch [4/50], Step [20/777], Loss: 0.0811\n",
      "Epoch [4/50], Step [30/777], Loss: 0.2014\n",
      "Epoch [4/50], Step [40/777], Loss: 0.1211\n",
      "Epoch [4/50], Step [50/777], Loss: 0.1584\n",
      "Epoch [4/50], Step [60/777], Loss: 0.2662\n",
      "Epoch [4/50], Step [70/777], Loss: 0.3618\n",
      "Epoch [4/50], Step [80/777], Loss: 0.6689\n",
      "Epoch [4/50], Step [90/777], Loss: 0.1513\n",
      "Epoch [4/50], Step [100/777], Loss: 0.3277\n",
      "Epoch [4/50], Step [110/777], Loss: 0.2399\n",
      "Epoch [4/50], Step [120/777], Loss: 0.2438\n",
      "Epoch [4/50], Step [130/777], Loss: 0.4690\n",
      "Epoch [4/50], Step [140/777], Loss: 0.2088\n",
      "Epoch [4/50], Step [150/777], Loss: 0.2275\n",
      "Epoch [4/50], Step [160/777], Loss: 0.3331\n",
      "Epoch [4/50], Step [170/777], Loss: 0.1043\n",
      "Epoch [4/50], Step [180/777], Loss: 0.2439\n",
      "Epoch [4/50], Step [190/777], Loss: 0.6224\n",
      "Epoch [4/50], Step [200/777], Loss: 0.2446\n",
      "Epoch [4/50], Step [210/777], Loss: 0.2364\n",
      "Epoch [4/50], Step [220/777], Loss: 0.2070\n",
      "Epoch [4/50], Step [230/777], Loss: 0.8206\n",
      "Epoch [4/50], Step [240/777], Loss: 0.5290\n",
      "Epoch [4/50], Step [250/777], Loss: 0.9191\n",
      "Epoch [4/50], Step [260/777], Loss: 0.5391\n",
      "Epoch [4/50], Step [270/777], Loss: 0.3330\n",
      "Epoch [4/50], Step [280/777], Loss: 0.1153\n",
      "Epoch [4/50], Step [290/777], Loss: 0.2011\n",
      "Epoch [4/50], Step [300/777], Loss: 0.2673\n",
      "Epoch [4/50], Step [310/777], Loss: 0.2471\n",
      "Epoch [4/50], Step [320/777], Loss: 0.2284\n",
      "Epoch [4/50], Step [330/777], Loss: 0.1146\n",
      "Epoch [4/50], Step [340/777], Loss: 0.4664\n",
      "Epoch [4/50], Step [350/777], Loss: 0.2099\n",
      "Epoch [4/50], Step [360/777], Loss: 0.4403\n",
      "Epoch [4/50], Step [370/777], Loss: 0.1015\n",
      "Epoch [4/50], Step [380/777], Loss: 0.3556\n",
      "Epoch [4/50], Step [390/777], Loss: 0.4349\n",
      "Epoch [4/50], Step [400/777], Loss: 0.4773\n",
      "Epoch [4/50], Step [410/777], Loss: 0.5412\n",
      "Epoch [4/50], Step [420/777], Loss: 0.3423\n",
      "Epoch [4/50], Step [430/777], Loss: 0.1184\n",
      "Epoch [4/50], Step [440/777], Loss: 0.0689\n",
      "Epoch [4/50], Step [450/777], Loss: 0.2415\n",
      "Epoch [4/50], Step [460/777], Loss: 0.0796\n",
      "Epoch [4/50], Step [470/777], Loss: 0.7657\n",
      "Epoch [4/50], Step [480/777], Loss: 0.3100\n",
      "Epoch [4/50], Step [490/777], Loss: 0.3448\n",
      "Epoch [4/50], Step [500/777], Loss: 0.1431\n",
      "Epoch [4/50], Step [510/777], Loss: 0.4746\n",
      "Epoch [4/50], Step [520/777], Loss: 0.1717\n",
      "Epoch [4/50], Step [530/777], Loss: 0.5393\n",
      "Epoch [4/50], Step [540/777], Loss: 0.3776\n",
      "Epoch [4/50], Step [550/777], Loss: 0.1305\n",
      "Epoch [4/50], Step [560/777], Loss: 0.1534\n",
      "Epoch [4/50], Step [570/777], Loss: 0.2036\n",
      "Epoch [4/50], Step [580/777], Loss: 0.1967\n",
      "Epoch [4/50], Step [590/777], Loss: 0.1513\n",
      "Epoch [4/50], Step [600/777], Loss: 0.1586\n",
      "Epoch [4/50], Step [610/777], Loss: 0.0688\n",
      "Epoch [4/50], Step [620/777], Loss: 0.3372\n",
      "Epoch [4/50], Step [630/777], Loss: 0.4811\n",
      "Epoch [4/50], Step [640/777], Loss: 0.4722\n",
      "Epoch [4/50], Step [650/777], Loss: 0.2653\n",
      "Epoch [4/50], Step [660/777], Loss: 0.3657\n",
      "Epoch [4/50], Step [670/777], Loss: 0.1258\n",
      "Epoch [4/50], Step [680/777], Loss: 0.4275\n",
      "Epoch [4/50], Step [690/777], Loss: 0.2341\n",
      "Epoch [4/50], Step [700/777], Loss: 0.2328\n",
      "Epoch [4/50], Step [710/777], Loss: 0.1791\n",
      "Epoch [4/50], Step [720/777], Loss: 0.1312\n",
      "Epoch [4/50], Step [730/777], Loss: 0.3752\n",
      "Epoch [4/50], Step [740/777], Loss: 0.5974\n",
      "Epoch [4/50], Step [750/777], Loss: 0.4030\n",
      "Epoch [4/50], Step [760/777], Loss: 0.0747\n",
      "Epoch [4/50], Step [770/777], Loss: 0.1541\n",
      "Epoch [4/50], Train Loss: 0.3130, Val Loss: 0.2736, Val Accuracy: 0.9282\n",
      "Model saved with validation accuracy: 0.9282\n",
      "Epoch [5/50], Step [10/777], Loss: 0.6170\n",
      "Epoch [5/50], Step [20/777], Loss: 0.1482\n",
      "Epoch [5/50], Step [30/777], Loss: 0.2530\n",
      "Epoch [5/50], Step [40/777], Loss: 0.1595\n",
      "Epoch [5/50], Step [50/777], Loss: 0.0967\n",
      "Epoch [5/50], Step [60/777], Loss: 0.4566\n",
      "Epoch [5/50], Step [70/777], Loss: 0.2736\n",
      "Epoch [5/50], Step [80/777], Loss: 0.2348\n",
      "Epoch [5/50], Step [90/777], Loss: 0.0956\n",
      "Epoch [5/50], Step [100/777], Loss: 0.3071\n",
      "Epoch [5/50], Step [110/777], Loss: 0.2808\n",
      "Epoch [5/50], Step [120/777], Loss: 0.2071\n",
      "Epoch [5/50], Step [130/777], Loss: 0.3666\n",
      "Epoch [5/50], Step [140/777], Loss: 0.2995\n",
      "Epoch [5/50], Step [150/777], Loss: 0.0938\n",
      "Epoch [5/50], Step [160/777], Loss: 0.1665\n",
      "Epoch [5/50], Step [170/777], Loss: 0.2275\n",
      "Epoch [5/50], Step [180/777], Loss: 0.5064\n",
      "Epoch [5/50], Step [190/777], Loss: 0.1923\n",
      "Epoch [5/50], Step [200/777], Loss: 0.2363\n",
      "Epoch [5/50], Step [210/777], Loss: 0.1724\n",
      "Epoch [5/50], Step [220/777], Loss: 0.4869\n",
      "Epoch [5/50], Step [230/777], Loss: 0.2572\n",
      "Epoch [5/50], Step [240/777], Loss: 0.1396\n",
      "Epoch [5/50], Step [250/777], Loss: 0.1323\n",
      "Epoch [5/50], Step [260/777], Loss: 0.2827\n",
      "Epoch [5/50], Step [270/777], Loss: 0.4160\n",
      "Epoch [5/50], Step [280/777], Loss: 0.0491\n",
      "Epoch [5/50], Step [290/777], Loss: 0.1162\n",
      "Epoch [5/50], Step [300/777], Loss: 0.1844\n",
      "Epoch [5/50], Step [310/777], Loss: 0.1002\n",
      "Epoch [5/50], Step [320/777], Loss: 0.3167\n",
      "Epoch [5/50], Step [330/777], Loss: 0.5295\n",
      "Epoch [5/50], Step [340/777], Loss: 0.2298\n",
      "Epoch [5/50], Step [350/777], Loss: 0.2247\n",
      "Epoch [5/50], Step [360/777], Loss: 0.5715\n",
      "Epoch [5/50], Step [370/777], Loss: 0.0669\n",
      "Epoch [5/50], Step [380/777], Loss: 0.1619\n",
      "Epoch [5/50], Step [390/777], Loss: 0.2146\n",
      "Epoch [5/50], Step [400/777], Loss: 0.2875\n",
      "Epoch [5/50], Step [410/777], Loss: 0.2004\n",
      "Epoch [5/50], Step [420/777], Loss: 0.2787\n",
      "Epoch [5/50], Step [430/777], Loss: 0.2756\n",
      "Epoch [5/50], Step [440/777], Loss: 0.3360\n",
      "Epoch [5/50], Step [450/777], Loss: 0.4952\n",
      "Epoch [5/50], Step [460/777], Loss: 0.0516\n",
      "Epoch [5/50], Step [470/777], Loss: 0.1240\n",
      "Epoch [5/50], Step [480/777], Loss: 0.0662\n",
      "Epoch [5/50], Step [490/777], Loss: 0.4582\n",
      "Epoch [5/50], Step [500/777], Loss: 0.0403\n",
      "Epoch [5/50], Step [510/777], Loss: 0.4827\n",
      "Epoch [5/50], Step [520/777], Loss: 0.0883\n",
      "Epoch [5/50], Step [530/777], Loss: 0.2568\n",
      "Epoch [5/50], Step [540/777], Loss: 0.3872\n",
      "Epoch [5/50], Step [550/777], Loss: 0.0651\n",
      "Epoch [5/50], Step [560/777], Loss: 0.1202\n",
      "Epoch [5/50], Step [570/777], Loss: 0.9117\n",
      "Epoch [5/50], Step [580/777], Loss: 0.3155\n",
      "Epoch [5/50], Step [590/777], Loss: 0.2542\n",
      "Epoch [5/50], Step [600/777], Loss: 0.3513\n",
      "Epoch [5/50], Step [610/777], Loss: 0.0982\n",
      "Epoch [5/50], Step [620/777], Loss: 0.2768\n",
      "Epoch [5/50], Step [630/777], Loss: 0.1232\n",
      "Epoch [5/50], Step [640/777], Loss: 0.0664\n",
      "Epoch [5/50], Step [650/777], Loss: 0.3334\n",
      "Epoch [5/50], Step [660/777], Loss: 0.1228\n",
      "Epoch [5/50], Step [670/777], Loss: 0.3008\n",
      "Epoch [5/50], Step [680/777], Loss: 0.2009\n",
      "Epoch [5/50], Step [690/777], Loss: 0.4183\n",
      "Epoch [5/50], Step [700/777], Loss: 0.1812\n",
      "Epoch [5/50], Step [710/777], Loss: 0.2656\n",
      "Epoch [5/50], Step [720/777], Loss: 0.1805\n",
      "Epoch [5/50], Step [730/777], Loss: 0.4762\n",
      "Epoch [5/50], Step [740/777], Loss: 0.7257\n",
      "Epoch [5/50], Step [750/777], Loss: 0.2873\n",
      "Epoch [5/50], Step [760/777], Loss: 0.2496\n",
      "Epoch [5/50], Step [770/777], Loss: 0.0546\n",
      "Epoch [5/50], Train Loss: 0.2527, Val Loss: 0.2822, Val Accuracy: 0.9309\n",
      "Model saved with validation accuracy: 0.9309\n",
      "Epoch [6/50], Step [10/777], Loss: 0.1519\n",
      "Epoch [6/50], Step [20/777], Loss: 0.2890\n",
      "Epoch [6/50], Step [30/777], Loss: 0.3096\n",
      "Epoch [6/50], Step [40/777], Loss: 0.1198\n",
      "Epoch [6/50], Step [50/777], Loss: 0.1775\n",
      "Epoch [6/50], Step [60/777], Loss: 0.0880\n",
      "Epoch [6/50], Step [70/777], Loss: 0.1380\n",
      "Epoch [6/50], Step [80/777], Loss: 0.4008\n",
      "Epoch [6/50], Step [90/777], Loss: 0.0975\n",
      "Epoch [6/50], Step [100/777], Loss: 0.3004\n",
      "Epoch [6/50], Step [110/777], Loss: 0.3323\n",
      "Epoch [6/50], Step [120/777], Loss: 0.2349\n",
      "Epoch [6/50], Step [130/777], Loss: 0.1101\n",
      "Epoch [6/50], Step [140/777], Loss: 0.0803\n",
      "Epoch [6/50], Step [150/777], Loss: 0.1143\n",
      "Epoch [6/50], Step [160/777], Loss: 0.3171\n",
      "Epoch [6/50], Step [170/777], Loss: 0.6197\n",
      "Epoch [6/50], Step [180/777], Loss: 0.0717\n",
      "Epoch [6/50], Step [190/777], Loss: 0.0899\n",
      "Epoch [6/50], Step [200/777], Loss: 0.0243\n",
      "Epoch [6/50], Step [210/777], Loss: 0.2222\n",
      "Epoch [6/50], Step [220/777], Loss: 0.2508\n",
      "Epoch [6/50], Step [230/777], Loss: 0.0436\n",
      "Epoch [6/50], Step [240/777], Loss: 0.2392\n",
      "Epoch [6/50], Step [250/777], Loss: 0.4698\n",
      "Epoch [6/50], Step [260/777], Loss: 0.1504\n",
      "Epoch [6/50], Step [270/777], Loss: 0.2593\n",
      "Epoch [6/50], Step [280/777], Loss: 0.2565\n",
      "Epoch [6/50], Step [290/777], Loss: 0.0461\n",
      "Epoch [6/50], Step [300/777], Loss: 0.0833\n",
      "Epoch [6/50], Step [310/777], Loss: 0.1336\n",
      "Epoch [6/50], Step [320/777], Loss: 0.2116\n",
      "Epoch [6/50], Step [330/777], Loss: 0.0625\n",
      "Epoch [6/50], Step [340/777], Loss: 0.2040\n",
      "Epoch [6/50], Step [350/777], Loss: 0.2610\n",
      "Epoch [6/50], Step [360/777], Loss: 0.2506\n",
      "Epoch [6/50], Step [370/777], Loss: 0.3459\n",
      "Epoch [6/50], Step [380/777], Loss: 0.0447\n",
      "Epoch [6/50], Step [390/777], Loss: 0.1090\n",
      "Epoch [6/50], Step [400/777], Loss: 0.0371\n",
      "Epoch [6/50], Step [410/777], Loss: 0.0564\n",
      "Epoch [6/50], Step [420/777], Loss: 0.1286\n",
      "Epoch [6/50], Step [430/777], Loss: 0.4978\n",
      "Epoch [6/50], Step [440/777], Loss: 0.2974\n",
      "Epoch [6/50], Step [450/777], Loss: 0.1543\n",
      "Epoch [6/50], Step [460/777], Loss: 0.0691\n",
      "Epoch [6/50], Step [470/777], Loss: 0.5585\n",
      "Epoch [6/50], Step [480/777], Loss: 0.1822\n",
      "Epoch [6/50], Step [490/777], Loss: 0.4085\n",
      "Epoch [6/50], Step [500/777], Loss: 0.2101\n",
      "Epoch [6/50], Step [510/777], Loss: 0.2003\n",
      "Epoch [6/50], Step [520/777], Loss: 0.0369\n",
      "Epoch [6/50], Step [530/777], Loss: 0.2164\n",
      "Epoch [6/50], Step [540/777], Loss: 0.2616\n",
      "Epoch [6/50], Step [550/777], Loss: 0.2570\n",
      "Epoch [6/50], Step [560/777], Loss: 0.5391\n",
      "Epoch [6/50], Step [570/777], Loss: 0.0211\n",
      "Epoch [6/50], Step [580/777], Loss: 0.1989\n",
      "Epoch [6/50], Step [590/777], Loss: 0.2824\n",
      "Epoch [6/50], Step [600/777], Loss: 0.3355\n",
      "Epoch [6/50], Step [610/777], Loss: 0.1731\n",
      "Epoch [6/50], Step [620/777], Loss: 0.2065\n",
      "Epoch [6/50], Step [630/777], Loss: 0.3622\n",
      "Epoch [6/50], Step [640/777], Loss: 0.5320\n",
      "Epoch [6/50], Step [650/777], Loss: 0.0423\n",
      "Epoch [6/50], Step [660/777], Loss: 0.2146\n",
      "Epoch [6/50], Step [670/777], Loss: 0.0508\n",
      "Epoch [6/50], Step [680/777], Loss: 0.2988\n",
      "Epoch [6/50], Step [690/777], Loss: 0.5202\n",
      "Epoch [6/50], Step [700/777], Loss: 0.2948\n",
      "Epoch [6/50], Step [710/777], Loss: 0.0732\n",
      "Epoch [6/50], Step [720/777], Loss: 0.1689\n",
      "Epoch [6/50], Step [730/777], Loss: 0.0833\n",
      "Epoch [6/50], Step [740/777], Loss: 0.4953\n",
      "Epoch [6/50], Step [750/777], Loss: 0.5881\n",
      "Epoch [6/50], Step [760/777], Loss: 0.1873\n",
      "Epoch [6/50], Step [770/777], Loss: 0.1071\n",
      "Epoch [6/50], Train Loss: 0.2030, Val Loss: 0.2757, Val Accuracy: 0.9278\n",
      "Epoch [7/50], Step [10/777], Loss: 0.1555\n",
      "Epoch [7/50], Step [20/777], Loss: 0.0729\n",
      "Epoch [7/50], Step [30/777], Loss: 0.0594\n",
      "Epoch [7/50], Step [40/777], Loss: 0.3447\n",
      "Epoch [7/50], Step [50/777], Loss: 0.3831\n",
      "Epoch [7/50], Step [60/777], Loss: 0.1772\n",
      "Epoch [7/50], Step [70/777], Loss: 0.0886\n",
      "Epoch [7/50], Step [80/777], Loss: 0.0947\n",
      "Epoch [7/50], Step [90/777], Loss: 0.1209\n",
      "Epoch [7/50], Step [100/777], Loss: 0.1306\n",
      "Epoch [7/50], Step [110/777], Loss: 0.1798\n",
      "Epoch [7/50], Step [120/777], Loss: 0.1194\n",
      "Epoch [7/50], Step [130/777], Loss: 0.2480\n",
      "Epoch [7/50], Step [140/777], Loss: 0.0192\n",
      "Epoch [7/50], Step [150/777], Loss: 0.0363\n",
      "Epoch [7/50], Step [160/777], Loss: 0.2150\n",
      "Epoch [7/50], Step [170/777], Loss: 0.0597\n",
      "Epoch [7/50], Step [180/777], Loss: 0.1368\n",
      "Epoch [7/50], Step [190/777], Loss: 0.4809\n",
      "Epoch [7/50], Step [200/777], Loss: 0.1598\n",
      "Epoch [7/50], Step [210/777], Loss: 0.2505\n",
      "Epoch [7/50], Step [220/777], Loss: 0.7876\n",
      "Epoch [7/50], Step [230/777], Loss: 0.3819\n",
      "Epoch [7/50], Step [240/777], Loss: 0.0220\n",
      "Epoch [7/50], Step [250/777], Loss: 0.1532\n",
      "Epoch [7/50], Step [260/777], Loss: 0.2026\n",
      "Epoch [7/50], Step [270/777], Loss: 0.0859\n",
      "Epoch [7/50], Step [280/777], Loss: 0.1596\n",
      "Epoch [7/50], Step [290/777], Loss: 0.0255\n",
      "Epoch [7/50], Step [300/777], Loss: 0.0483\n",
      "Epoch [7/50], Step [310/777], Loss: 0.0481\n",
      "Epoch [7/50], Step [320/777], Loss: 0.3000\n",
      "Epoch [7/50], Step [330/777], Loss: 0.2899\n",
      "Epoch [7/50], Step [340/777], Loss: 0.0912\n",
      "Epoch [7/50], Step [350/777], Loss: 0.0517\n",
      "Epoch [7/50], Step [360/777], Loss: 0.1163\n",
      "Epoch [7/50], Step [370/777], Loss: 0.0582\n",
      "Epoch [7/50], Step [380/777], Loss: 0.0793\n",
      "Epoch [7/50], Step [390/777], Loss: 0.0695\n",
      "Epoch [7/50], Step [400/777], Loss: 0.3031\n",
      "Epoch [7/50], Step [410/777], Loss: 0.3079\n",
      "Epoch [7/50], Step [420/777], Loss: 0.0885\n",
      "Epoch [7/50], Step [430/777], Loss: 0.1433\n",
      "Epoch [7/50], Step [440/777], Loss: 0.1013\n",
      "Epoch [7/50], Step [450/777], Loss: 0.2003\n",
      "Epoch [7/50], Step [460/777], Loss: 0.1777\n",
      "Epoch [7/50], Step [470/777], Loss: 0.2567\n",
      "Epoch [7/50], Step [480/777], Loss: 0.0709\n",
      "Epoch [7/50], Step [490/777], Loss: 0.2396\n",
      "Epoch [7/50], Step [500/777], Loss: 0.0389\n",
      "Epoch [7/50], Step [510/777], Loss: 0.3830\n",
      "Epoch [7/50], Step [520/777], Loss: 0.1316\n",
      "Epoch [7/50], Step [530/777], Loss: 0.0770\n",
      "Epoch [7/50], Step [540/777], Loss: 0.2301\n",
      "Epoch [7/50], Step [550/777], Loss: 0.2407\n",
      "Epoch [7/50], Step [560/777], Loss: 0.1506\n",
      "Epoch [7/50], Step [570/777], Loss: 0.0472\n",
      "Epoch [7/50], Step [580/777], Loss: 0.0239\n",
      "Epoch [7/50], Step [590/777], Loss: 0.0700\n",
      "Epoch [7/50], Step [600/777], Loss: 0.3746\n",
      "Epoch [7/50], Step [610/777], Loss: 0.0254\n",
      "Epoch [7/50], Step [620/777], Loss: 0.3280\n",
      "Epoch [7/50], Step [630/777], Loss: 0.1466\n",
      "Epoch [7/50], Step [640/777], Loss: 0.0998\n",
      "Epoch [7/50], Step [650/777], Loss: 0.3164\n",
      "Epoch [7/50], Step [660/777], Loss: 0.0986\n",
      "Epoch [7/50], Step [670/777], Loss: 0.6146\n",
      "Epoch [7/50], Step [680/777], Loss: 0.0595\n",
      "Epoch [7/50], Step [690/777], Loss: 0.0883\n",
      "Epoch [7/50], Step [700/777], Loss: 0.2500\n",
      "Epoch [7/50], Step [710/777], Loss: 0.4669\n",
      "Epoch [7/50], Step [720/777], Loss: 0.0271\n",
      "Epoch [7/50], Step [730/777], Loss: 0.1054\n",
      "Epoch [7/50], Step [740/777], Loss: 0.2072\n",
      "Epoch [7/50], Step [750/777], Loss: 0.0742\n",
      "Epoch [7/50], Step [760/777], Loss: 0.1714\n",
      "Epoch [7/50], Step [770/777], Loss: 0.2117\n",
      "Epoch [7/50], Train Loss: 0.1742, Val Loss: 0.3028, Val Accuracy: 0.9309\n",
      "Epoch [8/50], Step [10/777], Loss: 0.2953\n",
      "Epoch [8/50], Step [20/777], Loss: 0.0605\n",
      "Epoch [8/50], Step [30/777], Loss: 0.4262\n",
      "Epoch [8/50], Step [40/777], Loss: 0.2827\n",
      "Epoch [8/50], Step [50/777], Loss: 0.4500\n",
      "Epoch [8/50], Step [60/777], Loss: 0.0330\n",
      "Epoch [8/50], Step [70/777], Loss: 0.0501\n",
      "Epoch [8/50], Step [80/777], Loss: 0.1283\n",
      "Epoch [8/50], Step [90/777], Loss: 0.0622\n",
      "Epoch [8/50], Step [100/777], Loss: 0.2752\n",
      "Epoch [8/50], Step [110/777], Loss: 0.0406\n",
      "Epoch [8/50], Step [120/777], Loss: 0.1524\n",
      "Epoch [8/50], Step [130/777], Loss: 0.0737\n",
      "Epoch [8/50], Step [140/777], Loss: 0.0264\n",
      "Epoch [8/50], Step [150/777], Loss: 0.0692\n",
      "Epoch [8/50], Step [160/777], Loss: 0.1533\n",
      "Epoch [8/50], Step [170/777], Loss: 0.1581\n",
      "Epoch [8/50], Step [180/777], Loss: 0.0200\n",
      "Epoch [8/50], Step [190/777], Loss: 0.0459\n",
      "Epoch [8/50], Step [200/777], Loss: 0.1579\n",
      "Epoch [8/50], Step [210/777], Loss: 0.1688\n",
      "Epoch [8/50], Step [220/777], Loss: 0.0149\n",
      "Epoch [8/50], Step [230/777], Loss: 0.1574\n",
      "Epoch [8/50], Step [240/777], Loss: 0.3692\n",
      "Epoch [8/50], Step [250/777], Loss: 0.0271\n",
      "Epoch [8/50], Step [260/777], Loss: 0.2563\n",
      "Epoch [8/50], Step [270/777], Loss: 0.2204\n",
      "Epoch [8/50], Step [280/777], Loss: 0.2629\n",
      "Epoch [8/50], Step [290/777], Loss: 0.0641\n",
      "Epoch [8/50], Step [300/777], Loss: 0.1607\n",
      "Epoch [8/50], Step [310/777], Loss: 0.0785\n",
      "Epoch [8/50], Step [320/777], Loss: 0.0864\n",
      "Epoch [8/50], Step [330/777], Loss: 0.0267\n",
      "Epoch [8/50], Step [340/777], Loss: 0.1777\n",
      "Epoch [8/50], Step [350/777], Loss: 0.1027\n",
      "Epoch [8/50], Step [360/777], Loss: 0.1575\n",
      "Epoch [8/50], Step [370/777], Loss: 0.1540\n",
      "Epoch [8/50], Step [380/777], Loss: 0.0611\n",
      "Epoch [8/50], Step [390/777], Loss: 0.0865\n",
      "Epoch [8/50], Step [400/777], Loss: 0.0774\n",
      "Epoch [8/50], Step [410/777], Loss: 0.2411\n",
      "Epoch [8/50], Step [420/777], Loss: 0.2691\n",
      "Epoch [8/50], Step [430/777], Loss: 0.1952\n",
      "Epoch [8/50], Step [440/777], Loss: 0.0374\n",
      "Epoch [8/50], Step [450/777], Loss: 0.0224\n",
      "Epoch [8/50], Step [460/777], Loss: 0.0983\n",
      "Epoch [8/50], Step [470/777], Loss: 0.3842\n",
      "Epoch [8/50], Step [480/777], Loss: 0.0536\n",
      "Epoch [8/50], Step [490/777], Loss: 0.0508\n",
      "Epoch [8/50], Step [500/777], Loss: 0.3993\n",
      "Epoch [8/50], Step [510/777], Loss: 0.1643\n",
      "Epoch [8/50], Step [520/777], Loss: 0.0267\n",
      "Epoch [8/50], Step [530/777], Loss: 0.1166\n",
      "Epoch [8/50], Step [540/777], Loss: 0.5048\n",
      "Epoch [8/50], Step [550/777], Loss: 0.1078\n",
      "Epoch [8/50], Step [560/777], Loss: 0.1337\n",
      "Epoch [8/50], Step [570/777], Loss: 0.1504\n",
      "Epoch [8/50], Step [580/777], Loss: 0.0645\n",
      "Epoch [8/50], Step [590/777], Loss: 0.1327\n",
      "Epoch [8/50], Step [600/777], Loss: 0.0450\n",
      "Epoch [8/50], Step [610/777], Loss: 0.2996\n",
      "Epoch [8/50], Step [620/777], Loss: 0.1970\n",
      "Epoch [8/50], Step [630/777], Loss: 0.0192\n",
      "Epoch [8/50], Step [640/777], Loss: 0.0764\n",
      "Epoch [8/50], Step [650/777], Loss: 0.0536\n",
      "Epoch [8/50], Step [660/777], Loss: 0.0358\n",
      "Epoch [8/50], Step [670/777], Loss: 0.1239\n",
      "Epoch [8/50], Step [680/777], Loss: 0.1945\n",
      "Epoch [8/50], Step [690/777], Loss: 0.0623\n",
      "Epoch [8/50], Step [700/777], Loss: 0.0234\n",
      "Epoch [8/50], Step [710/777], Loss: 0.2347\n",
      "Epoch [8/50], Step [720/777], Loss: 0.3144\n",
      "Epoch [8/50], Step [730/777], Loss: 0.0206\n",
      "Epoch [8/50], Step [740/777], Loss: 0.0457\n",
      "Epoch [8/50], Step [750/777], Loss: 0.0785\n",
      "Epoch [8/50], Step [760/777], Loss: 0.3173\n",
      "Epoch [8/50], Step [770/777], Loss: 0.4011\n",
      "Epoch [8/50], Train Loss: 0.1335, Val Loss: 0.3536, Val Accuracy: 0.9346\n",
      "Model saved with validation accuracy: 0.9346\n",
      "Epoch [9/50], Step [10/777], Loss: 0.0393\n",
      "Epoch [9/50], Step [20/777], Loss: 0.0484\n",
      "Epoch [9/50], Step [30/777], Loss: 0.0216\n",
      "Epoch [9/50], Step [40/777], Loss: 0.0185\n",
      "Epoch [9/50], Step [50/777], Loss: 0.1104\n",
      "Epoch [9/50], Step [60/777], Loss: 0.0212\n",
      "Epoch [9/50], Step [70/777], Loss: 0.0186\n",
      "Epoch [9/50], Step [80/777], Loss: 0.0413\n",
      "Epoch [9/50], Step [90/777], Loss: 0.0118\n",
      "Epoch [9/50], Step [100/777], Loss: 0.0660\n",
      "Epoch [9/50], Step [110/777], Loss: 0.0249\n",
      "Epoch [9/50], Step [120/777], Loss: 0.0308\n",
      "Epoch [9/50], Step [130/777], Loss: 0.0549\n",
      "Epoch [9/50], Step [140/777], Loss: 0.2715\n",
      "Epoch [9/50], Step [150/777], Loss: 0.2499\n",
      "Epoch [9/50], Step [160/777], Loss: 0.0135\n",
      "Epoch [9/50], Step [170/777], Loss: 0.3929\n",
      "Epoch [9/50], Step [180/777], Loss: 0.1685\n",
      "Epoch [9/50], Step [190/777], Loss: 0.0533\n",
      "Epoch [9/50], Step [200/777], Loss: 0.0270\n",
      "Epoch [9/50], Step [210/777], Loss: 0.0259\n",
      "Epoch [9/50], Step [220/777], Loss: 0.0305\n",
      "Epoch [9/50], Step [230/777], Loss: 0.0946\n",
      "Epoch [9/50], Step [240/777], Loss: 0.0504\n",
      "Epoch [9/50], Step [250/777], Loss: 0.0165\n",
      "Epoch [9/50], Step [260/777], Loss: 0.0482\n",
      "Epoch [9/50], Step [270/777], Loss: 0.1942\n",
      "Epoch [9/50], Step [280/777], Loss: 0.0153\n",
      "Epoch [9/50], Step [290/777], Loss: 0.0092\n",
      "Epoch [9/50], Step [300/777], Loss: 0.0721\n",
      "Epoch [9/50], Step [310/777], Loss: 0.1516\n",
      "Epoch [9/50], Step [320/777], Loss: 0.0262\n",
      "Epoch [9/50], Step [330/777], Loss: 0.0512\n",
      "Epoch [9/50], Step [340/777], Loss: 0.0851\n",
      "Epoch [9/50], Step [350/777], Loss: 0.0453\n",
      "Epoch [9/50], Step [360/777], Loss: 0.2912\n",
      "Epoch [9/50], Step [370/777], Loss: 0.1040\n",
      "Epoch [9/50], Step [380/777], Loss: 0.0129\n",
      "Epoch [9/50], Step [390/777], Loss: 0.1643\n",
      "Epoch [9/50], Step [400/777], Loss: 0.0210\n",
      "Epoch [9/50], Step [410/777], Loss: 0.0229\n",
      "Epoch [9/50], Step [420/777], Loss: 0.0265\n",
      "Epoch [9/50], Step [430/777], Loss: 0.2975\n",
      "Epoch [9/50], Step [440/777], Loss: 0.0878\n",
      "Epoch [9/50], Step [450/777], Loss: 0.0803\n",
      "Epoch [9/50], Step [460/777], Loss: 0.0328\n",
      "Epoch [9/50], Step [470/777], Loss: 0.0085\n",
      "Epoch [9/50], Step [480/777], Loss: 0.0310\n",
      "Epoch [9/50], Step [490/777], Loss: 0.0278\n",
      "Epoch [9/50], Step [500/777], Loss: 0.0435\n",
      "Epoch [9/50], Step [510/777], Loss: 0.0288\n",
      "Epoch [9/50], Step [520/777], Loss: 0.0151\n",
      "Epoch [9/50], Step [530/777], Loss: 0.1253\n",
      "Epoch [9/50], Step [540/777], Loss: 0.0759\n",
      "Epoch [9/50], Step [550/777], Loss: 0.0565\n",
      "Epoch [9/50], Step [560/777], Loss: 0.0198\n",
      "Epoch [9/50], Step [570/777], Loss: 0.0301\n",
      "Epoch [9/50], Step [580/777], Loss: 0.0523\n",
      "Epoch [9/50], Step [590/777], Loss: 0.1394\n",
      "Epoch [9/50], Step [600/777], Loss: 0.0306\n",
      "Epoch [9/50], Step [610/777], Loss: 0.0869\n",
      "Epoch [9/50], Step [620/777], Loss: 0.0117\n",
      "Epoch [9/50], Step [630/777], Loss: 0.5518\n",
      "Epoch [9/50], Step [640/777], Loss: 0.0587\n",
      "Epoch [9/50], Step [650/777], Loss: 0.0212\n",
      "Epoch [9/50], Step [660/777], Loss: 0.0345\n",
      "Epoch [9/50], Step [670/777], Loss: 0.1000\n",
      "Epoch [9/50], Step [680/777], Loss: 0.0444\n",
      "Epoch [9/50], Step [690/777], Loss: 0.0153\n",
      "Epoch [9/50], Step [700/777], Loss: 0.0400\n",
      "Epoch [9/50], Step [710/777], Loss: 0.1053\n",
      "Epoch [9/50], Step [720/777], Loss: 0.1213\n",
      "Epoch [9/50], Step [730/777], Loss: 0.0269\n",
      "Epoch [9/50], Step [740/777], Loss: 0.0951\n",
      "Epoch [9/50], Step [750/777], Loss: 0.0159\n",
      "Epoch [9/50], Step [760/777], Loss: 0.0060\n",
      "Epoch [9/50], Step [770/777], Loss: 0.0191\n",
      "Epoch [9/50], Train Loss: 0.0975, Val Loss: 0.3016, Val Accuracy: 0.9286\n",
      "Epoch [10/50], Step [10/777], Loss: 0.3650\n",
      "Epoch [10/50], Step [20/777], Loss: 0.0435\n",
      "Epoch [10/50], Step [30/777], Loss: 0.2148\n",
      "Epoch [10/50], Step [40/777], Loss: 0.0338\n",
      "Epoch [10/50], Step [50/777], Loss: 0.1031\n",
      "Epoch [10/50], Step [60/777], Loss: 0.0616\n",
      "Epoch [10/50], Step [70/777], Loss: 0.0123\n",
      "Epoch [10/50], Step [80/777], Loss: 0.0389\n",
      "Epoch [10/50], Step [90/777], Loss: 0.3434\n",
      "Epoch [10/50], Step [100/777], Loss: 0.0185\n",
      "Epoch [10/50], Step [110/777], Loss: 0.0062\n",
      "Epoch [10/50], Step [120/777], Loss: 0.0205\n",
      "Epoch [10/50], Step [130/777], Loss: 0.0565\n",
      "Epoch [10/50], Step [140/777], Loss: 0.0218\n",
      "Epoch [10/50], Step [150/777], Loss: 0.0355\n",
      "Epoch [10/50], Step [160/777], Loss: 0.1231\n",
      "Epoch [10/50], Step [170/777], Loss: 0.0186\n",
      "Epoch [10/50], Step [180/777], Loss: 0.0633\n",
      "Epoch [10/50], Step [190/777], Loss: 0.0585\n",
      "Epoch [10/50], Step [200/777], Loss: 0.0160\n",
      "Epoch [10/50], Step [210/777], Loss: 0.1543\n",
      "Epoch [10/50], Step [220/777], Loss: 0.1636\n",
      "Epoch [10/50], Step [230/777], Loss: 0.0052\n",
      "Epoch [10/50], Step [240/777], Loss: 0.1859\n",
      "Epoch [10/50], Step [250/777], Loss: 0.1232\n",
      "Epoch [10/50], Step [260/777], Loss: 0.0203\n",
      "Epoch [10/50], Step [270/777], Loss: 0.1647\n",
      "Epoch [10/50], Step [280/777], Loss: 0.0298\n",
      "Epoch [10/50], Step [290/777], Loss: 0.0926\n",
      "Epoch [10/50], Step [300/777], Loss: 0.1985\n",
      "Epoch [10/50], Step [310/777], Loss: 0.0585\n",
      "Epoch [10/50], Step [320/777], Loss: 0.0956\n",
      "Epoch [10/50], Step [330/777], Loss: 0.0561\n",
      "Epoch [10/50], Step [340/777], Loss: 0.1179\n",
      "Epoch [10/50], Step [350/777], Loss: 0.0159\n",
      "Epoch [10/50], Step [360/777], Loss: 0.0125\n",
      "Epoch [10/50], Step [370/777], Loss: 0.0184\n",
      "Epoch [10/50], Step [380/777], Loss: 0.0330\n",
      "Epoch [10/50], Step [390/777], Loss: 0.0192\n",
      "Epoch [10/50], Step [400/777], Loss: 0.0554\n",
      "Epoch [10/50], Step [410/777], Loss: 0.0576\n",
      "Epoch [10/50], Step [420/777], Loss: 0.0638\n",
      "Epoch [10/50], Step [430/777], Loss: 0.0775\n",
      "Epoch [10/50], Step [440/777], Loss: 0.0948\n",
      "Epoch [10/50], Step [450/777], Loss: 0.0104\n",
      "Epoch [10/50], Step [460/777], Loss: 0.2394\n",
      "Epoch [10/50], Step [470/777], Loss: 0.1275\n",
      "Epoch [10/50], Step [480/777], Loss: 0.1626\n",
      "Epoch [10/50], Step [490/777], Loss: 0.0351\n",
      "Epoch [10/50], Step [500/777], Loss: 0.0495\n",
      "Epoch [10/50], Step [510/777], Loss: 0.0350\n",
      "Epoch [10/50], Step [520/777], Loss: 0.0329\n",
      "Epoch [10/50], Step [530/777], Loss: 0.0176\n",
      "Epoch [10/50], Step [540/777], Loss: 0.0216\n",
      "Epoch [10/50], Step [550/777], Loss: 0.2596\n",
      "Epoch [10/50], Step [560/777], Loss: 0.0065\n",
      "Epoch [10/50], Step [570/777], Loss: 0.1824\n",
      "Epoch [10/50], Step [580/777], Loss: 0.2286\n",
      "Epoch [10/50], Step [590/777], Loss: 0.0280\n",
      "Epoch [10/50], Step [600/777], Loss: 0.1993\n",
      "Epoch [10/50], Step [610/777], Loss: 0.0375\n",
      "Epoch [10/50], Step [620/777], Loss: 0.1213\n",
      "Epoch [10/50], Step [630/777], Loss: 0.0134\n",
      "Epoch [10/50], Step [640/777], Loss: 0.0156\n",
      "Epoch [10/50], Step [650/777], Loss: 0.0107\n",
      "Epoch [10/50], Step [660/777], Loss: 0.0863\n",
      "Epoch [10/50], Step [670/777], Loss: 0.2572\n",
      "Epoch [10/50], Step [680/777], Loss: 0.0081\n",
      "Epoch [10/50], Step [690/777], Loss: 0.1200\n",
      "Epoch [10/50], Step [700/777], Loss: 0.0502\n",
      "Epoch [10/50], Step [710/777], Loss: 0.3524\n",
      "Epoch [10/50], Step [720/777], Loss: 0.0401\n",
      "Epoch [10/50], Step [730/777], Loss: 0.6892\n",
      "Epoch [10/50], Step [740/777], Loss: 0.0265\n",
      "Epoch [10/50], Step [750/777], Loss: 0.0690\n",
      "Epoch [10/50], Step [760/777], Loss: 0.0115\n",
      "Epoch [10/50], Step [770/777], Loss: 0.0430\n",
      "Epoch [10/50], Train Loss: 0.0903, Val Loss: 0.3146, Val Accuracy: 0.9331\n",
      "Epoch [11/50], Step [10/777], Loss: 0.0710\n",
      "Epoch [11/50], Step [20/777], Loss: 0.0618\n",
      "Epoch [11/50], Step [30/777], Loss: 0.0087\n",
      "Epoch [11/50], Step [40/777], Loss: 0.0494\n",
      "Epoch [11/50], Step [50/777], Loss: 0.2209\n",
      "Epoch [11/50], Step [60/777], Loss: 0.0979\n",
      "Epoch [11/50], Step [70/777], Loss: 0.1736\n",
      "Epoch [11/50], Step [80/777], Loss: 0.0677\n",
      "Epoch [11/50], Step [90/777], Loss: 0.0140\n",
      "Epoch [11/50], Step [100/777], Loss: 0.0972\n",
      "Epoch [11/50], Step [110/777], Loss: 0.0399\n",
      "Epoch [11/50], Step [120/777], Loss: 0.0534\n",
      "Epoch [11/50], Step [130/777], Loss: 0.0273\n",
      "Epoch [11/50], Step [140/777], Loss: 0.1889\n",
      "Epoch [11/50], Step [150/777], Loss: 0.1127\n",
      "Epoch [11/50], Step [160/777], Loss: 0.0343\n",
      "Epoch [11/50], Step [170/777], Loss: 0.0321\n",
      "Epoch [11/50], Step [180/777], Loss: 0.0961\n",
      "Epoch [11/50], Step [190/777], Loss: 0.0131\n",
      "Epoch [11/50], Step [200/777], Loss: 0.0151\n",
      "Epoch [11/50], Step [210/777], Loss: 0.0069\n",
      "Epoch [11/50], Step [220/777], Loss: 0.0131\n",
      "Epoch [11/50], Step [230/777], Loss: 0.0177\n",
      "Epoch [11/50], Step [240/777], Loss: 0.4191\n",
      "Epoch [11/50], Step [250/777], Loss: 0.0766\n",
      "Epoch [11/50], Step [260/777], Loss: 0.0549\n",
      "Epoch [11/50], Step [270/777], Loss: 0.0510\n",
      "Epoch [11/50], Step [280/777], Loss: 0.0182\n",
      "Epoch [11/50], Step [290/777], Loss: 0.0170\n",
      "Epoch [11/50], Step [300/777], Loss: 0.0275\n",
      "Epoch [11/50], Step [310/777], Loss: 0.0956\n",
      "Epoch [11/50], Step [320/777], Loss: 0.0145\n",
      "Epoch [11/50], Step [330/777], Loss: 0.1045\n",
      "Epoch [11/50], Step [340/777], Loss: 0.2977\n",
      "Epoch [11/50], Step [350/777], Loss: 0.0241\n",
      "Epoch [11/50], Step [360/777], Loss: 0.0179\n",
      "Epoch [11/50], Step [370/777], Loss: 0.1183\n",
      "Epoch [11/50], Step [380/777], Loss: 0.0150\n",
      "Epoch [11/50], Step [390/777], Loss: 0.0180\n",
      "Epoch [11/50], Step [400/777], Loss: 0.0473\n",
      "Epoch [11/50], Step [410/777], Loss: 0.1033\n",
      "Epoch [11/50], Step [420/777], Loss: 0.0280\n",
      "Epoch [11/50], Step [430/777], Loss: 0.0173\n",
      "Epoch [11/50], Step [440/777], Loss: 0.0368\n",
      "Epoch [11/50], Step [450/777], Loss: 0.1681\n",
      "Epoch [11/50], Step [460/777], Loss: 0.1880\n",
      "Epoch [11/50], Step [470/777], Loss: 0.0940\n",
      "Epoch [11/50], Step [480/777], Loss: 0.0787\n",
      "Epoch [11/50], Step [490/777], Loss: 0.1425\n",
      "Epoch [11/50], Step [500/777], Loss: 0.0212\n",
      "Epoch [11/50], Step [510/777], Loss: 0.0588\n",
      "Epoch [11/50], Step [520/777], Loss: 0.2725\n",
      "Epoch [11/50], Step [530/777], Loss: 0.0412\n",
      "Epoch [11/50], Step [540/777], Loss: 0.0819\n",
      "Epoch [11/50], Step [550/777], Loss: 0.0197\n",
      "Epoch [11/50], Step [560/777], Loss: 0.0493\n",
      "Epoch [11/50], Step [570/777], Loss: 0.0406\n",
      "Epoch [11/50], Step [580/777], Loss: 0.0910\n",
      "Epoch [11/50], Step [590/777], Loss: 0.1035\n",
      "Epoch [11/50], Step [600/777], Loss: 0.0052\n",
      "Epoch [11/50], Step [610/777], Loss: 0.0315\n",
      "Epoch [11/50], Step [620/777], Loss: 0.0586\n",
      "Epoch [11/50], Step [630/777], Loss: 0.0189\n",
      "Epoch [11/50], Step [640/777], Loss: 0.1715\n",
      "Epoch [11/50], Step [650/777], Loss: 0.1616\n",
      "Epoch [11/50], Step [660/777], Loss: 0.0593\n",
      "Epoch [11/50], Step [670/777], Loss: 0.0178\n",
      "Epoch [11/50], Step [680/777], Loss: 0.0302\n",
      "Epoch [11/50], Step [690/777], Loss: 0.1158\n",
      "Epoch [11/50], Step [700/777], Loss: 0.1597\n",
      "Epoch [11/50], Step [710/777], Loss: 0.0450\n",
      "Epoch [11/50], Step [720/777], Loss: 0.4711\n",
      "Epoch [11/50], Step [730/777], Loss: 0.0163\n",
      "Epoch [11/50], Step [740/777], Loss: 0.0197\n",
      "Epoch [11/50], Step [750/777], Loss: 0.0307\n",
      "Epoch [11/50], Step [760/777], Loss: 0.1035\n",
      "Epoch [11/50], Step [770/777], Loss: 0.0476\n",
      "Epoch [11/50], Train Loss: 0.0824, Val Loss: 0.3151, Val Accuracy: 0.9297\n",
      "Epoch [12/50], Step [10/777], Loss: 0.0213\n",
      "Epoch [12/50], Step [20/777], Loss: 0.0456\n",
      "Epoch [12/50], Step [30/777], Loss: 0.0484\n",
      "Epoch [12/50], Step [40/777], Loss: 0.2045\n",
      "Epoch [12/50], Step [50/777], Loss: 0.1211\n",
      "Epoch [12/50], Step [60/777], Loss: 0.1236\n",
      "Epoch [12/50], Step [70/777], Loss: 0.1292\n",
      "Epoch [12/50], Step [80/777], Loss: 0.0334\n",
      "Epoch [12/50], Step [90/777], Loss: 0.1928\n",
      "Epoch [12/50], Step [100/777], Loss: 0.2921\n",
      "Epoch [12/50], Step [110/777], Loss: 0.0943\n",
      "Epoch [12/50], Step [120/777], Loss: 0.0420\n",
      "Epoch [12/50], Step [130/777], Loss: 0.0169\n",
      "Epoch [12/50], Step [140/777], Loss: 0.0226\n",
      "Epoch [12/50], Step [150/777], Loss: 0.2141\n",
      "Epoch [12/50], Step [160/777], Loss: 0.0825\n",
      "Epoch [12/50], Step [170/777], Loss: 0.0861\n",
      "Epoch [12/50], Step [180/777], Loss: 0.2264\n",
      "Epoch [12/50], Step [190/777], Loss: 0.0087\n",
      "Epoch [12/50], Step [200/777], Loss: 0.0389\n",
      "Epoch [12/50], Step [210/777], Loss: 0.0130\n",
      "Epoch [12/50], Step [220/777], Loss: 0.1188\n",
      "Epoch [12/50], Step [230/777], Loss: 0.0365\n",
      "Epoch [12/50], Step [240/777], Loss: 0.0684\n",
      "Epoch [12/50], Step [250/777], Loss: 0.0485\n",
      "Epoch [12/50], Step [260/777], Loss: 0.1254\n",
      "Epoch [12/50], Step [270/777], Loss: 0.2721\n",
      "Epoch [12/50], Step [280/777], Loss: 0.0619\n",
      "Epoch [12/50], Step [290/777], Loss: 0.0090\n",
      "Epoch [12/50], Step [300/777], Loss: 0.1873\n",
      "Epoch [12/50], Step [310/777], Loss: 0.1082\n",
      "Epoch [12/50], Step [320/777], Loss: 0.1126\n",
      "Epoch [12/50], Step [330/777], Loss: 0.0142\n",
      "Epoch [12/50], Step [340/777], Loss: 0.3828\n",
      "Epoch [12/50], Step [350/777], Loss: 0.0718\n",
      "Epoch [12/50], Step [360/777], Loss: 0.0187\n",
      "Epoch [12/50], Step [370/777], Loss: 0.1083\n",
      "Epoch [12/50], Step [380/777], Loss: 0.0655\n",
      "Epoch [12/50], Step [390/777], Loss: 0.0742\n",
      "Epoch [12/50], Step [400/777], Loss: 0.0437\n",
      "Epoch [12/50], Step [410/777], Loss: 0.0226\n",
      "Epoch [12/50], Step [420/777], Loss: 0.1279\n",
      "Epoch [12/50], Step [430/777], Loss: 0.0350\n",
      "Epoch [12/50], Step [440/777], Loss: 0.0072\n",
      "Epoch [12/50], Step [450/777], Loss: 0.0550\n",
      "Epoch [12/50], Step [460/777], Loss: 0.0908\n",
      "Epoch [12/50], Step [470/777], Loss: 0.0117\n",
      "Epoch [12/50], Step [480/777], Loss: 0.0423\n",
      "Epoch [12/50], Step [490/777], Loss: 0.0102\n",
      "Epoch [12/50], Step [500/777], Loss: 0.0917\n",
      "Epoch [12/50], Step [510/777], Loss: 0.0406\n",
      "Epoch [12/50], Step [520/777], Loss: 0.1323\n",
      "Epoch [12/50], Step [530/777], Loss: 0.0128\n",
      "Epoch [12/50], Step [540/777], Loss: 0.0523\n",
      "Epoch [12/50], Step [550/777], Loss: 0.0500\n",
      "Epoch [12/50], Step [560/777], Loss: 0.0165\n",
      "Epoch [12/50], Step [570/777], Loss: 0.0947\n",
      "Epoch [12/50], Step [580/777], Loss: 0.0198\n",
      "Epoch [12/50], Step [590/777], Loss: 0.0543\n",
      "Epoch [12/50], Step [600/777], Loss: 0.0342\n",
      "Epoch [12/50], Step [610/777], Loss: 0.0940\n",
      "Epoch [12/50], Step [620/777], Loss: 0.0170\n",
      "Epoch [12/50], Step [630/777], Loss: 0.1076\n",
      "Epoch [12/50], Step [640/777], Loss: 0.1107\n",
      "Epoch [12/50], Step [650/777], Loss: 0.1571\n",
      "Epoch [12/50], Step [660/777], Loss: 0.0426\n",
      "Epoch [12/50], Step [670/777], Loss: 0.0177\n",
      "Epoch [12/50], Step [680/777], Loss: 0.0102\n",
      "Epoch [12/50], Step [690/777], Loss: 0.0425\n",
      "Epoch [12/50], Step [700/777], Loss: 0.0173\n",
      "Epoch [12/50], Step [710/777], Loss: 0.0318\n",
      "Epoch [12/50], Step [720/777], Loss: 0.0054\n",
      "Epoch [12/50], Step [730/777], Loss: 0.0128\n",
      "Epoch [12/50], Step [740/777], Loss: 0.0223\n",
      "Epoch [12/50], Step [750/777], Loss: 0.3484\n",
      "Epoch [12/50], Step [760/777], Loss: 0.0149\n",
      "Epoch [12/50], Step [770/777], Loss: 0.3153\n",
      "Epoch [12/50], Train Loss: 0.0769, Val Loss: 0.3231, Val Accuracy: 0.9290\n",
      "Epoch [13/50], Step [10/777], Loss: 0.0047\n",
      "Epoch [13/50], Step [20/777], Loss: 0.1178\n",
      "Epoch [13/50], Step [30/777], Loss: 0.1140\n",
      "Epoch [13/50], Step [40/777], Loss: 0.0117\n",
      "Epoch [13/50], Step [50/777], Loss: 0.1437\n",
      "Epoch [13/50], Step [60/777], Loss: 0.2199\n",
      "Epoch [13/50], Step [70/777], Loss: 0.0466\n",
      "Epoch [13/50], Step [80/777], Loss: 0.1580\n",
      "Epoch [13/50], Step [90/777], Loss: 0.0554\n",
      "Epoch [13/50], Step [100/777], Loss: 0.0126\n",
      "Epoch [13/50], Step [110/777], Loss: 0.1227\n",
      "Epoch [13/50], Step [120/777], Loss: 0.0922\n",
      "Epoch [13/50], Step [130/777], Loss: 0.0253\n",
      "Epoch [13/50], Step [140/777], Loss: 0.2050\n",
      "Epoch [13/50], Step [150/777], Loss: 0.0409\n",
      "Epoch [13/50], Step [160/777], Loss: 0.0288\n",
      "Epoch [13/50], Step [170/777], Loss: 0.0271\n",
      "Epoch [13/50], Step [180/777], Loss: 0.0218\n",
      "Epoch [13/50], Step [190/777], Loss: 0.0555\n",
      "Epoch [13/50], Step [200/777], Loss: 0.0698\n",
      "Epoch [13/50], Step [210/777], Loss: 0.0981\n",
      "Epoch [13/50], Step [220/777], Loss: 0.0590\n",
      "Epoch [13/50], Step [230/777], Loss: 0.0171\n",
      "Epoch [13/50], Step [240/777], Loss: 0.0718\n",
      "Epoch [13/50], Step [250/777], Loss: 0.2309\n",
      "Epoch [13/50], Step [260/777], Loss: 0.0628\n",
      "Epoch [13/50], Step [270/777], Loss: 0.0586\n",
      "Epoch [13/50], Step [280/777], Loss: 0.0873\n",
      "Epoch [13/50], Step [290/777], Loss: 0.1182\n",
      "Epoch [13/50], Step [300/777], Loss: 0.0164\n",
      "Epoch [13/50], Step [310/777], Loss: 0.0128\n",
      "Epoch [13/50], Step [320/777], Loss: 0.0799\n",
      "Epoch [13/50], Step [330/777], Loss: 0.0459\n",
      "Epoch [13/50], Step [340/777], Loss: 0.0086\n",
      "Epoch [13/50], Step [350/777], Loss: 0.0267\n",
      "Epoch [13/50], Step [360/777], Loss: 0.0222\n",
      "Epoch [13/50], Step [370/777], Loss: 0.0157\n",
      "Epoch [13/50], Step [380/777], Loss: 0.1781\n",
      "Epoch [13/50], Step [390/777], Loss: 0.0594\n",
      "Epoch [13/50], Step [400/777], Loss: 0.3016\n",
      "Epoch [13/50], Step [410/777], Loss: 0.0159\n",
      "Epoch [13/50], Step [420/777], Loss: 0.0747\n",
      "Epoch [13/50], Step [430/777], Loss: 0.0124\n",
      "Epoch [13/50], Step [440/777], Loss: 0.1631\n",
      "Epoch [13/50], Step [450/777], Loss: 0.0269\n",
      "Epoch [13/50], Step [460/777], Loss: 0.0127\n",
      "Epoch [13/50], Step [470/777], Loss: 0.0468\n",
      "Epoch [13/50], Step [480/777], Loss: 0.0058\n",
      "Epoch [13/50], Step [490/777], Loss: 0.0311\n",
      "Epoch [13/50], Step [500/777], Loss: 0.2453\n",
      "Epoch [13/50], Step [510/777], Loss: 0.0649\n",
      "Epoch [13/50], Step [520/777], Loss: 0.1651\n",
      "Epoch [13/50], Step [530/777], Loss: 0.1008\n",
      "Epoch [13/50], Step [540/777], Loss: 0.0043\n",
      "Epoch [13/50], Step [550/777], Loss: 0.0812\n",
      "Epoch [13/50], Step [560/777], Loss: 0.0729\n",
      "Epoch [13/50], Step [570/777], Loss: 0.1406\n",
      "Epoch [13/50], Step [580/777], Loss: 0.0526\n",
      "Epoch [13/50], Step [590/777], Loss: 0.1234\n",
      "Epoch [13/50], Step [600/777], Loss: 0.0271\n",
      "Epoch [13/50], Step [610/777], Loss: 0.0037\n",
      "Epoch [13/50], Step [620/777], Loss: 0.0809\n",
      "Epoch [13/50], Step [630/777], Loss: 0.0142\n",
      "Epoch [13/50], Step [640/777], Loss: 0.1746\n",
      "Epoch [13/50], Step [650/777], Loss: 0.0129\n",
      "Epoch [13/50], Step [660/777], Loss: 0.1099\n",
      "Epoch [13/50], Step [670/777], Loss: 0.0076\n",
      "Epoch [13/50], Step [680/777], Loss: 0.0087\n",
      "Epoch [13/50], Step [690/777], Loss: 0.0402\n",
      "Epoch [13/50], Step [700/777], Loss: 0.1268\n",
      "Epoch [13/50], Step [710/777], Loss: 0.0601\n",
      "Epoch [13/50], Step [720/777], Loss: 0.0239\n",
      "Epoch [13/50], Step [730/777], Loss: 0.0312\n",
      "Epoch [13/50], Step [740/777], Loss: 0.1457\n",
      "Epoch [13/50], Step [750/777], Loss: 0.0293\n",
      "Epoch [13/50], Step [760/777], Loss: 0.0104\n",
      "Epoch [13/50], Step [770/777], Loss: 0.0310\n",
      "Epoch [13/50], Train Loss: 0.0745, Val Loss: 0.3174, Val Accuracy: 0.9278\n",
      "Early stopping triggered after 13 epochs\n",
      "\n",
      "Evaluating ShuffleNet on test set...\n",
      "\n",
      "ShuffleNet Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DJI       0.93      0.77      0.84       200\n",
      "   FutabaT14       0.95      0.83      0.89       548\n",
      "    FutabaT7       0.89      0.88      0.89        93\n",
      "    Graupner       1.00      0.96      0.98       107\n",
      "       Noise       0.89      0.98      0.94      1314\n",
      "     Taranis       0.99      0.97      0.98       268\n",
      "     Turnigy       0.99      0.90      0.94       133\n",
      "\n",
      "    accuracy                           0.92      2663\n",
      "   macro avg       0.95      0.90      0.92      2663\n",
      "weighted avg       0.93      0.92      0.92      2663\n",
      "\n",
      "\n",
      "ShuffleNet Multi-class ROC AUC Score: 0.9743\n",
      "\n",
      "Analyzing ShuffleNet performance by SNR levels...\n",
      "\n",
      "ShuffleNet Performance by SNR level:\n",
      "SNR (dB) | Accuracy | Samples\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ShuffleNet Total Number of Parameters: 1,260,779\n",
      "ShuffleNet Average Inference Time per Sample: 6.554 ms\n",
      "ShuffleNet FLOPs: 151,691,960.0 (151.69 M)\n",
      "ShuffleNet MACs: 1,260,779.0 (1.26 M)\n",
      "\n",
      "✅ ShuffleNet Accuracy for class 'DJI': 77.00%\n",
      "✅ ShuffleNet Accuracy for class 'FutabaT14': 82.85%\n",
      "✅ ShuffleNet Accuracy for class 'FutabaT7': 88.17%\n",
      "✅ ShuffleNet Accuracy for class 'Graupner': 96.26%\n",
      "✅ ShuffleNet Accuracy for class 'Noise': 98.17%\n",
      "✅ ShuffleNet Accuracy for class 'Taranis': 97.01%\n",
      "✅ ShuffleNet Accuracy for class 'Turnigy': 90.23%\n",
      "\n",
      "✅ ShuffleNet Test Set Accuracy: 0.925\n",
      "📊 ShuffleNet Model Size: 4.81 MB\n",
      "\n",
      "Key characteristics of ShuffleNet:\n",
      "- Designed for mobile devices with limited computing power\n",
      "- Uses pointwise group convolutions and channel shuffle operations\n",
      "- Very computationally efficient with low FLOPs\n",
      "- Good balance between accuracy and model size\n",
      "\n",
      "Metrics saved to shufflenet_drone_rf_metrics.json\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING EFFICIENTNET\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 159MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training and Evaluating EfficientNet\n",
      "==================================================\n",
      "\n",
      "EfficientNet Summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]             864\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "              SiLU-3         [-1, 32, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]             288\n",
      "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
      "              SiLU-6         [-1, 32, 112, 112]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 32, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]             264\n",
      "              SiLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10             [-1, 32, 1, 1]             288\n",
      "          Sigmoid-11             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-12         [-1, 32, 112, 112]               0\n",
      "           Conv2d-13         [-1, 16, 112, 112]             512\n",
      "      BatchNorm2d-14         [-1, 16, 112, 112]              32\n",
      "           MBConv-15         [-1, 16, 112, 112]               0\n",
      "           Conv2d-16         [-1, 96, 112, 112]           1,536\n",
      "      BatchNorm2d-17         [-1, 96, 112, 112]             192\n",
      "             SiLU-18         [-1, 96, 112, 112]               0\n",
      "           Conv2d-19           [-1, 96, 56, 56]             864\n",
      "      BatchNorm2d-20           [-1, 96, 56, 56]             192\n",
      "             SiLU-21           [-1, 96, 56, 56]               0\n",
      "AdaptiveAvgPool2d-22             [-1, 96, 1, 1]               0\n",
      "           Conv2d-23              [-1, 4, 1, 1]             388\n",
      "             SiLU-24              [-1, 4, 1, 1]               0\n",
      "           Conv2d-25             [-1, 96, 1, 1]             480\n",
      "          Sigmoid-26             [-1, 96, 1, 1]               0\n",
      "SqueezeExcitation-27           [-1, 96, 56, 56]               0\n",
      "           Conv2d-28           [-1, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-29           [-1, 24, 56, 56]              48\n",
      "           MBConv-30           [-1, 24, 56, 56]               0\n",
      "           Conv2d-31          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-32          [-1, 144, 56, 56]             288\n",
      "             SiLU-33          [-1, 144, 56, 56]               0\n",
      "           Conv2d-34          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-35          [-1, 144, 56, 56]             288\n",
      "             SiLU-36          [-1, 144, 56, 56]               0\n",
      "AdaptiveAvgPool2d-37            [-1, 144, 1, 1]               0\n",
      "           Conv2d-38              [-1, 6, 1, 1]             870\n",
      "             SiLU-39              [-1, 6, 1, 1]               0\n",
      "           Conv2d-40            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-41            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-42          [-1, 144, 56, 56]               0\n",
      "           Conv2d-43           [-1, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-44           [-1, 24, 56, 56]              48\n",
      "  StochasticDepth-45           [-1, 24, 56, 56]               0\n",
      "           MBConv-46           [-1, 24, 56, 56]               0\n",
      "           Conv2d-47          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-48          [-1, 144, 56, 56]             288\n",
      "             SiLU-49          [-1, 144, 56, 56]               0\n",
      "           Conv2d-50          [-1, 144, 28, 28]           3,600\n",
      "      BatchNorm2d-51          [-1, 144, 28, 28]             288\n",
      "             SiLU-52          [-1, 144, 28, 28]               0\n",
      "AdaptiveAvgPool2d-53            [-1, 144, 1, 1]               0\n",
      "           Conv2d-54              [-1, 6, 1, 1]             870\n",
      "             SiLU-55              [-1, 6, 1, 1]               0\n",
      "           Conv2d-56            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-57            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-58          [-1, 144, 28, 28]               0\n",
      "           Conv2d-59           [-1, 40, 28, 28]           5,760\n",
      "      BatchNorm2d-60           [-1, 40, 28, 28]              80\n",
      "           MBConv-61           [-1, 40, 28, 28]               0\n",
      "           Conv2d-62          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-63          [-1, 240, 28, 28]             480\n",
      "             SiLU-64          [-1, 240, 28, 28]               0\n",
      "           Conv2d-65          [-1, 240, 28, 28]           6,000\n",
      "      BatchNorm2d-66          [-1, 240, 28, 28]             480\n",
      "             SiLU-67          [-1, 240, 28, 28]               0\n",
      "AdaptiveAvgPool2d-68            [-1, 240, 1, 1]               0\n",
      "           Conv2d-69             [-1, 10, 1, 1]           2,410\n",
      "             SiLU-70             [-1, 10, 1, 1]               0\n",
      "           Conv2d-71            [-1, 240, 1, 1]           2,640\n",
      "          Sigmoid-72            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-73          [-1, 240, 28, 28]               0\n",
      "           Conv2d-74           [-1, 40, 28, 28]           9,600\n",
      "      BatchNorm2d-75           [-1, 40, 28, 28]              80\n",
      "  StochasticDepth-76           [-1, 40, 28, 28]               0\n",
      "           MBConv-77           [-1, 40, 28, 28]               0\n",
      "           Conv2d-78          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-79          [-1, 240, 28, 28]             480\n",
      "             SiLU-80          [-1, 240, 28, 28]               0\n",
      "           Conv2d-81          [-1, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-82          [-1, 240, 14, 14]             480\n",
      "             SiLU-83          [-1, 240, 14, 14]               0\n",
      "AdaptiveAvgPool2d-84            [-1, 240, 1, 1]               0\n",
      "           Conv2d-85             [-1, 10, 1, 1]           2,410\n",
      "             SiLU-86             [-1, 10, 1, 1]               0\n",
      "           Conv2d-87            [-1, 240, 1, 1]           2,640\n",
      "          Sigmoid-88            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-89          [-1, 240, 14, 14]               0\n",
      "           Conv2d-90           [-1, 80, 14, 14]          19,200\n",
      "      BatchNorm2d-91           [-1, 80, 14, 14]             160\n",
      "           MBConv-92           [-1, 80, 14, 14]               0\n",
      "           Conv2d-93          [-1, 480, 14, 14]          38,400\n",
      "      BatchNorm2d-94          [-1, 480, 14, 14]             960\n",
      "             SiLU-95          [-1, 480, 14, 14]               0\n",
      "           Conv2d-96          [-1, 480, 14, 14]           4,320\n",
      "      BatchNorm2d-97          [-1, 480, 14, 14]             960\n",
      "             SiLU-98          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-99            [-1, 480, 1, 1]               0\n",
      "          Conv2d-100             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-101             [-1, 20, 1, 1]               0\n",
      "          Conv2d-102            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-103            [-1, 480, 1, 1]               0\n",
      "SqueezeExcitation-104          [-1, 480, 14, 14]               0\n",
      "          Conv2d-105           [-1, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-106           [-1, 80, 14, 14]             160\n",
      " StochasticDepth-107           [-1, 80, 14, 14]               0\n",
      "          MBConv-108           [-1, 80, 14, 14]               0\n",
      "          Conv2d-109          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-110          [-1, 480, 14, 14]             960\n",
      "            SiLU-111          [-1, 480, 14, 14]               0\n",
      "          Conv2d-112          [-1, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-113          [-1, 480, 14, 14]             960\n",
      "            SiLU-114          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-115            [-1, 480, 1, 1]               0\n",
      "          Conv2d-116             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-117             [-1, 20, 1, 1]               0\n",
      "          Conv2d-118            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-119            [-1, 480, 1, 1]               0\n",
      "SqueezeExcitation-120          [-1, 480, 14, 14]               0\n",
      "          Conv2d-121           [-1, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-122           [-1, 80, 14, 14]             160\n",
      " StochasticDepth-123           [-1, 80, 14, 14]               0\n",
      "          MBConv-124           [-1, 80, 14, 14]               0\n",
      "          Conv2d-125          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-126          [-1, 480, 14, 14]             960\n",
      "            SiLU-127          [-1, 480, 14, 14]               0\n",
      "          Conv2d-128          [-1, 480, 14, 14]          12,000\n",
      "     BatchNorm2d-129          [-1, 480, 14, 14]             960\n",
      "            SiLU-130          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-131            [-1, 480, 1, 1]               0\n",
      "          Conv2d-132             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-133             [-1, 20, 1, 1]               0\n",
      "          Conv2d-134            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-135            [-1, 480, 1, 1]               0\n",
      "SqueezeExcitation-136          [-1, 480, 14, 14]               0\n",
      "          Conv2d-137          [-1, 112, 14, 14]          53,760\n",
      "     BatchNorm2d-138          [-1, 112, 14, 14]             224\n",
      "          MBConv-139          [-1, 112, 14, 14]               0\n",
      "          Conv2d-140          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-141          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-142          [-1, 672, 14, 14]               0\n",
      "          Conv2d-143          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-144          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-145          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-146            [-1, 672, 1, 1]               0\n",
      "          Conv2d-147             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-148             [-1, 28, 1, 1]               0\n",
      "          Conv2d-149            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-150            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-151          [-1, 672, 14, 14]               0\n",
      "          Conv2d-152          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-153          [-1, 112, 14, 14]             224\n",
      " StochasticDepth-154          [-1, 112, 14, 14]               0\n",
      "          MBConv-155          [-1, 112, 14, 14]               0\n",
      "          Conv2d-156          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-157          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-158          [-1, 672, 14, 14]               0\n",
      "          Conv2d-159          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-160          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-161          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-162            [-1, 672, 1, 1]               0\n",
      "          Conv2d-163             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-164             [-1, 28, 1, 1]               0\n",
      "          Conv2d-165            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-166            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-167          [-1, 672, 14, 14]               0\n",
      "          Conv2d-168          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-169          [-1, 112, 14, 14]             224\n",
      " StochasticDepth-170          [-1, 112, 14, 14]               0\n",
      "          MBConv-171          [-1, 112, 14, 14]               0\n",
      "          Conv2d-172          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-173          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-174          [-1, 672, 14, 14]               0\n",
      "          Conv2d-175            [-1, 672, 7, 7]          16,800\n",
      "     BatchNorm2d-176            [-1, 672, 7, 7]           1,344\n",
      "            SiLU-177            [-1, 672, 7, 7]               0\n",
      "AdaptiveAvgPool2d-178            [-1, 672, 1, 1]               0\n",
      "          Conv2d-179             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-180             [-1, 28, 1, 1]               0\n",
      "          Conv2d-181            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-182            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-183            [-1, 672, 7, 7]               0\n",
      "          Conv2d-184            [-1, 192, 7, 7]         129,024\n",
      "     BatchNorm2d-185            [-1, 192, 7, 7]             384\n",
      "          MBConv-186            [-1, 192, 7, 7]               0\n",
      "          Conv2d-187           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-188           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-189           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-190           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-191           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-192           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-193           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-194             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-195             [-1, 48, 1, 1]               0\n",
      "          Conv2d-196           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-197           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-198           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-199            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-200            [-1, 192, 7, 7]             384\n",
      " StochasticDepth-201            [-1, 192, 7, 7]               0\n",
      "          MBConv-202            [-1, 192, 7, 7]               0\n",
      "          Conv2d-203           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-204           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-205           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-206           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-207           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-208           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-209           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-210             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-211             [-1, 48, 1, 1]               0\n",
      "          Conv2d-212           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-213           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-214           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-215            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-216            [-1, 192, 7, 7]             384\n",
      " StochasticDepth-217            [-1, 192, 7, 7]               0\n",
      "          MBConv-218            [-1, 192, 7, 7]               0\n",
      "          Conv2d-219           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-220           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-221           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-222           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-223           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-224           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-225           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-226             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-227             [-1, 48, 1, 1]               0\n",
      "          Conv2d-228           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-229           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-230           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-231            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-232            [-1, 192, 7, 7]             384\n",
      " StochasticDepth-233            [-1, 192, 7, 7]               0\n",
      "          MBConv-234            [-1, 192, 7, 7]               0\n",
      "          Conv2d-235           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-236           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-237           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-238           [-1, 1152, 7, 7]          10,368\n",
      "     BatchNorm2d-239           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-240           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-241           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-242             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-243             [-1, 48, 1, 1]               0\n",
      "          Conv2d-244           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-245           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-246           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-247            [-1, 320, 7, 7]         368,640\n",
      "     BatchNorm2d-248            [-1, 320, 7, 7]             640\n",
      "          MBConv-249            [-1, 320, 7, 7]               0\n",
      "          Conv2d-250           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-251           [-1, 1280, 7, 7]           2,560\n",
      "            SiLU-252           [-1, 1280, 7, 7]               0\n",
      "AdaptiveAvgPool2d-253           [-1, 1280, 1, 1]               0\n",
      "         Dropout-254                 [-1, 1280]               0\n",
      "          Linear-255                    [-1, 7]           8,967\n",
      "================================================================\n",
      "Total params: 4,016,515\n",
      "Trainable params: 4,016,515\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 173.64\n",
      "Params size (MB): 15.32\n",
      "Estimated Total Size (MB): 189.54\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Starting training EfficientNet...\n",
      "Epoch [1/50], Step [10/777], Loss: 1.7996\n",
      "Epoch [1/50], Step [20/777], Loss: 1.6517\n",
      "Epoch [1/50], Step [30/777], Loss: 1.4118\n",
      "Epoch [1/50], Step [40/777], Loss: 1.2418\n",
      "Epoch [1/50], Step [50/777], Loss: 1.2357\n",
      "Epoch [1/50], Step [60/777], Loss: 1.2837\n",
      "Epoch [1/50], Step [70/777], Loss: 1.5326\n",
      "Epoch [1/50], Step [80/777], Loss: 1.0253\n",
      "Epoch [1/50], Step [90/777], Loss: 1.1245\n",
      "Epoch [1/50], Step [100/777], Loss: 1.1858\n",
      "Epoch [1/50], Step [110/777], Loss: 1.6132\n",
      "Epoch [1/50], Step [120/777], Loss: 1.0306\n",
      "Epoch [1/50], Step [130/777], Loss: 1.4806\n",
      "Epoch [1/50], Step [140/777], Loss: 0.7451\n",
      "Epoch [1/50], Step [150/777], Loss: 1.1831\n",
      "Epoch [1/50], Step [160/777], Loss: 0.8198\n",
      "Epoch [1/50], Step [170/777], Loss: 0.8446\n",
      "Epoch [1/50], Step [180/777], Loss: 1.0125\n",
      "Epoch [1/50], Step [190/777], Loss: 0.7392\n",
      "Epoch [1/50], Step [200/777], Loss: 1.2854\n",
      "Epoch [1/50], Step [210/777], Loss: 1.0094\n",
      "Epoch [1/50], Step [220/777], Loss: 0.6620\n",
      "Epoch [1/50], Step [230/777], Loss: 0.7037\n",
      "Epoch [1/50], Step [240/777], Loss: 0.9913\n",
      "Epoch [1/50], Step [250/777], Loss: 0.8323\n",
      "Epoch [1/50], Step [260/777], Loss: 0.7205\n",
      "Epoch [1/50], Step [270/777], Loss: 0.9964\n",
      "Epoch [1/50], Step [280/777], Loss: 0.5665\n",
      "Epoch [1/50], Step [290/777], Loss: 0.5600\n",
      "Epoch [1/50], Step [300/777], Loss: 0.9646\n",
      "Epoch [1/50], Step [310/777], Loss: 0.7651\n",
      "Epoch [1/50], Step [320/777], Loss: 0.2786\n",
      "Epoch [1/50], Step [330/777], Loss: 0.9499\n",
      "Epoch [1/50], Step [340/777], Loss: 0.9381\n",
      "Epoch [1/50], Step [350/777], Loss: 0.4104\n",
      "Epoch [1/50], Step [360/777], Loss: 0.4725\n",
      "Epoch [1/50], Step [370/777], Loss: 0.9746\n",
      "Epoch [1/50], Step [380/777], Loss: 0.4391\n",
      "Epoch [1/50], Step [390/777], Loss: 0.7745\n",
      "Epoch [1/50], Step [400/777], Loss: 0.2677\n",
      "Epoch [1/50], Step [410/777], Loss: 0.5056\n",
      "Epoch [1/50], Step [420/777], Loss: 0.9959\n",
      "Epoch [1/50], Step [430/777], Loss: 0.4442\n",
      "Epoch [1/50], Step [440/777], Loss: 0.9151\n",
      "Epoch [1/50], Step [450/777], Loss: 0.3311\n",
      "Epoch [1/50], Step [460/777], Loss: 0.4270\n",
      "Epoch [1/50], Step [470/777], Loss: 0.7161\n",
      "Epoch [1/50], Step [480/777], Loss: 0.1884\n",
      "Epoch [1/50], Step [490/777], Loss: 0.6113\n",
      "Epoch [1/50], Step [500/777], Loss: 0.6484\n",
      "Epoch [1/50], Step [510/777], Loss: 0.7473\n",
      "Epoch [1/50], Step [520/777], Loss: 0.2395\n",
      "Epoch [1/50], Step [530/777], Loss: 0.7489\n",
      "Epoch [1/50], Step [540/777], Loss: 0.7613\n",
      "Epoch [1/50], Step [550/777], Loss: 0.7998\n",
      "Epoch [1/50], Step [560/777], Loss: 0.3423\n",
      "Epoch [1/50], Step [570/777], Loss: 0.6816\n",
      "Epoch [1/50], Step [580/777], Loss: 0.2236\n",
      "Epoch [1/50], Step [590/777], Loss: 0.9298\n",
      "Epoch [1/50], Step [600/777], Loss: 0.5771\n",
      "Epoch [1/50], Step [610/777], Loss: 0.3139\n",
      "Epoch [1/50], Step [620/777], Loss: 0.3922\n",
      "Epoch [1/50], Step [630/777], Loss: 0.2317\n",
      "Epoch [1/50], Step [640/777], Loss: 0.4742\n",
      "Epoch [1/50], Step [650/777], Loss: 0.5633\n",
      "Epoch [1/50], Step [660/777], Loss: 0.4296\n",
      "Epoch [1/50], Step [670/777], Loss: 0.7104\n",
      "Epoch [1/50], Step [680/777], Loss: 0.3569\n",
      "Epoch [1/50], Step [690/777], Loss: 0.5812\n",
      "Epoch [1/50], Step [700/777], Loss: 0.6390\n",
      "Epoch [1/50], Step [710/777], Loss: 0.3264\n",
      "Epoch [1/50], Step [720/777], Loss: 0.6474\n",
      "Epoch [1/50], Step [730/777], Loss: 0.5788\n",
      "Epoch [1/50], Step [740/777], Loss: 0.2201\n",
      "Epoch [1/50], Step [750/777], Loss: 0.0808\n",
      "Epoch [1/50], Step [760/777], Loss: 0.1122\n",
      "Epoch [1/50], Step [770/777], Loss: 0.1706\n",
      "Epoch [1/50], Train Loss: 0.6988, Val Loss: 0.2863, Val Accuracy: 0.9245\n",
      "Model saved with validation accuracy: 0.9245\n",
      "Epoch [2/50], Step [10/777], Loss: 0.3113\n",
      "Epoch [2/50], Step [20/777], Loss: 0.1585\n",
      "Epoch [2/50], Step [30/777], Loss: 0.0640\n",
      "Epoch [2/50], Step [40/777], Loss: 0.1415\n",
      "Epoch [2/50], Step [50/777], Loss: 0.2103\n",
      "Epoch [2/50], Step [60/777], Loss: 0.1559\n",
      "Epoch [2/50], Step [70/777], Loss: 0.1297\n",
      "Epoch [2/50], Step [80/777], Loss: 0.1226\n",
      "Epoch [2/50], Step [90/777], Loss: 0.1601\n",
      "Epoch [2/50], Step [100/777], Loss: 0.3599\n",
      "Epoch [2/50], Step [110/777], Loss: 0.1062\n",
      "Epoch [2/50], Step [120/777], Loss: 0.5881\n",
      "Epoch [2/50], Step [130/777], Loss: 0.2409\n",
      "Epoch [2/50], Step [140/777], Loss: 0.1034\n",
      "Epoch [2/50], Step [150/777], Loss: 0.4484\n",
      "Epoch [2/50], Step [160/777], Loss: 0.1777\n",
      "Epoch [2/50], Step [170/777], Loss: 0.0780\n",
      "Epoch [2/50], Step [180/777], Loss: 0.1472\n",
      "Epoch [2/50], Step [190/777], Loss: 0.0732\n",
      "Epoch [2/50], Step [200/777], Loss: 0.2014\n",
      "Epoch [2/50], Step [210/777], Loss: 0.1688\n",
      "Epoch [2/50], Step [220/777], Loss: 0.1009\n",
      "Epoch [2/50], Step [230/777], Loss: 0.1999\n",
      "Epoch [2/50], Step [240/777], Loss: 0.4842\n",
      "Epoch [2/50], Step [250/777], Loss: 0.1196\n",
      "Epoch [2/50], Step [260/777], Loss: 0.3524\n",
      "Epoch [2/50], Step [270/777], Loss: 0.2263\n",
      "Epoch [2/50], Step [280/777], Loss: 0.6270\n",
      "Epoch [2/50], Step [290/777], Loss: 0.5072\n",
      "Epoch [2/50], Step [300/777], Loss: 0.1302\n",
      "Epoch [2/50], Step [310/777], Loss: 0.6185\n",
      "Epoch [2/50], Step [320/777], Loss: 0.3298\n",
      "Epoch [2/50], Step [330/777], Loss: 0.3298\n",
      "Epoch [2/50], Step [340/777], Loss: 0.1138\n",
      "Epoch [2/50], Step [350/777], Loss: 0.3451\n",
      "Epoch [2/50], Step [360/777], Loss: 0.0826\n",
      "Epoch [2/50], Step [370/777], Loss: 0.6260\n",
      "Epoch [2/50], Step [380/777], Loss: 0.1869\n",
      "Epoch [2/50], Step [390/777], Loss: 0.1741\n",
      "Epoch [2/50], Step [400/777], Loss: 0.4035\n",
      "Epoch [2/50], Step [410/777], Loss: 0.2547\n",
      "Epoch [2/50], Step [420/777], Loss: 0.1989\n",
      "Epoch [2/50], Step [430/777], Loss: 0.6685\n",
      "Epoch [2/50], Step [440/777], Loss: 0.3035\n",
      "Epoch [2/50], Step [450/777], Loss: 0.1797\n",
      "Epoch [2/50], Step [460/777], Loss: 0.0567\n",
      "Epoch [2/50], Step [470/777], Loss: 0.3977\n",
      "Epoch [2/50], Step [480/777], Loss: 0.2964\n",
      "Epoch [2/50], Step [490/777], Loss: 0.2781\n",
      "Epoch [2/50], Step [500/777], Loss: 0.9844\n",
      "Epoch [2/50], Step [510/777], Loss: 0.3744\n",
      "Epoch [2/50], Step [520/777], Loss: 0.0945\n",
      "Epoch [2/50], Step [530/777], Loss: 0.0535\n",
      "Epoch [2/50], Step [540/777], Loss: 0.0722\n",
      "Epoch [2/50], Step [550/777], Loss: 0.4813\n",
      "Epoch [2/50], Step [560/777], Loss: 0.1301\n",
      "Epoch [2/50], Step [570/777], Loss: 0.7577\n",
      "Epoch [2/50], Step [580/777], Loss: 0.2436\n",
      "Epoch [2/50], Step [590/777], Loss: 0.4027\n",
      "Epoch [2/50], Step [600/777], Loss: 0.0414\n",
      "Epoch [2/50], Step [610/777], Loss: 0.3155\n",
      "Epoch [2/50], Step [620/777], Loss: 0.4847\n",
      "Epoch [2/50], Step [630/777], Loss: 0.2835\n",
      "Epoch [2/50], Step [640/777], Loss: 0.0243\n",
      "Epoch [2/50], Step [650/777], Loss: 0.1102\n",
      "Epoch [2/50], Step [660/777], Loss: 0.3676\n",
      "Epoch [2/50], Step [670/777], Loss: 0.4687\n",
      "Epoch [2/50], Step [680/777], Loss: 0.3625\n",
      "Epoch [2/50], Step [690/777], Loss: 0.1955\n",
      "Epoch [2/50], Step [700/777], Loss: 0.0584\n",
      "Epoch [2/50], Step [710/777], Loss: 0.4415\n",
      "Epoch [2/50], Step [720/777], Loss: 0.1951\n",
      "Epoch [2/50], Step [730/777], Loss: 0.0446\n",
      "Epoch [2/50], Step [740/777], Loss: 0.5436\n",
      "Epoch [2/50], Step [750/777], Loss: 0.1759\n",
      "Epoch [2/50], Step [760/777], Loss: 0.1744\n",
      "Epoch [2/50], Step [770/777], Loss: 0.1198\n",
      "Epoch [2/50], Train Loss: 0.3172, Val Loss: 0.2259, Val Accuracy: 0.9384\n",
      "Model saved with validation accuracy: 0.9384\n",
      "Epoch [3/50], Step [10/777], Loss: 0.7722\n",
      "Epoch [3/50], Step [20/777], Loss: 0.2579\n",
      "Epoch [3/50], Step [30/777], Loss: 0.1743\n",
      "Epoch [3/50], Step [40/777], Loss: 0.0659\n",
      "Epoch [3/50], Step [50/777], Loss: 0.4983\n",
      "Epoch [3/50], Step [60/777], Loss: 0.4997\n",
      "Epoch [3/50], Step [70/777], Loss: 0.0804\n",
      "Epoch [3/50], Step [80/777], Loss: 0.0981\n",
      "Epoch [3/50], Step [90/777], Loss: 0.2909\n",
      "Epoch [3/50], Step [100/777], Loss: 0.4071\n",
      "Epoch [3/50], Step [110/777], Loss: 0.0515\n",
      "Epoch [3/50], Step [120/777], Loss: 0.0474\n",
      "Epoch [3/50], Step [130/777], Loss: 0.3146\n",
      "Epoch [3/50], Step [140/777], Loss: 0.3113\n",
      "Epoch [3/50], Step [150/777], Loss: 0.4052\n",
      "Epoch [3/50], Step [160/777], Loss: 0.1385\n",
      "Epoch [3/50], Step [170/777], Loss: 0.4805\n",
      "Epoch [3/50], Step [180/777], Loss: 0.3324\n",
      "Epoch [3/50], Step [190/777], Loss: 0.2996\n",
      "Epoch [3/50], Step [200/777], Loss: 0.1948\n",
      "Epoch [3/50], Step [210/777], Loss: 0.0470\n",
      "Epoch [3/50], Step [220/777], Loss: 0.2140\n",
      "Epoch [3/50], Step [230/777], Loss: 0.1643\n",
      "Epoch [3/50], Step [240/777], Loss: 0.0361\n",
      "Epoch [3/50], Step [250/777], Loss: 0.3035\n",
      "Epoch [3/50], Step [260/777], Loss: 0.1382\n",
      "Epoch [3/50], Step [270/777], Loss: 0.0960\n",
      "Epoch [3/50], Step [280/777], Loss: 0.2093\n",
      "Epoch [3/50], Step [290/777], Loss: 0.2677\n",
      "Epoch [3/50], Step [300/777], Loss: 0.7393\n",
      "Epoch [3/50], Step [310/777], Loss: 0.3985\n",
      "Epoch [3/50], Step [320/777], Loss: 0.0682\n",
      "Epoch [3/50], Step [330/777], Loss: 0.1377\n",
      "Epoch [3/50], Step [340/777], Loss: 0.2385\n",
      "Epoch [3/50], Step [350/777], Loss: 0.2041\n",
      "Epoch [3/50], Step [360/777], Loss: 0.0198\n",
      "Epoch [3/50], Step [370/777], Loss: 0.3656\n",
      "Epoch [3/50], Step [380/777], Loss: 0.3169\n",
      "Epoch [3/50], Step [390/777], Loss: 0.1781\n",
      "Epoch [3/50], Step [400/777], Loss: 0.1655\n",
      "Epoch [3/50], Step [410/777], Loss: 0.5250\n",
      "Epoch [3/50], Step [420/777], Loss: 0.2994\n",
      "Epoch [3/50], Step [430/777], Loss: 0.1776\n",
      "Epoch [3/50], Step [440/777], Loss: 0.1458\n",
      "Epoch [3/50], Step [450/777], Loss: 0.3765\n",
      "Epoch [3/50], Step [460/777], Loss: 0.0329\n",
      "Epoch [3/50], Step [470/777], Loss: 0.3325\n",
      "Epoch [3/50], Step [480/777], Loss: 0.3911\n",
      "Epoch [3/50], Step [490/777], Loss: 0.2392\n",
      "Epoch [3/50], Step [500/777], Loss: 0.1526\n",
      "Epoch [3/50], Step [510/777], Loss: 0.4345\n",
      "Epoch [3/50], Step [520/777], Loss: 0.6360\n",
      "Epoch [3/50], Step [530/777], Loss: 0.3826\n",
      "Epoch [3/50], Step [540/777], Loss: 0.2717\n",
      "Epoch [3/50], Step [550/777], Loss: 0.4069\n",
      "Epoch [3/50], Step [560/777], Loss: 0.1894\n",
      "Epoch [3/50], Step [570/777], Loss: 0.4725\n",
      "Epoch [3/50], Step [580/777], Loss: 0.0884\n",
      "Epoch [3/50], Step [590/777], Loss: 0.3153\n",
      "Epoch [3/50], Step [600/777], Loss: 0.5052\n",
      "Epoch [3/50], Step [610/777], Loss: 0.3159\n",
      "Epoch [3/50], Step [620/777], Loss: 0.1203\n",
      "Epoch [3/50], Step [630/777], Loss: 0.1138\n",
      "Epoch [3/50], Step [640/777], Loss: 0.3180\n",
      "Epoch [3/50], Step [650/777], Loss: 0.5541\n",
      "Epoch [3/50], Step [660/777], Loss: 0.0399\n",
      "Epoch [3/50], Step [670/777], Loss: 0.3704\n",
      "Epoch [3/50], Step [680/777], Loss: 0.1092\n",
      "Epoch [3/50], Step [690/777], Loss: 0.4410\n",
      "Epoch [3/50], Step [700/777], Loss: 0.1778\n",
      "Epoch [3/50], Step [710/777], Loss: 0.0231\n",
      "Epoch [3/50], Step [720/777], Loss: 0.4955\n",
      "Epoch [3/50], Step [730/777], Loss: 0.1419\n",
      "Epoch [3/50], Step [740/777], Loss: 0.2559\n",
      "Epoch [3/50], Step [750/777], Loss: 0.4659\n",
      "Epoch [3/50], Step [760/777], Loss: 0.0325\n",
      "Epoch [3/50], Step [770/777], Loss: 0.0882\n",
      "Epoch [3/50], Train Loss: 0.2351, Val Loss: 0.2156, Val Accuracy: 0.9474\n",
      "Model saved with validation accuracy: 0.9474\n",
      "Epoch [4/50], Step [10/777], Loss: 0.0429\n",
      "Epoch [4/50], Step [20/777], Loss: 0.1043\n",
      "Epoch [4/50], Step [30/777], Loss: 0.2182\n",
      "Epoch [4/50], Step [40/777], Loss: 0.1378\n",
      "Epoch [4/50], Step [50/777], Loss: 0.0233\n",
      "Epoch [4/50], Step [60/777], Loss: 0.0377\n",
      "Epoch [4/50], Step [70/777], Loss: 0.0265\n",
      "Epoch [4/50], Step [80/777], Loss: 0.0391\n",
      "Epoch [4/50], Step [90/777], Loss: 0.1001\n",
      "Epoch [4/50], Step [100/777], Loss: 0.0517\n",
      "Epoch [4/50], Step [110/777], Loss: 0.2133\n",
      "Epoch [4/50], Step [120/777], Loss: 0.2187\n",
      "Epoch [4/50], Step [130/777], Loss: 0.0372\n",
      "Epoch [4/50], Step [140/777], Loss: 0.0286\n",
      "Epoch [4/50], Step [150/777], Loss: 0.0719\n",
      "Epoch [4/50], Step [160/777], Loss: 0.0385\n",
      "Epoch [4/50], Step [170/777], Loss: 0.0714\n",
      "Epoch [4/50], Step [180/777], Loss: 0.6180\n",
      "Epoch [4/50], Step [190/777], Loss: 0.0883\n",
      "Epoch [4/50], Step [200/777], Loss: 0.1295\n",
      "Epoch [4/50], Step [210/777], Loss: 0.0764\n",
      "Epoch [4/50], Step [220/777], Loss: 0.2740\n",
      "Epoch [4/50], Step [230/777], Loss: 0.1263\n",
      "Epoch [4/50], Step [240/777], Loss: 0.2000\n",
      "Epoch [4/50], Step [250/777], Loss: 0.1523\n",
      "Epoch [4/50], Step [260/777], Loss: 0.0287\n",
      "Epoch [4/50], Step [270/777], Loss: 0.4100\n",
      "Epoch [4/50], Step [280/777], Loss: 0.0330\n",
      "Epoch [4/50], Step [290/777], Loss: 0.0642\n",
      "Epoch [4/50], Step [300/777], Loss: 0.0784\n",
      "Epoch [4/50], Step [310/777], Loss: 0.1402\n",
      "Epoch [4/50], Step [320/777], Loss: 0.2655\n",
      "Epoch [4/50], Step [330/777], Loss: 0.4146\n",
      "Epoch [4/50], Step [340/777], Loss: 0.2701\n",
      "Epoch [4/50], Step [350/777], Loss: 0.2846\n",
      "Epoch [4/50], Step [360/777], Loss: 0.0613\n",
      "Epoch [4/50], Step [370/777], Loss: 0.0430\n",
      "Epoch [4/50], Step [380/777], Loss: 0.1090\n",
      "Epoch [4/50], Step [390/777], Loss: 0.5862\n",
      "Epoch [4/50], Step [400/777], Loss: 0.1104\n",
      "Epoch [4/50], Step [410/777], Loss: 0.3243\n",
      "Epoch [4/50], Step [420/777], Loss: 0.0222\n",
      "Epoch [4/50], Step [430/777], Loss: 0.2100\n",
      "Epoch [4/50], Step [440/777], Loss: 0.2864\n",
      "Epoch [4/50], Step [450/777], Loss: 0.2596\n",
      "Epoch [4/50], Step [460/777], Loss: 0.4404\n",
      "Epoch [4/50], Step [470/777], Loss: 0.2272\n",
      "Epoch [4/50], Step [480/777], Loss: 0.0468\n",
      "Epoch [4/50], Step [490/777], Loss: 0.2678\n",
      "Epoch [4/50], Step [500/777], Loss: 0.3370\n",
      "Epoch [4/50], Step [510/777], Loss: 0.0748\n",
      "Epoch [4/50], Step [520/777], Loss: 0.0467\n",
      "Epoch [4/50], Step [530/777], Loss: 0.2666\n",
      "Epoch [4/50], Step [540/777], Loss: 0.0807\n",
      "Epoch [4/50], Step [550/777], Loss: 0.0683\n",
      "Epoch [4/50], Step [560/777], Loss: 0.1708\n",
      "Epoch [4/50], Step [570/777], Loss: 0.2167\n",
      "Epoch [4/50], Step [580/777], Loss: 0.5379\n",
      "Epoch [4/50], Step [590/777], Loss: 0.0408\n",
      "Epoch [4/50], Step [600/777], Loss: 0.3492\n",
      "Epoch [4/50], Step [610/777], Loss: 0.0251\n",
      "Epoch [4/50], Step [620/777], Loss: 0.2059\n",
      "Epoch [4/50], Step [630/777], Loss: 0.5679\n",
      "Epoch [4/50], Step [640/777], Loss: 0.0568\n",
      "Epoch [4/50], Step [650/777], Loss: 0.0183\n",
      "Epoch [4/50], Step [660/777], Loss: 0.0979\n",
      "Epoch [4/50], Step [670/777], Loss: 0.4964\n",
      "Epoch [4/50], Step [680/777], Loss: 0.1898\n",
      "Epoch [4/50], Step [690/777], Loss: 0.1279\n",
      "Epoch [4/50], Step [700/777], Loss: 0.2091\n",
      "Epoch [4/50], Step [710/777], Loss: 0.0586\n",
      "Epoch [4/50], Step [720/777], Loss: 0.2241\n",
      "Epoch [4/50], Step [730/777], Loss: 0.8322\n",
      "Epoch [4/50], Step [740/777], Loss: 0.2397\n",
      "Epoch [4/50], Step [750/777], Loss: 0.5698\n",
      "Epoch [4/50], Step [760/777], Loss: 0.0650\n",
      "Epoch [4/50], Step [770/777], Loss: 0.0845\n",
      "Epoch [4/50], Train Loss: 0.1930, Val Loss: 0.2087, Val Accuracy: 0.9448\n",
      "Epoch [5/50], Step [10/777], Loss: 0.2535\n",
      "Epoch [5/50], Step [20/777], Loss: 0.0525\n",
      "Epoch [5/50], Step [30/777], Loss: 0.0151\n",
      "Epoch [5/50], Step [40/777], Loss: 0.1573\n",
      "Epoch [5/50], Step [50/777], Loss: 0.3263\n",
      "Epoch [5/50], Step [60/777], Loss: 0.0302\n",
      "Epoch [5/50], Step [70/777], Loss: 0.0419\n",
      "Epoch [5/50], Step [80/777], Loss: 0.0139\n",
      "Epoch [5/50], Step [90/777], Loss: 0.1153\n",
      "Epoch [5/50], Step [100/777], Loss: 0.2896\n",
      "Epoch [5/50], Step [110/777], Loss: 0.1248\n",
      "Epoch [5/50], Step [120/777], Loss: 0.0627\n",
      "Epoch [5/50], Step [130/777], Loss: 0.2451\n",
      "Epoch [5/50], Step [140/777], Loss: 0.0607\n",
      "Epoch [5/50], Step [150/777], Loss: 0.0965\n",
      "Epoch [5/50], Step [160/777], Loss: 0.1131\n",
      "Epoch [5/50], Step [170/777], Loss: 0.1102\n",
      "Epoch [5/50], Step [180/777], Loss: 0.0571\n",
      "Epoch [5/50], Step [190/777], Loss: 0.0206\n",
      "Epoch [5/50], Step [200/777], Loss: 0.0529\n",
      "Epoch [5/50], Step [210/777], Loss: 0.0476\n",
      "Epoch [5/50], Step [220/777], Loss: 0.1329\n",
      "Epoch [5/50], Step [230/777], Loss: 0.1293\n",
      "Epoch [5/50], Step [240/777], Loss: 0.1443\n",
      "Epoch [5/50], Step [250/777], Loss: 0.0353\n",
      "Epoch [5/50], Step [260/777], Loss: 0.0486\n",
      "Epoch [5/50], Step [270/777], Loss: 0.0135\n",
      "Epoch [5/50], Step [280/777], Loss: 0.0475\n",
      "Epoch [5/50], Step [290/777], Loss: 0.0751\n",
      "Epoch [5/50], Step [300/777], Loss: 0.4319\n",
      "Epoch [5/50], Step [310/777], Loss: 0.1028\n",
      "Epoch [5/50], Step [320/777], Loss: 0.3526\n",
      "Epoch [5/50], Step [330/777], Loss: 0.1020\n",
      "Epoch [5/50], Step [340/777], Loss: 0.0936\n",
      "Epoch [5/50], Step [350/777], Loss: 0.0662\n",
      "Epoch [5/50], Step [360/777], Loss: 0.1286\n",
      "Epoch [5/50], Step [370/777], Loss: 0.0470\n",
      "Epoch [5/50], Step [380/777], Loss: 0.8652\n",
      "Epoch [5/50], Step [390/777], Loss: 0.0457\n",
      "Epoch [5/50], Step [400/777], Loss: 0.1602\n",
      "Epoch [5/50], Step [410/777], Loss: 0.0554\n",
      "Epoch [5/50], Step [420/777], Loss: 0.2143\n",
      "Epoch [5/50], Step [430/777], Loss: 0.0142\n",
      "Epoch [5/50], Step [440/777], Loss: 0.3183\n",
      "Epoch [5/50], Step [450/777], Loss: 0.0320\n",
      "Epoch [5/50], Step [460/777], Loss: 0.4296\n",
      "Epoch [5/50], Step [470/777], Loss: 0.0338\n",
      "Epoch [5/50], Step [480/777], Loss: 0.5681\n",
      "Epoch [5/50], Step [490/777], Loss: 0.1784\n",
      "Epoch [5/50], Step [500/777], Loss: 0.0170\n",
      "Epoch [5/50], Step [510/777], Loss: 0.0387\n",
      "Epoch [5/50], Step [520/777], Loss: 0.0508\n",
      "Epoch [5/50], Step [530/777], Loss: 0.2405\n",
      "Epoch [5/50], Step [540/777], Loss: 0.0722\n",
      "Epoch [5/50], Step [550/777], Loss: 0.1229\n",
      "Epoch [5/50], Step [560/777], Loss: 0.2894\n",
      "Epoch [5/50], Step [570/777], Loss: 0.2320\n",
      "Epoch [5/50], Step [580/777], Loss: 0.0355\n",
      "Epoch [5/50], Step [590/777], Loss: 0.4412\n",
      "Epoch [5/50], Step [600/777], Loss: 0.1414\n",
      "Epoch [5/50], Step [610/777], Loss: 0.2793\n",
      "Epoch [5/50], Step [620/777], Loss: 0.0189\n",
      "Epoch [5/50], Step [630/777], Loss: 0.3278\n",
      "Epoch [5/50], Step [640/777], Loss: 0.0437\n",
      "Epoch [5/50], Step [650/777], Loss: 0.1858\n",
      "Epoch [5/50], Step [660/777], Loss: 0.0172\n",
      "Epoch [5/50], Step [670/777], Loss: 0.0321\n",
      "Epoch [5/50], Step [680/777], Loss: 0.0124\n",
      "Epoch [5/50], Step [690/777], Loss: 0.6366\n",
      "Epoch [5/50], Step [700/777], Loss: 0.1401\n",
      "Epoch [5/50], Step [710/777], Loss: 0.1331\n",
      "Epoch [5/50], Step [720/777], Loss: 0.1226\n",
      "Epoch [5/50], Step [730/777], Loss: 0.2301\n",
      "Epoch [5/50], Step [740/777], Loss: 0.0288\n",
      "Epoch [5/50], Step [750/777], Loss: 0.0217\n",
      "Epoch [5/50], Step [760/777], Loss: 0.2348\n",
      "Epoch [5/50], Step [770/777], Loss: 0.1901\n",
      "Epoch [5/50], Train Loss: 0.1596, Val Loss: 0.2148, Val Accuracy: 0.9455\n",
      "Epoch [6/50], Step [10/777], Loss: 0.0582\n",
      "Epoch [6/50], Step [20/777], Loss: 0.0347\n",
      "Epoch [6/50], Step [30/777], Loss: 0.0768\n",
      "Epoch [6/50], Step [40/777], Loss: 0.1194\n",
      "Epoch [6/50], Step [50/777], Loss: 0.3121\n",
      "Epoch [6/50], Step [60/777], Loss: 0.0964\n",
      "Epoch [6/50], Step [70/777], Loss: 0.0280\n",
      "Epoch [6/50], Step [80/777], Loss: 0.0351\n",
      "Epoch [6/50], Step [90/777], Loss: 0.0507\n",
      "Epoch [6/50], Step [100/777], Loss: 0.2895\n",
      "Epoch [6/50], Step [110/777], Loss: 0.0524\n",
      "Epoch [6/50], Step [120/777], Loss: 0.0886\n",
      "Epoch [6/50], Step [130/777], Loss: 0.1015\n",
      "Epoch [6/50], Step [140/777], Loss: 0.0218\n",
      "Epoch [6/50], Step [150/777], Loss: 0.1517\n",
      "Epoch [6/50], Step [160/777], Loss: 0.1198\n",
      "Epoch [6/50], Step [170/777], Loss: 0.0316\n",
      "Epoch [6/50], Step [180/777], Loss: 0.1223\n",
      "Epoch [6/50], Step [190/777], Loss: 0.0224\n",
      "Epoch [6/50], Step [200/777], Loss: 0.0286\n",
      "Epoch [6/50], Step [210/777], Loss: 0.0179\n",
      "Epoch [6/50], Step [220/777], Loss: 0.2478\n",
      "Epoch [6/50], Step [230/777], Loss: 0.0327\n",
      "Epoch [6/50], Step [240/777], Loss: 0.0440\n",
      "Epoch [6/50], Step [250/777], Loss: 0.0314\n",
      "Epoch [6/50], Step [260/777], Loss: 0.0596\n",
      "Epoch [6/50], Step [270/777], Loss: 0.1962\n",
      "Epoch [6/50], Step [280/777], Loss: 0.0971\n",
      "Epoch [6/50], Step [290/777], Loss: 0.2105\n",
      "Epoch [6/50], Step [300/777], Loss: 0.0897\n",
      "Epoch [6/50], Step [310/777], Loss: 0.0419\n",
      "Epoch [6/50], Step [320/777], Loss: 0.2619\n",
      "Epoch [6/50], Step [330/777], Loss: 0.2218\n",
      "Epoch [6/50], Step [340/777], Loss: 0.2245\n",
      "Epoch [6/50], Step [350/777], Loss: 0.0117\n",
      "Epoch [6/50], Step [360/777], Loss: 0.1827\n",
      "Epoch [6/50], Step [370/777], Loss: 0.0180\n",
      "Epoch [6/50], Step [380/777], Loss: 0.2968\n",
      "Epoch [6/50], Step [390/777], Loss: 0.1909\n",
      "Epoch [6/50], Step [400/777], Loss: 0.0956\n",
      "Epoch [6/50], Step [410/777], Loss: 0.0826\n",
      "Epoch [6/50], Step [420/777], Loss: 0.1531\n",
      "Epoch [6/50], Step [430/777], Loss: 0.1602\n",
      "Epoch [6/50], Step [440/777], Loss: 0.0352\n",
      "Epoch [6/50], Step [450/777], Loss: 0.2096\n",
      "Epoch [6/50], Step [460/777], Loss: 0.0555\n",
      "Epoch [6/50], Step [470/777], Loss: 0.0113\n",
      "Epoch [6/50], Step [480/777], Loss: 0.1312\n",
      "Epoch [6/50], Step [490/777], Loss: 0.1183\n",
      "Epoch [6/50], Step [500/777], Loss: 0.3607\n",
      "Epoch [6/50], Step [510/777], Loss: 0.0385\n",
      "Epoch [6/50], Step [520/777], Loss: 0.1395\n",
      "Epoch [6/50], Step [530/777], Loss: 0.0193\n",
      "Epoch [6/50], Step [540/777], Loss: 0.1545\n",
      "Epoch [6/50], Step [550/777], Loss: 0.0095\n",
      "Epoch [6/50], Step [560/777], Loss: 0.0295\n",
      "Epoch [6/50], Step [570/777], Loss: 0.0477\n",
      "Epoch [6/50], Step [580/777], Loss: 0.0127\n",
      "Epoch [6/50], Step [590/777], Loss: 0.0766\n",
      "Epoch [6/50], Step [600/777], Loss: 0.0216\n",
      "Epoch [6/50], Step [610/777], Loss: 0.0062\n",
      "Epoch [6/50], Step [620/777], Loss: 0.2398\n",
      "Epoch [6/50], Step [630/777], Loss: 0.0492\n",
      "Epoch [6/50], Step [640/777], Loss: 0.0822\n",
      "Epoch [6/50], Step [650/777], Loss: 0.0487\n",
      "Epoch [6/50], Step [660/777], Loss: 0.1353\n",
      "Epoch [6/50], Step [670/777], Loss: 0.1797\n",
      "Epoch [6/50], Step [680/777], Loss: 0.1585\n",
      "Epoch [6/50], Step [690/777], Loss: 0.0691\n",
      "Epoch [6/50], Step [700/777], Loss: 0.0177\n",
      "Epoch [6/50], Step [710/777], Loss: 0.0432\n",
      "Epoch [6/50], Step [720/777], Loss: 0.1795\n",
      "Epoch [6/50], Step [730/777], Loss: 0.1719\n",
      "Epoch [6/50], Step [740/777], Loss: 0.0239\n",
      "Epoch [6/50], Step [750/777], Loss: 0.3043\n",
      "Epoch [6/50], Step [760/777], Loss: 0.0619\n",
      "Epoch [6/50], Step [770/777], Loss: 0.0912\n",
      "Epoch [6/50], Train Loss: 0.1288, Val Loss: 0.2361, Val Accuracy: 0.9459\n",
      "Epoch [7/50], Step [10/777], Loss: 0.1729\n",
      "Epoch [7/50], Step [20/777], Loss: 0.0534\n",
      "Epoch [7/50], Step [30/777], Loss: 0.0240\n",
      "Epoch [7/50], Step [40/777], Loss: 0.0259\n",
      "Epoch [7/50], Step [50/777], Loss: 0.0150\n",
      "Epoch [7/50], Step [60/777], Loss: 0.2250\n",
      "Epoch [7/50], Step [70/777], Loss: 0.0952\n",
      "Epoch [7/50], Step [80/777], Loss: 0.1929\n",
      "Epoch [7/50], Step [90/777], Loss: 0.0456\n",
      "Epoch [7/50], Step [100/777], Loss: 0.0514\n",
      "Epoch [7/50], Step [110/777], Loss: 0.2632\n",
      "Epoch [7/50], Step [120/777], Loss: 0.1088\n",
      "Epoch [7/50], Step [130/777], Loss: 0.0108\n",
      "Epoch [7/50], Step [140/777], Loss: 0.0144\n",
      "Epoch [7/50], Step [150/777], Loss: 0.2341\n",
      "Epoch [7/50], Step [160/777], Loss: 0.0545\n",
      "Epoch [7/50], Step [170/777], Loss: 0.0234\n",
      "Epoch [7/50], Step [180/777], Loss: 0.0164\n",
      "Epoch [7/50], Step [190/777], Loss: 0.0551\n",
      "Epoch [7/50], Step [200/777], Loss: 0.2191\n",
      "Epoch [7/50], Step [210/777], Loss: 0.0028\n",
      "Epoch [7/50], Step [220/777], Loss: 0.0642\n",
      "Epoch [7/50], Step [230/777], Loss: 0.0182\n",
      "Epoch [7/50], Step [240/777], Loss: 0.3457\n",
      "Epoch [7/50], Step [250/777], Loss: 0.0120\n",
      "Epoch [7/50], Step [260/777], Loss: 0.0425\n",
      "Epoch [7/50], Step [270/777], Loss: 0.0115\n",
      "Epoch [7/50], Step [280/777], Loss: 0.0132\n",
      "Epoch [7/50], Step [290/777], Loss: 0.0077\n",
      "Epoch [7/50], Step [300/777], Loss: 0.0074\n",
      "Epoch [7/50], Step [310/777], Loss: 0.0060\n",
      "Epoch [7/50], Step [320/777], Loss: 0.0172\n",
      "Epoch [7/50], Step [330/777], Loss: 0.3259\n",
      "Epoch [7/50], Step [340/777], Loss: 0.1475\n",
      "Epoch [7/50], Step [350/777], Loss: 0.0220\n",
      "Epoch [7/50], Step [360/777], Loss: 0.2629\n",
      "Epoch [7/50], Step [370/777], Loss: 0.0172\n",
      "Epoch [7/50], Step [380/777], Loss: 0.0287\n",
      "Epoch [7/50], Step [390/777], Loss: 0.0499\n",
      "Epoch [7/50], Step [400/777], Loss: 0.0157\n",
      "Epoch [7/50], Step [410/777], Loss: 0.2056\n",
      "Epoch [7/50], Step [420/777], Loss: 0.0046\n",
      "Epoch [7/50], Step [430/777], Loss: 0.1589\n",
      "Epoch [7/50], Step [440/777], Loss: 0.3262\n",
      "Epoch [7/50], Step [450/777], Loss: 0.0635\n",
      "Epoch [7/50], Step [460/777], Loss: 0.0498\n",
      "Epoch [7/50], Step [470/777], Loss: 0.0788\n",
      "Epoch [7/50], Step [480/777], Loss: 0.0422\n",
      "Epoch [7/50], Step [490/777], Loss: 0.0155\n",
      "Epoch [7/50], Step [500/777], Loss: 0.1197\n",
      "Epoch [7/50], Step [510/777], Loss: 0.0709\n",
      "Epoch [7/50], Step [520/777], Loss: 0.0975\n",
      "Epoch [7/50], Step [530/777], Loss: 0.0606\n",
      "Epoch [7/50], Step [540/777], Loss: 0.0569\n",
      "Epoch [7/50], Step [550/777], Loss: 0.2130\n",
      "Epoch [7/50], Step [560/777], Loss: 0.0181\n",
      "Epoch [7/50], Step [570/777], Loss: 0.0238\n",
      "Epoch [7/50], Step [580/777], Loss: 0.6901\n",
      "Epoch [7/50], Step [590/777], Loss: 0.0766\n",
      "Epoch [7/50], Step [600/777], Loss: 0.0602\n",
      "Epoch [7/50], Step [610/777], Loss: 0.0216\n",
      "Epoch [7/50], Step [620/777], Loss: 0.0546\n",
      "Epoch [7/50], Step [630/777], Loss: 0.1960\n",
      "Epoch [7/50], Step [640/777], Loss: 0.0203\n",
      "Epoch [7/50], Step [650/777], Loss: 0.0321\n",
      "Epoch [7/50], Step [660/777], Loss: 0.0944\n",
      "Epoch [7/50], Step [670/777], Loss: 0.3163\n",
      "Epoch [7/50], Step [680/777], Loss: 0.0534\n",
      "Epoch [7/50], Step [690/777], Loss: 0.0921\n",
      "Epoch [7/50], Step [700/777], Loss: 0.0080\n",
      "Epoch [7/50], Step [710/777], Loss: 0.1199\n",
      "Epoch [7/50], Step [720/777], Loss: 0.0191\n",
      "Epoch [7/50], Step [730/777], Loss: 0.0617\n",
      "Epoch [7/50], Step [740/777], Loss: 0.0960\n",
      "Epoch [7/50], Step [750/777], Loss: 0.3338\n",
      "Epoch [7/50], Step [760/777], Loss: 0.2937\n",
      "Epoch [7/50], Step [770/777], Loss: 0.0335\n",
      "Epoch [7/50], Train Loss: 0.1034, Val Loss: 0.2570, Val Accuracy: 0.9448\n",
      "Epoch [8/50], Step [10/777], Loss: 0.0343\n",
      "Epoch [8/50], Step [20/777], Loss: 0.0658\n",
      "Epoch [8/50], Step [30/777], Loss: 0.0130\n",
      "Epoch [8/50], Step [40/777], Loss: 0.0729\n",
      "Epoch [8/50], Step [50/777], Loss: 0.1327\n",
      "Epoch [8/50], Step [60/777], Loss: 0.0110\n",
      "Epoch [8/50], Step [70/777], Loss: 0.0307\n",
      "Epoch [8/50], Step [80/777], Loss: 0.0291\n",
      "Epoch [8/50], Step [90/777], Loss: 0.0883\n",
      "Epoch [8/50], Step [100/777], Loss: 0.0286\n",
      "Epoch [8/50], Step [110/777], Loss: 0.2865\n",
      "Epoch [8/50], Step [120/777], Loss: 0.2751\n",
      "Epoch [8/50], Step [130/777], Loss: 0.1490\n",
      "Epoch [8/50], Step [140/777], Loss: 0.0194\n",
      "Epoch [8/50], Step [150/777], Loss: 0.0331\n",
      "Epoch [8/50], Step [160/777], Loss: 0.0870\n",
      "Epoch [8/50], Step [170/777], Loss: 0.2001\n",
      "Epoch [8/50], Step [180/777], Loss: 0.0409\n",
      "Epoch [8/50], Step [190/777], Loss: 0.0799\n",
      "Epoch [8/50], Step [200/777], Loss: 0.0060\n",
      "Epoch [8/50], Step [210/777], Loss: 0.0181\n",
      "Epoch [8/50], Step [220/777], Loss: 0.0159\n",
      "Epoch [8/50], Step [230/777], Loss: 0.1589\n",
      "Epoch [8/50], Step [240/777], Loss: 0.0780\n",
      "Epoch [8/50], Step [250/777], Loss: 0.0230\n",
      "Epoch [8/50], Step [260/777], Loss: 0.0582\n",
      "Epoch [8/50], Step [270/777], Loss: 0.0534\n",
      "Epoch [8/50], Step [280/777], Loss: 0.0141\n",
      "Epoch [8/50], Step [290/777], Loss: 0.1179\n",
      "Epoch [8/50], Step [300/777], Loss: 0.0335\n",
      "Epoch [8/50], Step [310/777], Loss: 0.1965\n",
      "Epoch [8/50], Step [320/777], Loss: 0.0424\n",
      "Epoch [8/50], Step [330/777], Loss: 0.0027\n",
      "Epoch [8/50], Step [340/777], Loss: 0.0538\n",
      "Epoch [8/50], Step [350/777], Loss: 0.0871\n",
      "Epoch [8/50], Step [360/777], Loss: 0.0225\n",
      "Epoch [8/50], Step [370/777], Loss: 0.2050\n",
      "Epoch [8/50], Step [380/777], Loss: 0.1815\n",
      "Epoch [8/50], Step [390/777], Loss: 0.0173\n",
      "Epoch [8/50], Step [400/777], Loss: 0.0241\n",
      "Epoch [8/50], Step [410/777], Loss: 0.0230\n",
      "Epoch [8/50], Step [420/777], Loss: 0.0205\n",
      "Epoch [8/50], Step [430/777], Loss: 0.0143\n",
      "Epoch [8/50], Step [440/777], Loss: 0.0146\n",
      "Epoch [8/50], Step [450/777], Loss: 0.0206\n",
      "Epoch [8/50], Step [460/777], Loss: 0.0185\n",
      "Epoch [8/50], Step [470/777], Loss: 0.0169\n",
      "Epoch [8/50], Step [480/777], Loss: 0.2188\n",
      "Epoch [8/50], Step [490/777], Loss: 0.0524\n",
      "Epoch [8/50], Step [500/777], Loss: 0.0747\n",
      "Epoch [8/50], Step [510/777], Loss: 0.0601\n",
      "Epoch [8/50], Step [520/777], Loss: 0.0065\n",
      "Epoch [8/50], Step [530/777], Loss: 0.0165\n",
      "Epoch [8/50], Step [540/777], Loss: 0.2760\n",
      "Epoch [8/50], Step [550/777], Loss: 0.0131\n",
      "Epoch [8/50], Step [560/777], Loss: 0.0160\n",
      "Epoch [8/50], Step [570/777], Loss: 0.0668\n",
      "Epoch [8/50], Step [580/777], Loss: 0.1555\n",
      "Epoch [8/50], Step [590/777], Loss: 0.0700\n",
      "Epoch [8/50], Step [600/777], Loss: 0.1156\n",
      "Epoch [8/50], Step [610/777], Loss: 0.0384\n",
      "Epoch [8/50], Step [620/777], Loss: 0.0360\n",
      "Epoch [8/50], Step [630/777], Loss: 0.0220\n",
      "Epoch [8/50], Step [640/777], Loss: 0.0467\n",
      "Epoch [8/50], Step [650/777], Loss: 0.0173\n",
      "Epoch [8/50], Step [660/777], Loss: 0.2187\n",
      "Epoch [8/50], Step [670/777], Loss: 0.0779\n",
      "Epoch [8/50], Step [680/777], Loss: 0.0059\n",
      "Epoch [8/50], Step [690/777], Loss: 0.0729\n",
      "Epoch [8/50], Step [700/777], Loss: 0.0479\n",
      "Epoch [8/50], Step [710/777], Loss: 0.0225\n",
      "Epoch [8/50], Step [720/777], Loss: 0.3096\n",
      "Epoch [8/50], Step [730/777], Loss: 0.0491\n",
      "Epoch [8/50], Step [740/777], Loss: 0.0054\n",
      "Epoch [8/50], Step [750/777], Loss: 0.0495\n",
      "Epoch [8/50], Step [760/777], Loss: 0.1140\n",
      "Epoch [8/50], Step [770/777], Loss: 0.0156\n",
      "Epoch [8/50], Train Loss: 0.0777, Val Loss: 0.2747, Val Accuracy: 0.9361\n",
      "Early stopping triggered after 8 epochs\n",
      "\n",
      "Evaluating EfficientNet on test set...\n",
      "\n",
      "EfficientNet Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DJI       0.93      0.85      0.89       200\n",
      "   FutabaT14       0.95      0.85      0.90       548\n",
      "    FutabaT7       0.94      0.91      0.93        93\n",
      "    Graupner       0.94      0.98      0.96       107\n",
      "       Noise       0.92      0.98      0.95      1314\n",
      "     Taranis       1.00      0.97      0.98       268\n",
      "     Turnigy       0.98      0.92      0.95       133\n",
      "\n",
      "    accuracy                           0.94      2663\n",
      "   macro avg       0.95      0.92      0.94      2663\n",
      "weighted avg       0.94      0.94      0.94      2663\n",
      "\n",
      "\n",
      "EfficientNet Multi-class ROC AUC Score: 0.9875\n",
      "\n",
      "Analyzing EfficientNet performance by SNR levels...\n",
      "\n",
      "EfficientNet Performance by SNR level:\n",
      "SNR (dB) | Accuracy | Samples\n",
      "------------------------------\n",
      "\n",
      "EfficientNet Total Number of Parameters: 4,016,515\n",
      "EfficientNet Average Inference Time per Sample: 8.263 ms\n",
      "EfficientNet FLOPs: 413,873,984.0 (413.87 M)\n",
      "EfficientNet MACs: 4,016,515.0 (4.02 M)\n",
      "\n",
      "✅ EfficientNet Accuracy for class 'DJI': 85.00%\n",
      "✅ EfficientNet Accuracy for class 'FutabaT14': 85.04%\n",
      "✅ EfficientNet Accuracy for class 'FutabaT7': 91.40%\n",
      "✅ EfficientNet Accuracy for class 'Graupner': 98.13%\n",
      "✅ EfficientNet Accuracy for class 'Noise': 98.17%\n",
      "✅ EfficientNet Accuracy for class 'Taranis': 96.64%\n",
      "✅ EfficientNet Accuracy for class 'Turnigy': 91.73%\n",
      "\n",
      "✅ EfficientNet Test Set Accuracy: 0.938\n",
      "📊 EfficientNet Model Size: 15.32 MB\n",
      "\n",
      "Key characteristics of EfficientNet:\n",
      "- Uses compound scaling to balance network depth, width, and resolution\n",
      "- Achieves state-of-the-art accuracy with fewer parameters\n",
      "- Family of models with different size-accuracy tradeoffs (B0-B7)\n",
      "- Uses MBConv blocks with squeeze-and-excitation optimization\n",
      "\n",
      "Metrics saved to efficientnet_drone_rf_metrics.json\n",
      "\n",
      "\n",
      "All models have been trained and evaluated!\n",
      "Results have been saved to individual files for each model.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from torchsummary import summary\n",
    "from thop import profile  # For FLOPs and MACs calculation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "\n",
    "# Helper function for formatting large numbers\n",
    "def format_units(num):\n",
    "    \"\"\"Format large numbers with units (K, M, G, etc.)\"\"\"\n",
    "    magnitude = 0\n",
    "    while abs(num) >= 1000:\n",
    "        magnitude += 1\n",
    "        num /= 1000.0\n",
    "    return f\"{num:.2f} {['', 'K', 'M', 'G', 'T', 'P'][magnitude]}\"\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Parameters\n",
    "batch_size = 16\n",
    "img_size = 224  # Standard size for models\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-4\n",
    "split_ratio = [0.7, 0.15, 0.15]  # 70% training, 15% validation, 15% test\n",
    "\n",
    "# Dataset Directory\n",
    "dataset_dir = \"/kaggle/input/drone-data/clean_spectrograms\"\n",
    "\n",
    "# Data Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB if needed\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to create data loaders with proper dataset splits and consistent indices\n",
    "def create_data_loaders(full_dataset):\n",
    "    # Get a generator with fixed seed for consistent splits\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    \n",
    "    # Split into Train, Validation, and Test\n",
    "    train_size = int(split_ratio[0] * len(full_dataset))\n",
    "    val_size = int(split_ratio[1] * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        full_dataset, \n",
    "        [train_size, val_size, test_size],\n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Load Full Dataset\n",
    "full_dataset = datasets.ImageFolder(root=dataset_dir, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = create_data_loaders(full_dataset)\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Total number of samples: {len(full_dataset)}\")\n",
    "class_to_idx = full_dataset.class_to_idx\n",
    "print(\"Class to index mapping:\", class_to_idx)\n",
    "for class_name, idx in class_to_idx.items():\n",
    "    class_samples = len([x for x, y in full_dataset.samples if y == idx])\n",
    "    print(f\"Class {class_name}: {class_samples} samples\")\n",
    "\n",
    "# Number of Classes\n",
    "num_classes = len(full_dataset.classes)\n",
    "class_names = full_dataset.classes\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Class Names: {class_names}\")\n",
    "\n",
    "# Function to extract SNR from filename\n",
    "def extract_snr(filename):\n",
    "    try:\n",
    "        # Assuming filename format like \"sample_0_snr_-14.png\"\n",
    "        parts = os.path.basename(filename).split('_')\n",
    "        snr_idx = parts.index('snr') + 1\n",
    "        return int(parts[snr_idx])\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "# Define a function to train and evaluate a model\n",
    "def train_and_evaluate_model(model_name, model, train_loader, val_loader, test_loader, test_dataset, full_dataset):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training and Evaluating {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model summary\n",
    "    print(f\"\\n{model_name} Summary:\")\n",
    "    try:\n",
    "        summary(model, (3, img_size, img_size))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate detailed summary due to: {str(e)}\")\n",
    "        print(\"Continuing with training...\")\n",
    "    \n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "    \n",
    "    # Training Loop\n",
    "    best_val_acc = 0.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    early_stop_counter = 0\n",
    "    early_stop_patience = 5\n",
    "    best_model_weights = None\n",
    "    \n",
    "    print(f\"\\nStarting training {model_name}...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Print statistics (optional)\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), f\"{model_name.lower().replace('-', '_')}_drone_rf.pth\")\n",
    "            print(f\"Model saved with validation accuracy: {val_accuracy:.4f}\")\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'{model_name} - Loss Over Epochs')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(f'{model_name} - Accuracy Over Epochs')\n",
    "    plt.savefig(f\"{model_name.lower().replace('-', '_')}_drone_rf_training_history.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Load the best model for evaluation\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    model.eval()\n",
    "    \n",
    "    # Testing the Model\n",
    "    print(f\"\\nEvaluating {model_name} on test set...\")\n",
    "    y_true, y_pred, y_scores = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_scores.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
    "    \n",
    "    # Classification Report\n",
    "    cls_report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    plt.savefig(f\"{model_name.lower().replace('-', '_')}_drone_rf_confusion_matrix.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # ROC Curve (for multi-class classification)\n",
    "    roc_auc = None\n",
    "    if num_classes > 2:\n",
    "        y_true_bin = label_binarize(y_true, classes=np.arange(num_classes))\n",
    "        y_scores_array = np.array(y_scores)\n",
    "        \n",
    "        roc_auc = roc_auc_score(y_true_bin, y_scores_array, multi_class=\"ovr\")\n",
    "        print(f\"\\n{model_name} Multi-class ROC AUC Score: {roc_auc:.4f}\")\n",
    "    \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for i in range(num_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_scores_array[:, i])\n",
    "            auc_score = roc_auc_score(y_true_bin[:, i], y_scores_array[:, i])\n",
    "            plt.plot(fpr, tpr, label=f\"Class {class_names[i]} (AUC = {auc_score:.2f})\")\n",
    "    \n",
    "        plt.plot([0, 1], [0, 1], \"k--\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"{model_name} ROC Curve\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{model_name.lower().replace('-', '_')}_drone_rf_roc_curve.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # SNR-based performance analysis\n",
    "    print(f\"\\nAnalyzing {model_name} performance by SNR levels...\")\n",
    "    \n",
    "    # Create a dictionary to store predictions by SNR\n",
    "    snr_results = {}\n",
    "    \n",
    "    # Re-run through test dataset to get filenames and predictions\n",
    "    test_dataset_files = [full_dataset.samples[i][0] for i in test_dataset.indices]\n",
    "    test_dataset_labels = [full_dataset.samples[i][1] for i in test_dataset.indices]\n",
    "    \n",
    "    # Match predictions with SNR values\n",
    "    for i, (file_path, true_label) in enumerate(zip(test_dataset_files, test_dataset_labels)):\n",
    "        snr = extract_snr(file_path)\n",
    "        if snr is not None:\n",
    "            if snr not in snr_results:\n",
    "                snr_results[snr] = {'correct': 0, 'total': 0}\n",
    "            snr_results[snr]['total'] += 1\n",
    "            if y_pred[i] == y_true[i]:\n",
    "                snr_results[snr]['correct'] += 1\n",
    "    \n",
    "    # Calculate accuracy by SNR\n",
    "    snr_accuracy = {snr: results['correct'] / results['total'] \n",
    "                    for snr, results in snr_results.items() if results['total'] > 0}\n",
    "    \n",
    "    # Plot SNR vs. Accuracy\n",
    "    sorted_snrs = sorted(snr_accuracy.keys())\n",
    "    accuracies = [snr_accuracy[snr] for snr in sorted_snrs]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sorted_snrs, accuracies, 'o-')\n",
    "    plt.xlabel('Signal-to-Noise Ratio (dB)')\n",
    "    plt.ylabel('Classification Accuracy')\n",
    "    plt.title(f'{model_name} Performance vs. Signal-to-Noise Ratio')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{model_name.lower().replace('-', '_')}_drone_rf_snr_performance.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Print SNR performance table\n",
    "    print(f\"\\n{model_name} Performance by SNR level:\")\n",
    "    print(\"SNR (dB) | Accuracy | Samples\")\n",
    "    print(\"-\" * 30)\n",
    "    snr_table = []\n",
    "    for snr in sorted_snrs:\n",
    "        acc = snr_accuracy[snr]\n",
    "        samples = snr_results[snr]['total']\n",
    "        print(f\"{snr:7d} | {acc:.4f} | {samples}\")\n",
    "        snr_table.append({\"snr\": snr, \"accuracy\": acc, \"samples\": samples})\n",
    "    \n",
    "    # Inference Time Calculation\n",
    "    sample_input = torch.randn(1, 3, img_size, img_size).to(device)\n",
    "    num_samples = 100\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            _ = model(sample_input)\n",
    "    inference_time = (time.time() - start_time) / num_samples\n",
    "    \n",
    "    # Number of Parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # FLOPs & MACs Calculation\n",
    "    try:\n",
    "        flops, macs = profile(model, inputs=(sample_input,), verbose=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating FLOPs and MACs: {str(e)}\")\n",
    "        flops, macs = 0, 0\n",
    "    \n",
    "    # Convert inference time to milliseconds\n",
    "    inference_time_ms = inference_time * 1000\n",
    "    \n",
    "    print(f\"\\n{model_name} Total Number of Parameters: {num_params:,}\")\n",
    "    print(f\"{model_name} Average Inference Time per Sample: {inference_time_ms:.3f} ms\")\n",
    "    print(f\"{model_name} FLOPs: {flops:,} ({format_units(flops)})\")\n",
    "    print(f\"{model_name} MACs: {macs:,} ({format_units(macs)})\\n\")\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "    class_acc_dict = {}\n",
    "    for i, acc in enumerate(class_accuracy):\n",
    "        print(f\"✅ {model_name} Accuracy for class '{class_names[i]}': {acc:.2%}\")\n",
    "        class_acc_dict[class_names[i]] = float(acc)\n",
    "    \n",
    "    # Calculate and display test accuracy with 3 decimal places\n",
    "    test_correct = sum([1 for i, j in zip(y_true, y_pred) if i == j])\n",
    "    test_total = len(y_true)\n",
    "    test_accuracy = test_correct / test_total\n",
    "    print(f\"\\n✅ {model_name} Test Set Accuracy: {test_accuracy:.3f}\")\n",
    "    \n",
    "    # Calculate and display model size in MB\n",
    "    # Each parameter is typically stored as a 32-bit float (4 bytes)\n",
    "    model_size_bytes = num_params * 4\n",
    "    model_size_mb = model_size_bytes / (1024 * 1024)\n",
    "    print(f\"📊 {model_name} Model Size: {model_size_mb:.2f} MB\")\n",
    "    \n",
    "    # Model characteristics based on model name\n",
    "    characteristics = []\n",
    "    if model_name == \"SqueezeNet\":\n",
    "        characteristics = [\n",
    "            \"Extremely lightweight model (around 3MB)\",\n",
    "            \"Uses 'fire modules' with squeeze and expand layers\",\n",
    "            \"Achieves AlexNet-level accuracy with 50x fewer parameters\",\n",
    "            \"Ideal for resource-constrained environments like drones\"\n",
    "        ]\n",
    "    elif model_name == \"ShuffleNet\":\n",
    "        characteristics = [\n",
    "            \"Designed for mobile devices with limited computing power\",\n",
    "            \"Uses pointwise group convolutions and channel shuffle operations\",\n",
    "            \"Very computationally efficient with low FLOPs\",\n",
    "            \"Good balance between accuracy and model size\"\n",
    "        ]\n",
    "    elif model_name == \"EfficientNet\":\n",
    "        characteristics = [\n",
    "            \"Uses compound scaling to balance network depth, width, and resolution\",\n",
    "            \"Achieves state-of-the-art accuracy with fewer parameters\",\n",
    "            \"Family of models with different size-accuracy tradeoffs (B0-B7)\",\n",
    "            \"Uses MBConv blocks with squeeze-and-excitation optimization\"\n",
    "        ]\n",
    "    \n",
    "    # Print characteristics\n",
    "    print(f\"\\nKey characteristics of {model_name}:\")\n",
    "    for char in characteristics:\n",
    "        print(f\"- {char}\")\n",
    "    \n",
    "    # Save all metrics to a JSON file\n",
    "    metrics = {\n",
    "        \"model_name\": model_name,\n",
    "        \"test_accuracy\": float(test_accuracy),\n",
    "        \"inference_time_ms\": float(inference_time_ms),\n",
    "        \"model_size_mb\": float(model_size_mb),\n",
    "        \"parameters\": int(num_params),\n",
    "        \"flops\": int(flops),\n",
    "        \"macs\": int(macs),\n",
    "        \"roc_auc_score\": float(roc_auc) if roc_auc is not None else None,\n",
    "        \"per_class_accuracy\": class_acc_dict,\n",
    "        \"snr_performance\": snr_table,\n",
    "        \"classification_report\": cls_report,\n",
    "        \"characteristics\": characteristics\n",
    "    }\n",
    "    \n",
    "    with open(f\"{model_name.lower().replace('-', '_')}_drone_rf_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    print(f\"\\nMetrics saved to {model_name.lower().replace('-', '_')}_drone_rf_metrics.json\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize and train models sequentially\n",
    "\n",
    "# 1. SqueezeNet\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SQUEEZENET\")\n",
    "print(\"=\"*80)\n",
    "squeezenet = models.squeezenet1_1(pretrained=True)\n",
    "squeezenet.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "train_and_evaluate_model('SqueezeNet', squeezenet, train_loader, val_loader, test_loader, test_dataset, full_dataset)\n",
    "\n",
    "# 2. ShuffleNet\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SHUFFLENET\")\n",
    "print(\"=\"*80)\n",
    "shufflenet = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "shufflenet.fc = nn.Linear(shufflenet.fc.in_features, num_classes)\n",
    "train_and_evaluate_model('ShuffleNet', shufflenet, train_loader, val_loader, test_loader, test_dataset, full_dataset)\n",
    "\n",
    "# 3. EfficientNet\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"TRAINING EFFICIENTNET\")\n",
    "print(\"=\"*80)\n",
    "efficientnet = models.efficientnet_b0(pretrained=True)\n",
    "efficientnet.classifier[1] = nn.Linear(efficientnet.classifier[1].in_features, num_classes)\n",
    "train_and_evaluate_model('EfficientNet', efficientnet, train_loader, val_loader, test_loader, test_dataset, full_dataset)\n",
    "\n",
    "print(\"\\n\\nAll models have been trained and evaluated!\")\n",
    "print(\"Results have been saved to individual files for each model.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 233589092,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8456.792532,
   "end_time": "2025-04-13T14:00:53.827359",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-13T11:39:57.034827",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
