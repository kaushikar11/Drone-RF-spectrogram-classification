{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fbe572e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-13T07:29:06.960077Z",
     "iopub.status.busy": "2025-04-13T07:29:06.959845Z",
     "iopub.status.idle": "2025-04-13T10:09:57.473389Z",
     "shell.execute_reply": "2025-04-13T10:09:57.472402Z"
    },
    "papermill": {
     "duration": 9650.520793,
     "end_time": "2025-04-13T10:09:57.476539",
     "exception": false,
     "start_time": "2025-04-13T07:29:06.955746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /kaggle/working\n",
      "Found data at: /kaggle/input/noisy-drone-rf-signal-classification-v2/drone_RF_data/\n",
      "Looking for class stats at: /kaggle/input/noisy-drone-rf-signal-classification-v2/drone_RF_data/class_stats.csv\n",
      "Found 7 classes: ['DJI' 'FutabaT14' 'FutabaT7' 'Graupner' 'Noise' 'Taranis' 'Turnigy']\n",
      "CUDA available: Tesla P100-PCIE-16GB\n",
      "Using device: cuda:0\n",
      "Processing 17744 samples out of 17744 total...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4436/4436 [2:40:43<00:00,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully created at ./clean_spectrograms/\n",
      "Processed 17744 out of 17744 samples\n",
      "\n",
      "Dataset creation complete!\n",
      "Total samples: 17744\n",
      "Processed samples: 17744\n",
      "\n",
      "Samples by class:\n",
      "  - DJI: 1280\n",
      "  - FutabaT14: 3472\n",
      "  - FutabaT7: 801\n",
      "  - Graupner: 801\n",
      "  - Noise: 8872\n",
      "  - Taranis: 1663\n",
      "  - Turnigy: 855\n",
      "\n",
      "Dataset creation complete!\n",
      "Total samples: 17744\n",
      "Processed samples: 17744\n",
      "\n",
      "Samples by class:\n",
      "  - DJI: 1280\n",
      "  - FutabaT14: 3472\n",
      "  - FutabaT7: 801\n",
      "  - Graupner: 801\n",
      "  - Noise: 8872\n",
      "  - Taranis: 1663\n",
      "  - Turnigy: 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchaudio.transforms import Spectrogram\n",
    "import gc\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Limit memory usage for matplotlib\n",
    "plt.rcParams['figure.max_open_warning'] = 10\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "\n",
    "class DroneDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for drone IQ Signals + transform to spectrogram\n",
    "    \"\"\"\n",
    "    def __init__(self, path, transform=None, device=None):\n",
    "        self.path = path\n",
    "        self.files = os.listdir(path)\n",
    "        self.files = [f for f in self.files if f.endswith('pt')] # filter for files with .pt extension  \n",
    "        self.files = [f for f in self.files if f.startswith('IQdata_sample')] # filter for files which start with IQdata_sample in name\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "        # create list of targets and snrs for all samples\n",
    "        self.targets = []\n",
    "        self.snrs = []\n",
    "        \n",
    "        for file in self.files:\n",
    "            self.targets.append(int(file.split('_')[2][6:])) # get target from file name\n",
    "            self.snrs.append(int(file.split('_')[3].split('.')[0][3:])) # get snr from file name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            file = self.files[idx]\n",
    "            sample_id = int(file.split('_')[1][6:]) # get sample id from file name\n",
    "            \n",
    "            # More robust file loading\n",
    "            try:\n",
    "                data_path = os.path.join(self.path, file)\n",
    "                # Try with weights_only=True first (safer)\n",
    "                try:\n",
    "                    data_dict = torch.load(data_path, weights_only=True, map_location='cpu')\n",
    "                except:\n",
    "                    # Fall back to default loading if needed\n",
    "                    data_dict = torch.load(data_path, map_location='cpu')\n",
    "                    \n",
    "                # Verify data structure\n",
    "                if not isinstance(data_dict, dict):\n",
    "                    print(f\"Warning: File {file} didn't load as expected dictionary, got {type(data_dict)}\")\n",
    "                    # Create a placeholder dictionary\n",
    "                    data_dict = {'x_iq': torch.zeros(2, 1024), 'y': torch.tensor(0), 'snr': torch.tensor(0)}\n",
    "                \n",
    "                # Check required keys\n",
    "                required_keys = ['x_iq', 'y', 'snr']\n",
    "                for key in required_keys:\n",
    "                    if key not in data_dict:\n",
    "                        print(f\"Warning: Missing key {key} in file {file}\")\n",
    "                        if key == 'x_iq':\n",
    "                            data_dict[key] = torch.zeros(2, 1024)\n",
    "                        elif key in ['y', 'snr']:\n",
    "                            data_dict[key] = torch.tensor(0)\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {file}: {e}\")\n",
    "                # Create dummy data\n",
    "                data_dict = {'x_iq': torch.zeros(2, 1024), 'y': torch.tensor(0), 'snr': torch.tensor(0)}\n",
    "            \n",
    "            iq_data = data_dict['x_iq']\n",
    "            act_target = data_dict['y']\n",
    "            act_snr = data_dict['snr']\n",
    "            \n",
    "            # Ensure data has correct dimensions\n",
    "            if not isinstance(iq_data, torch.Tensor):\n",
    "                print(f\"Warning: iq_data in {file} is not a tensor, converting...\")\n",
    "                iq_data = torch.tensor(iq_data, dtype=torch.float32)\n",
    "                \n",
    "            # Make sure data has the right shape\n",
    "            if len(iq_data.shape) < 2 or iq_data.shape[0] != 2:\n",
    "                print(f\"Warning: Incorrect shape for iq_data in {file}: {iq_data.shape}, reshaping...\")\n",
    "                # Try to reshape or recreate with zeros\n",
    "                try:\n",
    "                    if len(iq_data.shape) == 1:\n",
    "                        # Assume it's a flat array that needs reshaping\n",
    "                        length = iq_data.shape[0] // 2\n",
    "                        iq_data = iq_data.reshape(2, length)\n",
    "                    else:\n",
    "                        # Create zeros\n",
    "                        iq_data = torch.zeros(2, 1024, dtype=torch.float32)\n",
    "                except:\n",
    "                    iq_data = torch.zeros(2, 1024, dtype=torch.float32)\n",
    "\n",
    "            # Process with transform if provided\n",
    "            if self.transform:\n",
    "                if self.device:\n",
    "                    try:\n",
    "                        iq_data = iq_data.to(device=self.device)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error moving data to device: {e}\")\n",
    "                        iq_data = torch.zeros(2, 1024, dtype=torch.float32)\n",
    "                        \n",
    "                try:\n",
    "                    transformed_data = self.transform(iq_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Transform error: {e}\")\n",
    "                    transformed_data = None\n",
    "            else:\n",
    "                transformed_data = None\n",
    "\n",
    "            return iq_data, act_target, act_snr, sample_id, transformed_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Critical error in __getitem__ for index {idx}: {e}\")\n",
    "            # Return dummy data as fallback\n",
    "            dummy_iq = torch.zeros(2, 1024, dtype=torch.float32)\n",
    "            dummy_target = torch.tensor(0)\n",
    "            dummy_snr = torch.tensor(0)\n",
    "            dummy_id = -1\n",
    "            return dummy_iq, dummy_target, dummy_snr, dummy_id, None\n",
    "    \n",
    "    def get_targets(self): # return list of targets\n",
    "        return self.targets\n",
    "\n",
    "    def get_snrs(self): # return list of snrs\n",
    "        return self.snrs\n",
    "    \n",
    "    def get_files(self):\n",
    "        return self.files\n",
    "        \n",
    "        \n",
    "class SpectrogramTransform(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        n_fft=1024,\n",
    "        win_length=1024,\n",
    "        hop_length=1024,\n",
    "        window_fn=torch.hann_window,\n",
    "        power=None,\n",
    "        normalized=False,\n",
    "        center=False,\n",
    "        onesided=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.spec = Spectrogram(\n",
    "            n_fft=n_fft, \n",
    "            win_length=win_length, \n",
    "            hop_length=hop_length, \n",
    "            window_fn=window_fn, \n",
    "            power=power, \n",
    "            normalized=normalized, \n",
    "            center=center, \n",
    "            onesided=onesided\n",
    "        ).to(device=device)   \n",
    "        self.win_length = win_length\n",
    "        self.n_fft = n_fft\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, iq_signal: torch.Tensor) -> torch.Tensor:\n",
    "        try:\n",
    "            # Check input dimensions\n",
    "            if iq_signal is None or not isinstance(iq_signal, torch.Tensor):\n",
    "                print(f\"Warning: iq_signal is not a tensor or is None\")\n",
    "                return torch.zeros((2, self.n_fft, self.n_fft), device=self.spec.device)\n",
    "                \n",
    "            if len(iq_signal.shape) != 2 or iq_signal.shape[0] != 2:\n",
    "                print(f\"Warning: Unexpected shape {iq_signal.shape}, should be (2, N)\")\n",
    "                # Try to fix or return zeros\n",
    "                if len(iq_signal.shape) == 2 and iq_signal.shape[1] == 2:\n",
    "                    # Might be transposed\n",
    "                    iq_signal = iq_signal.t()\n",
    "                elif len(iq_signal.shape) == 1:\n",
    "                    # Single dimension, try to reshape\n",
    "                    half_len = iq_signal.shape[0] // 2\n",
    "                    iq_signal = iq_signal.reshape(2, half_len)\n",
    "                else:\n",
    "                    # Can't fix, return zeros\n",
    "                    return torch.zeros((2, self.n_fft, self.n_fft), device=self.spec.device)\n",
    "            \n",
    "            # Convert to complex signal\n",
    "            try:\n",
    "                complex_signal = iq_signal[0,:] + (1j * iq_signal[1,:])\n",
    "                \n",
    "                # Check for NaN or inf values\n",
    "                if torch.isnan(iq_signal).any() or torch.isinf(iq_signal).any():\n",
    "                    print(\"Warning: NaN or Inf values in input signal, replacing with zeros\")\n",
    "                    complex_signal = torch.zeros_like(complex_signal)\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating complex signal: {e}\")\n",
    "                # Return zeros on error\n",
    "                return torch.zeros((2, self.n_fft, self.n_fft), device=self.spec.device)\n",
    "                \n",
    "            # Compute spectrogram\n",
    "            try:\n",
    "                spec = self.spec(complex_signal)\n",
    "            except Exception as e:\n",
    "                print(f\"Spectrogram computation error: {e}\")\n",
    "                # Return zeros on error\n",
    "                return torch.zeros((2, self.n_fft, self.n_fft), device=self.spec.device)\n",
    "                \n",
    "            # Convert to real representation\n",
    "            try:\n",
    "                spec = torch.view_as_real(spec)  # Returns a view of a complex input as a real tensor\n",
    "                spec = torch.moveaxis(spec, 2, 0)  # move channel dimension to first dimension (1024, 1024, 2) -> (2, 1024, 1024)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in tensor conversion: {e}\")\n",
    "                # Return zeros on error\n",
    "                return torch.zeros((2, self.n_fft, self.n_fft), device=self.spec.device)\n",
    "                \n",
    "            # Normalize\n",
    "            spec = spec / self.win_length  # normalize by fft window size\n",
    "            \n",
    "            return spec\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Unhandled error in SpectrogramTransform.forward: {e}\")\n",
    "            # Return zeros as fallback\n",
    "            return torch.zeros((2, self.n_fft, self.n_fft), device=self.spec.device)\n",
    "\n",
    "\n",
    "def generate_clean_spectrogram(spectrogram_2d, n_fft=1024, save_path=None, \n",
    "                              colormap='viridis', normalize=True):\n",
    "    \"\"\"\n",
    "    Robust version to generate clean power spectrum image without axes, titles, or labels\n",
    "    Fixed to avoid matplotlib errors and handle edge cases.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input\n",
    "        if spectrogram_2d is None:\n",
    "            print(\"Warning: Input spectrogram is None\")\n",
    "            return None\n",
    "            \n",
    "        # Validate spectrogram shape\n",
    "        if not isinstance(spectrogram_2d, np.ndarray):\n",
    "            print(f\"Warning: spectrogram_2d is not a numpy array, but {type(spectrogram_2d)}\")\n",
    "            try:\n",
    "                spectrogram_2d = np.array(spectrogram_2d)\n",
    "            except:\n",
    "                print(\"Could not convert to numpy array\")\n",
    "                return None\n",
    "                \n",
    "        if len(spectrogram_2d.shape) < 3:\n",
    "            print(f\"Warning: unexpected spectrogram shape: {spectrogram_2d.shape}\")\n",
    "            return None\n",
    "            \n",
    "        # Make sure dimensions are appropriate\n",
    "        if spectrogram_2d.shape[0] != 2:\n",
    "            print(f\"Warning: First dimension should be 2, got {spectrogram_2d.shape}\")\n",
    "            # Try to reshape or transpose if possible\n",
    "            if len(spectrogram_2d.shape) == 3 and spectrogram_2d.shape[2] == 2:\n",
    "                # Maybe channels last format\n",
    "                spectrogram_2d = np.transpose(spectrogram_2d, (2, 0, 1))\n",
    "            else:\n",
    "                return None\n",
    "                \n",
    "        # Validate array sizes to prevent index errors\n",
    "        if spectrogram_2d.shape[1] < 2 or spectrogram_2d.shape[2] < 2:\n",
    "            print(f\"Error: Spectrogram dimensions too small: {spectrogram_2d.shape}\")\n",
    "            return None\n",
    "            \n",
    "        # FFT-shift to center frequencies - use try/except to avoid index errors\n",
    "        try:\n",
    "            spectrogram_2d = np.roll(spectrogram_2d, n_fft//2, axis=1)\n",
    "        except Exception as e:\n",
    "            print(f\"FFT-shift error: {e}, skipping shift\")\n",
    "        \n",
    "        # Calculate power spectrum (log10 of magnitude)\n",
    "        # Add small epsilon to avoid log(0)\n",
    "        epsilon = 1e-10\n",
    "        try:\n",
    "            # Use numpy broadcasting with bounds checking to prevent index errors\n",
    "            real_part = spectrogram_2d[0,:,:].copy()\n",
    "            imag_part = spectrogram_2d[1,:,:].copy()\n",
    "            \n",
    "            # Validate array values\n",
    "            real_part = np.nan_to_num(real_part, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            imag_part = np.nan_to_num(imag_part, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            \n",
    "            # Calculate magnitude\n",
    "            magnitude = np.sqrt(real_part**2 + imag_part**2) + epsilon\n",
    "            power_spec = np.log10(magnitude)\n",
    "            \n",
    "            # Extra check for NaN or inf values\n",
    "            if np.isnan(power_spec).any() or np.isinf(power_spec).any():\n",
    "                print(\"Warning: NaN or Inf values in power spectrum, replacing with zeros\")\n",
    "                power_spec = np.nan_to_num(power_spec, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating power spectrum: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Normalize if requested\n",
    "        if normalize:\n",
    "            try:\n",
    "                min_val = np.nanmin(power_spec)\n",
    "                max_val = np.nanmax(power_spec)\n",
    "                if max_val > min_val:  # Avoid division by zero\n",
    "                    power_spec = (power_spec - min_val) / (max_val - min_val)\n",
    "                    # Replace any remaining NaNs with zeros\n",
    "                    power_spec = np.nan_to_num(power_spec)\n",
    "            except Exception as e:\n",
    "                print(f\"Error normalizing: {e}\")\n",
    "        \n",
    "        # Use a simpler approach to avoid 'apply_aspect' errors\n",
    "        plt.ioff()  # Turn off interactive mode\n",
    "        plt.close('all')  # Close all previous figures\n",
    "        \n",
    "        # Create new figure with fixed ratio\n",
    "        fig = plt.figure(figsize=(5, 5), dpi=100)\n",
    "        \n",
    "        # Add a normal subplot (don't use custom Axes)\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.set_position([0, 0, 1, 1])  # Make it fill the entire figure\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        # Plot the image\n",
    "        try:\n",
    "            ax.imshow(power_spec, cmap=colormap)\n",
    "            # Make sure figure is drawn\n",
    "            fig.canvas.draw_idle()\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting image: {e}\")\n",
    "            plt.close(fig)\n",
    "            return None\n",
    "        \n",
    "        if save_path:\n",
    "            try:\n",
    "                # Create directory if it doesn't exist\n",
    "                save_dir = os.path.dirname(save_path)\n",
    "                if save_dir and not os.path.exists(save_dir):\n",
    "                    os.makedirs(save_dir, exist_ok=True)\n",
    "                    \n",
    "                # Save with simpler parameters\n",
    "                fig.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving figure: {e}\")\n",
    "                return None\n",
    "            finally:\n",
    "                plt.close(fig)  # Close the figure\n",
    "                plt.close('all')  # Make sure all figures are closed\n",
    "                gc.collect()  # Force garbage collection\n",
    "            \n",
    "            return save_path\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_clean_spectrogram: {e}\")\n",
    "        plt.close('all')  # Make sure to close all figures on error\n",
    "        gc.collect()\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_sample_batch(batch_indices, dataset, output_path, class_names, colormap, normalize, n_fft):\n",
    "    \"\"\"Process a batch of samples to reduce memory overhead\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx in batch_indices:\n",
    "        try:\n",
    "            # Get sample data\n",
    "            iq_data, target, snr, sample_id, transformed_data = dataset[idx]\n",
    "            target_idx = target.item()\n",
    "            snr_val = snr.item()\n",
    "            \n",
    "            # Get class name\n",
    "            class_name = class_names[target_idx] if target_idx < len(class_names) else f\"unknown_class_{target_idx}\"\n",
    "            \n",
    "            # Define save directory\n",
    "            save_dir = output_path / class_name\n",
    "            save_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Define image filename\n",
    "            img_filename = f\"sample_{sample_id}_snr_{snr_val}.png\"\n",
    "            save_path = save_dir / img_filename\n",
    "            \n",
    "            # Skip if file already exists\n",
    "            if save_path.exists():\n",
    "                results.append((target_idx, snr_val))\n",
    "                continue\n",
    "                \n",
    "            # Generate and save clean power spectrum image\n",
    "            if transformed_data is not None:\n",
    "                # Handle potential errors in transform shape\n",
    "                try:\n",
    "                    # Move to CPU and convert to numpy safely\n",
    "                    np_data = transformed_data.cpu().numpy()\n",
    "                    \n",
    "                    # Check if data has expected shape\n",
    "                    if np_data.shape[0] != 2 or len(np_data.shape) != 3:\n",
    "                        print(f\"Warning: Unexpected shape {np_data.shape} for sample {idx}, reshaping...\")\n",
    "                        # Try to reshape if possible\n",
    "                        if len(np_data.shape) >= 2:\n",
    "                            if np_data.shape[0] == 1 and np_data.shape[1] == 2:\n",
    "                                # Reshape to expected format\n",
    "                                np_data = np_data.reshape(2, np_data.shape[2], np_data.shape[3] if len(np_data.shape) > 3 else 1)\n",
    "                    \n",
    "                    # Extra validation for dimensions\n",
    "                    if len(np_data.shape) == 3 and np_data.shape[0] == 2 and np_data.shape[1] > 1 and np_data.shape[2] > 1:\n",
    "                        # Generate spectrogram\n",
    "                        success = generate_clean_spectrogram(\n",
    "                            np_data,\n",
    "                            n_fft=n_fft,\n",
    "                            save_path=save_path,\n",
    "                            colormap=colormap,\n",
    "                            normalize=normalize\n",
    "                        )\n",
    "                        \n",
    "                        # If successful, add to results\n",
    "                        if success:\n",
    "                            results.append((target_idx, snr_val))\n",
    "                    else:\n",
    "                        print(f\"Error: Invalid shape for spectrogram data: {np_data.shape}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Transform error for sample {idx}: {e}\")\n",
    "                \n",
    "                # Close all matplotlib figures to prevent memory leaks\n",
    "                plt.close('all')\n",
    "                \n",
    "                # Clear CUDA cache periodically\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            # Continue with next sample\n",
    "            continue\n",
    "            \n",
    "    # Final cleanup before returning\n",
    "    plt.close('all')\n",
    "    gc.collect()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def create_clean_image_dataset(data_path, output_dir, class_names, device=None, \n",
    "                              colormap='viridis', normalize=True, num_workers=4, \n",
    "                              batch_size=16, checkpoint_interval=100, resume=True):\n",
    "    \"\"\"\n",
    "    Create a clean image dataset from the drone RF data with optimizations:\n",
    "    - Parallel processing\n",
    "    - Batch processing\n",
    "    - Memory management\n",
    "    - Checkpointing\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the input data\n",
    "        output_dir: Directory to save the images\n",
    "        class_names: List of class names\n",
    "        device: Torch device to use (CPU or GPU)\n",
    "        colormap: Matplotlib colormap to use\n",
    "        normalize: Whether to normalize power spectrum values\n",
    "        num_workers: Number of parallel workers (threads)\n",
    "        batch_size: Size of batches to process (per worker)\n",
    "        checkpoint_interval: How often to save progress\n",
    "        resume: Whether to resume from previous run\n",
    "    \n",
    "    Returns:\n",
    "        Summary of the created dataset\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    if device is None:\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize multiprocessing method if using CUDA\n",
    "    if 'cuda' in str(device):\n",
    "        try:\n",
    "            mp.set_start_method('spawn', force=True)\n",
    "        except RuntimeError:\n",
    "            pass  # Already set\n",
    "    \n",
    "    # Create transform\n",
    "    transform = SpectrogramTransform(device=device)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = DroneDataset(path=data_path, device=device, transform=transform)\n",
    "    \n",
    "    # Get unique SNR values\n",
    "    unique_snrs = sorted(set(dataset.get_snrs()))\n",
    "    \n",
    "    # Create output directory\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create structure: class folders only\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        (output_path / class_name).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load progress if resuming\n",
    "    start_idx = 0\n",
    "    if resume and (output_path / \"progress.txt\").exists():\n",
    "        with open(output_path / \"progress.txt\", \"r\") as f:\n",
    "            try:\n",
    "                last_processed = f.read().strip().split(\": \")[1]\n",
    "                start_idx = int(last_processed) + 1\n",
    "                print(f\"Resuming from index {start_idx}\")\n",
    "            except:\n",
    "                start_idx = 0\n",
    "    \n",
    "    # Initialize statistics\n",
    "    stats = {\n",
    "        'total_samples': len(dataset),\n",
    "        'samples_by_class': {class_name: 0 for class_name in class_names},\n",
    "        'samples_by_snr': {snr: 0 for snr in unique_snrs},\n",
    "        'processed_samples': 0\n",
    "    }\n",
    "    \n",
    "    # Load existing stats if resuming\n",
    "    if resume and (output_path / \"stats.pt\").exists():\n",
    "        try:\n",
    "            saved_stats = torch.load(output_path / \"stats.pt\")\n",
    "            stats.update(saved_stats)\n",
    "            print(\"Loaded existing statistics\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Total samples to process\n",
    "    total_samples = len(dataset)\n",
    "    remaining_samples = total_samples - start_idx\n",
    "    \n",
    "    if remaining_samples <= 0:\n",
    "        print(\"All samples already processed!\")\n",
    "        return stats\n",
    "    \n",
    "    print(f\"Processing {remaining_samples} samples out of {total_samples} total...\")\n",
    "    \n",
    "    # Create batches\n",
    "    all_indices = list(range(start_idx, total_samples))\n",
    "    batches = [all_indices[i:i + batch_size] for i in range(0, len(all_indices), batch_size)]\n",
    "    \n",
    "    # Process batches with progress bar\n",
    "    with tqdm(total=len(batches)) as pbar:\n",
    "        for batch_idx, batch in enumerate(batches):\n",
    "            # Use ThreadPoolExecutor for parallelization\n",
    "            with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "                # Split the batch into sub-batches for each worker\n",
    "                sub_batch_size = max(1, len(batch) // num_workers)\n",
    "                sub_batches = [batch[i:i + sub_batch_size] for i in range(0, len(batch), sub_batch_size)]\n",
    "                \n",
    "                # Process sub-batches in parallel\n",
    "                future_results = [\n",
    "                    executor.submit(\n",
    "                        process_sample_batch, \n",
    "                        sub_batch, \n",
    "                        dataset, \n",
    "                        output_path, \n",
    "                        class_names, \n",
    "                        colormap, \n",
    "                        normalize,\n",
    "                        transform.n_fft\n",
    "                    ) \n",
    "                    for sub_batch in sub_batches\n",
    "                ]\n",
    "                \n",
    "                # Collect results\n",
    "                for future in future_results:\n",
    "                    results = future.result()\n",
    "                    # Update statistics\n",
    "                    for target_idx, snr_val in results:\n",
    "                        class_name = class_names[target_idx]\n",
    "                        stats['samples_by_class'][class_name] += 1\n",
    "                        stats['samples_by_snr'][snr_val] += 1\n",
    "                        stats['processed_samples'] += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if (batch_idx + 1) % checkpoint_interval == 0 or batch_idx == len(batches) - 1:\n",
    "                last_idx = batch[-1]\n",
    "                with open(output_path / \"progress.txt\", \"w\") as f:\n",
    "                    f.write(f\"Last processed: {last_idx}\\n\")\n",
    "                \n",
    "                # Save statistics\n",
    "                torch.save(stats, output_path / \"stats.pt\")\n",
    "                \n",
    "                # Clean up memory\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save final dataset information\n",
    "    with open(output_path / \"dataset_info.txt\", \"w\") as f:\n",
    "        f.write(f\"Total samples: {stats['total_samples']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Samples by class:\\n\")\n",
    "        for class_name, count in stats['samples_by_class'].items():\n",
    "            f.write(f\"  - {class_name}: {count}\\n\")\n",
    "        \n",
    "        f.write(\"\\nSamples by SNR:\\n\")\n",
    "        for snr, count in stats['samples_by_snr'].items():\n",
    "            f.write(f\"  - {snr} dB: {count}\\n\")\n",
    "    \n",
    "    print(f\"Dataset successfully created at {output_dir}\")\n",
    "    print(f\"Processed {stats['processed_samples']} out of {total_samples} samples\")\n",
    "    return stats\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Fix matplotlib settings to avoid errors\n",
    "        import matplotlib\n",
    "        matplotlib.use('Agg')  # Use non-interactive backend\n",
    "        \n",
    "        # Reduce memory usage in matplotlib\n",
    "        plt.rcParams['figure.max_open_warning'] = 10  # Lower threshold for warning\n",
    "        plt.rcParams['figure.dpi'] = 100\n",
    "        plt.rcParams['savefig.dpi'] = 100\n",
    "        \n",
    "        # Fix file paths for Kaggle\n",
    "        print(\"Current working directory:\", os.getcwd())\n",
    "        \n",
    "        # Example usage - try to find the data path\n",
    "        possible_data_paths = [\n",
    "            \"/kaggle/input/noisy-drone-rf-signal-classification-v2/drone_RF_data/\",\n",
    "            \"./drone_RF_data/\",\n",
    "            \"../input/noisy-drone-rf-signal-classification-v2/drone_RF_data/\",\n",
    "            \"../drone_RF_data/\"\n",
    "        ]\n",
    "        \n",
    "        # Find the first valid path\n",
    "        data_path = None\n",
    "        for path in possible_data_paths:\n",
    "            if os.path.exists(path):\n",
    "                data_path = path\n",
    "                print(f\"Found data at: {data_path}\")\n",
    "                break\n",
    "                \n",
    "        if data_path is None:\n",
    "            print(\"Warning: Could not find data path automatically\")\n",
    "            data_path = \"/kaggle/input/noisy-drone-rf-signal-classification-v2/drone_RF_data/\"  # Default\n",
    "            \n",
    "        # Create output directory\n",
    "        output_dir = \"./clean_spectrograms/\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Lower number of workers to avoid overloading\n",
    "        import multiprocessing\n",
    "        optimal_workers = 1  # Use just 1 worker to avoid issues\n",
    "        \n",
    "        # Read class information (assuming class_stats.csv exists)\n",
    "        import pandas as pd\n",
    "        try:\n",
    "            class_stats_path = os.path.join(data_path, 'class_stats.csv')\n",
    "            print(f\"Looking for class stats at: {class_stats_path}\")\n",
    "            \n",
    "            if os.path.exists(class_stats_path):\n",
    "                dataset_stats = pd.read_csv(class_stats_path, index_col=0)\n",
    "                class_names = dataset_stats['class'].values\n",
    "                print(f\"Found {len(class_names)} classes: {class_names}\")\n",
    "            else:\n",
    "                print(f\"Class stats file not found at {class_stats_path}\")\n",
    "                # Fallback class names if CSV can't be read\n",
    "                class_names = [f\"class_{i}\" for i in range(10)]  # Assuming 10 classes\n",
    "                print(f\"Using fallback class names: {class_names}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading class stats: {e}\")\n",
    "            # Fallback class names if CSV can't be read\n",
    "            class_names = [f\"class_{i}\" for i in range(10)]  # Assuming 10 classes\n",
    "            print(f\"Using fallback class names due to error: {class_names}\")\n",
    "        \n",
    "        # Check GPU availability\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "            device = torch.device('cuda:0')\n",
    "        else:\n",
    "            print(\"CUDA not available, using CPU\")\n",
    "            device = torch.device('cpu')\n",
    "            \n",
    "        # Create clean image dataset with optimizations\n",
    "        try:\n",
    "            stats = create_clean_image_dataset(\n",
    "                data_path=data_path,\n",
    "                output_dir=output_dir,\n",
    "                class_names=class_names,\n",
    "                device=device,\n",
    "                colormap='viridis',\n",
    "                normalize=True,\n",
    "                num_workers=optimal_workers,  # Single thread for stability\n",
    "                batch_size=4,  # Very small batch size to avoid memory issues\n",
    "                checkpoint_interval=2,  # Save progress very frequently\n",
    "                resume=True  # Resume from previous run if available\n",
    "            )\n",
    "            \n",
    "            print(\"\\nDataset creation complete!\")\n",
    "            print(f\"Total samples: {stats['total_samples']}\")\n",
    "            print(f\"Processed samples: {stats['processed_samples']}\")\n",
    "            print(\"\\nSamples by class:\")\n",
    "            for class_name, count in stats['samples_by_class'].items():\n",
    "                print(f\"  - {class_name}: {count}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in main processing: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Critical error in main script: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(\"\\nDataset creation complete!\")\n",
    "    print(f\"Total samples: {stats['total_samples']}\")\n",
    "    print(f\"Processed samples: {stats['processed_samples']}\")\n",
    "    print(\"\\nSamples by class:\")\n",
    "    for class_name, count in stats['samples_by_class'].items():\n",
    "        print(f\"  - {class_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1cb587",
   "metadata": {
    "papermill": {
     "duration": 0.17582,
     "end_time": "2025-04-13T10:09:57.833530",
     "exception": false,
     "start_time": "2025-04-13T10:09:57.657710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5286052,
     "sourceId": 8791897,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9657.814494,
   "end_time": "2025-04-13T10:10:00.677277",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-13T07:29:02.862783",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
